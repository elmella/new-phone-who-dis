{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import warnings\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import umap\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix_efficient(df, word_to_index, d, initial_values=1):\n",
    "    transition_counts = np.ones((d, d), dtype=int) * initial_values\n",
    "    for utterance in df['utterance']:\n",
    "        if type(utterance) != str:\n",
    "            try:\n",
    "                utterance = str(utterance)\n",
    "            except:\n",
    "                continue\n",
    "        words = utterance.split()\n",
    "        for i in range(1, len(words)):\n",
    "            word1 = words[i - 1]\n",
    "            word2 = words[i]\n",
    "            if word1 in word_to_index and word2 in word_to_index:\n",
    "                index1 = word_to_index[word1]\n",
    "                index2 = word_to_index[word2]\n",
    "                transition_counts[index2, index1] += 1\n",
    "    column_sums = transition_counts.sum(axis=0, keepdims=True)\n",
    "    transition_probabilities = transition_counts / column_sums\n",
    "    return transition_probabilities, word_to_index\n",
    "\n",
    "def tensor_trouble(df, word_to_index, d, initial_values=1):\n",
    "    tensor = np.ones((d, 2, 2), dtype=int) * initial_values\n",
    "    for episode, group in df.groupby('episode'):\n",
    "        if type(group['utterance']) != str:\n",
    "            group['utterance'] = group['utterance'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "        text = \" \".join(group['utterance'].astype(str))\n",
    "        words = text.split()\n",
    "        roles = np.concatenate(group.apply(lambda row: [row['is_host']] * len(str(row['utterance']).split()), axis=1).values)\n",
    "        for i in range(2, len(words)):\n",
    "            word = words[i - 2]\n",
    "            if word not in word_to_index:  # Check if the word exists in the dictionary\n",
    "                continue  # Skip this iteration if the word is not found\n",
    "\n",
    "            # get the roles of the current word and the next two words\n",
    "            next_role = roles[i - 1]\n",
    "            following_role = roles[i]\n",
    "\n",
    "            # get the index of the current word\n",
    "            current_index = word_to_index[word]\n",
    "\n",
    "            # get the index of the roles\n",
    "            current_role_index = 0 if next_role else 1\n",
    "            next_role_index = 0 if following_role else 1\n",
    "\n",
    "            \n",
    "            tensor[current_index, next_role_index, current_role_index] += 1\n",
    "    return tensor, word_to_index\n",
    "\n",
    "def make_emission(host, guest):\n",
    "    emission = np.zeros((len(host), len(host), 2))\n",
    "    emission[:, :, 0] = host\n",
    "    emission[:, :, 1] = guest\n",
    "    return np.swapaxes(emission, 0, 1)\n",
    "\n",
    "def tensor_viterbi(obs, transition, emission, initial):\n",
    "    b_eps = 1e-25\n",
    "    start_index = obs[0]\n",
    "    obs = obs[1:]\n",
    "    n = len(obs)\n",
    "    d = transition.shape[0]\n",
    "    eta = np.zeros((n, 2))\n",
    "    backpointers = np.zeros((n, 2), dtype=int)\n",
    "    eta[0] = np.log(initial) + np.log(emission[start_index, obs[1], :])\n",
    "    obs = obs[1:]\n",
    "    for i in range(1, n - 1):\n",
    "        b = emission[obs[i - 1], obs[i], :]\n",
    "        if np.any(b == 0):\n",
    "            zero_index = np.where(b == 0)\n",
    "            b[zero_index] = b_eps\n",
    "        eta_candidate = np.log(transition[obs[i - 1], :, :]) + np.log(b)[:, np.newaxis] + eta[i - 1][np.newaxis, :]\n",
    "        eta[i] = np.max(eta_candidate, axis=1)\n",
    "        backpointers[i] = np.argmax(eta_candidate, axis=1)\n",
    "    state_sequence = np.zeros(n, dtype=int)\n",
    "    state_sequence[-1] = np.argmax(eta[-1])\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        state_sequence[i] = backpointers[i + 1, state_sequence[i + 1]]\n",
    "    return state_sequence\n",
    "\n",
    "def pad_to_match(a, b):\n",
    "    \"\"\"\n",
    "    Pads the shorter array with its last element to match the length of the longer array.\n",
    "    \n",
    "    Args:\n",
    "        a (np.array): First array for comparison.\n",
    "        b (np.array): Second array for comparison.\n",
    "        \n",
    "    Returns:\n",
    "        np.array, np.array: The two arrays modified to have equal lengths.\n",
    "    \"\"\"\n",
    "    if len(a) == len(b):\n",
    "        return a, b\n",
    "    elif len(a) > len(b):\n",
    "        padding = np.full(len(a) - len(b), b[-1])\n",
    "        b_padded = np.concatenate((b, padding))\n",
    "        return a, b_padded\n",
    "    else:\n",
    "        padding = np.full(len(b) - len(a), a[-1])\n",
    "        a_padded = np.concatenate((a, padding))\n",
    "        return a_padded, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "embedding_path = 'archive/embed_df_with_hosts_filtered.csv'\n",
    "utterance_path = 'archive/processed_utterances-2sp.csv'\n",
    "umap_components = 10\n",
    "initial_state_probabilities = np.array([.5, .5])\n",
    "\n",
    "df = pd.read_csv(embedding_path)\n",
    "\n",
    "# get the number of unique hosts\n",
    "host_ids = df['host_id'].unique()\n",
    "n_components = len(host_ids) // 4\n",
    "\n",
    "\n",
    "X, y = df['embedding'].values, df['host_id'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# Convert the labels to integers\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Convert the lists of floats into numpy arrays\n",
    "X_train = np.array([np.array(x) for x in X_train])\n",
    "X_test = np.array([np.array(x) for x in X_test])\n",
    "# Reduce dimensions to 20 with UMAP\n",
    "umap_reducer = umap.UMAP(n_components=umap_components)\n",
    "X_reduced = umap_reducer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "print(f\"Fitting GMM with {n_components} components\")\n",
    "gmm = GaussianMixture(n_components=n_components)\n",
    "gmm.fit(X_reduced)\n",
    "\n",
    "# Predict cluster labels\n",
    "train_cluster_labels = gmm.predict(X_reduced)\n",
    "\n",
    "# reduce the dimensions of the test set\n",
    "X_test_reduced = umap_reducer.transform(X_test)\n",
    "\n",
    "# Predict the cluster labels of the test set\n",
    "test_cluster_labels = gmm.predict(X_test_reduced)\n",
    "\n",
    "# create dictionary to map train cluster labels to embeddings\n",
    "train_cluster_to_embedding = {cluster: [] for cluster in set(train_cluster_labels)}\n",
    "for cluster, embedding in zip(train_cluster_labels, X_train):\n",
    "    train_cluster_to_embedding[cluster].append(embedding)\n",
    "\n",
    "# create dictionary to map embeddings to episode ids\n",
    "embedding_to_episode = {tuple(embedding): episode for embedding, episode in zip(df['embedding'], df['episode'])}\n",
    "\n",
    "# create dictionary to map train cluster labels to episoded ids\n",
    "train_cluster_to_episode = {cluster: [] for cluster in set(train_cluster_labels)}\n",
    "for cluster, embedding in zip(train_cluster_labels, X_train):\n",
    "    train_cluster_to_episode[cluster].append(embedding_to_episode[tuple(embedding)])\n",
    "\n",
    "# create dictionary to map test cluster labels to embeddings\n",
    "test_cluster_to_embedding = {cluster: [] for cluster in set(test_cluster_labels)}\n",
    "for cluster, embedding in zip(test_cluster_labels, X_test):\n",
    "    test_cluster_to_embedding[cluster].append(embedding)\n",
    "\n",
    "\n",
    "# create dictionary to map test cluster labels to episoded ids\n",
    "test_cluster_to_episode = {cluster: [] for cluster in set(test_cluster_labels)}\n",
    "for cluster, embedding in zip(test_cluster_labels, X_test):\n",
    "    test_cluster_to_episode[cluster].append(embedding_to_episode[tuple(embedding)])\n",
    "\n",
    "results_dict = {episode: [] for episode in df['episode'].unique()}\n",
    "\n",
    " # Load the utterance df\n",
    "utterance_df = pd.read_csv(utterance_path)\n",
    "\n",
    "\n",
    "# Iterate through each unique test cluster label\n",
    "for cluster in set(test_cluster_labels):\n",
    "    # Get the embeddings of the cluster\n",
    "    cluster_embeddings = test_cluster_to_embedding[cluster]\n",
    "    # Get the episode ids of the test episodes in the cluster\n",
    "    test_episodes = test_cluster_to_episode[cluster]\n",
    "\n",
    "    # Get the episode ids of the train episodes in the cluster\n",
    "    train_episodes = train_cluster_to_episode[cluster]\n",
    "\n",
    "    # Filter the utterance df to only include the episodes in the cluster for training and testing\n",
    "    filtered_df = utterance_df[utterance_df['episode'].isin(train_episodes + test_episodes)][['utterance']]\n",
    "\n",
    "    # Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "    words_series = filtered_df['utterance'].str.split().explode()\n",
    "    unique_words = set(words_series)\n",
    "    word_frequencies = words_series.value_counts().to_dict()\n",
    "    word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "    # Filter the filter df to only include the episodes in the cluster for training and testing\n",
    "    training_df = filtered_df[filtered_df['episode'].isin(train_episodes)][['is_host', 'utterance']]\n",
    "    testing_df = filtered_df[filtered_df['episode'].isin(test_episodes)][['is_host', 'utterance']]\n",
    "\n",
    "    # Get the transition matrix for the training data\n",
    "    transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(training_df[training_df['is_host'] == True], word_indices, len(unique_words))\n",
    "    transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(training_df[training_df['is_host'] == False], word_indices, len(unique_words))\n",
    "\n",
    "    # Get the tensor for the training data\n",
    "    tensor, word_to_index = tensor_trouble(training_df, word_indices, len(unique_words))\n",
    "\n",
    "    # Normalize the tensor\n",
    "    tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "    # Get the emission matrix for the training data\n",
    "    emission_matrix = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "    for episode in test_episodes:\n",
    "        test_episode_df = testing_df[testing_df['episode'] == episode]\n",
    "        test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "        test_episode_df = test_episode_df.explode('utterance')\n",
    "        test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "        test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "        test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "        test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "        obs = [0] + test_episode_word_indices  # Add start token index (e.g., 0) if your Viterbi expects it\n",
    "\n",
    "        state_sequence = tensor_viterbi(obs, tensor_normalized, emission_matrix, initial_state_probabilities)\n",
    "        # print(len(state_sequence))\n",
    "        test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "        # Now, calculate the accuracy\n",
    "        accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "\n",
    "        # Switch the labels if accuracy is less than 0.5\n",
    "        if accuracy < 0.5:\n",
    "            accuracy = 1 - accuracy\n",
    "            state_sequence_padded = 1 - state_sequence_padded\n",
    "        \n",
    "        assume_0 = (test_label_padded == 1).astype(int)\n",
    "        assume_0_accuracy = np.mean((assume_0 == test_label_padded).astype(int))\n",
    "\n",
    "        # Switch the labels if accuracy is less than 0.5\n",
    "        if assume_0_accuracy < 0.5:\n",
    "            assume_0_accuracy = 1 - assume_0_accuracy\n",
    "            assume_host = 1 - assume_host\n",
    "\n",
    "        # Calculate the confusion matrices\n",
    "        confusion = confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "        assume_0_confusion = confusion_matrix(test_label_padded, assume_0)\n",
    "\n",
    "        # Append the confusion matrices to the results dictionary\n",
    "        results_dict[episode].append({\n",
    "            'accuracy': accuracy,\n",
    "            'confusion': confusion,\n",
    "            'assume_0_accuracy': assume_0_accuracy,\n",
    "            'assume_0_confusion': assume_0_confusion,\n",
    "            'method': 'Clustering'\n",
    "        })\n",
    "\n",
    "    for host_id in df['host_id'].unique():\n",
    "        # Filter the utterance and embedding dataframes for this specific host\n",
    "        host_df = df[df['host_id'] == host_id]\n",
    "\n",
    "        # Get the episodes related to this host\n",
    "        host_episodes = host_df['episode'].unique()\n",
    "\n",
    "        # Load the utterance dataframe, filtered by the host's episodes\n",
    "        host_utterance_df = utterance_df[utterance_df['episode'].isin(host_episodes)]\n",
    "\n",
    "        # Process the utterances as before\n",
    "        words_series = host_utterance_df['utterance'].str.split().explode()\n",
    "        unique_words = set(words_series)\n",
    "        word_frequencies = words_series.value_counts().to_dict()\n",
    "        word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "\n",
    "        training_df = host_utterance_df[host_utterance_df['episode'].isin(train_episodes)]\n",
    "        testing_df = host_utterance_df[host_utterance_df['episode'].isin(test_episodes)]\n",
    "\n",
    "        # Calculate transition matrices and tensor for training data\n",
    "        transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(training_df[training_df['is_host'] == True], word_indices, len(unique_words))\n",
    "        transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(training_df[training_df['is_host'] == False], word_indices, len(unique_words))\n",
    "        tensor, word_to_index = tensor_trouble(training_df, word_indices, len(unique_words))\n",
    "        tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "        # Emission matrix for training data\n",
    "        emission_matrix = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "\n",
    "        for episode in test_episodes:\n",
    "            test_episode_df = testing_df[testing_df['episode'] == episode]\n",
    "            test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "            test_episode_df = test_episode_df.explode('utterance')\n",
    "            test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "            test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "            test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "            test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "            obs = [0] + test_episode_word_indices\n",
    "\n",
    "            state_sequence = tensor_viterbi(obs, tensor_normalized, emission_matrix, initial_state_probabilities)\n",
    "            test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "            accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "\n",
    "            if accuracy < 0.5:\n",
    "                accuracy = 1 - accuracy\n",
    "                state_sequence_padded = 1 - state_sequence_padded\n",
    "\n",
    "            assume_0 = (test_label_padded == 1).astype(int)\n",
    "            assume_0_accuracy = np.mean((assume_0 == test_label_padded).astype(int))\n",
    "\n",
    "            if assume_0_accuracy < 0.5:\n",
    "                assume_0_accuracy = 1 - assume_0_accuracy\n",
    "                assume_host = 1 - assume_host\n",
    "\n",
    "            confusion = confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "            assume_0_confusion = confusion_matrix(test_label_padded, assume_0)\n",
    "\n",
    "            results_dict[episode].append({\n",
    "                'accuracy': accuracy,\n",
    "                'confusion': confusion,\n",
    "                'assume_0_accuracy': assume_0_accuracy,\n",
    "                'assume_0_confusion': assume_0_confusion,\n",
    "                'method': 'Host'\n",
    "            })\n",
    "\n",
    "# Save the results dictionary to a file\n",
    "with open('archive/results_dict.json', 'w') as f:\n",
    "    f.write(str(results_dict))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
