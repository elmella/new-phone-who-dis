{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import warnings\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import umap\n",
    "import ast\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix_efficient(df, word_to_index, d, initial_values=1):\n",
    "    transition_counts = np.ones((d, d), dtype=int) * initial_values\n",
    "    for utterance in df['utterance']:\n",
    "        if type(utterance) != str:\n",
    "            try:\n",
    "                utterance = str(utterance)\n",
    "            except:\n",
    "                continue\n",
    "        words = utterance.split()\n",
    "        for i in range(1, len(words)):\n",
    "            word1 = words[i - 1]\n",
    "            word2 = words[i]\n",
    "            if word1 in word_to_index and word2 in word_to_index:\n",
    "                index1 = word_to_index[word1]\n",
    "                index2 = word_to_index[word2]\n",
    "                transition_counts[index2, index1] += 1\n",
    "    column_sums = transition_counts.sum(axis=0, keepdims=True)\n",
    "    transition_probabilities = transition_counts / column_sums\n",
    "    return transition_probabilities, word_to_index\n",
    "\n",
    "def tensor_trouble(df, word_to_index, d, initial_values=1):\n",
    "    tensor = np.ones((d, 2, 2), dtype=int) * initial_values\n",
    "    for episode, group in df.groupby('episode'):\n",
    "        if type(group['utterance']) != str:\n",
    "            group['utterance'] = group['utterance'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "        text = \" \".join(group['utterance'].astype(str))\n",
    "        words = text.split()\n",
    "        roles = np.concatenate(group.apply(lambda row: [row['is_host']] * len(str(row['utterance']).split()), axis=1).values)\n",
    "        for i in range(2, len(words)):\n",
    "            word = words[i - 2]\n",
    "            if word not in word_to_index:  # Check if the word exists in the dictionary\n",
    "                continue  # Skip this iteration if the word is not found\n",
    "\n",
    "            # get the roles of the current word and the next two words\n",
    "            next_role = roles[i - 1]\n",
    "            following_role = roles[i]\n",
    "\n",
    "            # get the index of the current word\n",
    "            current_index = word_to_index[word]\n",
    "\n",
    "            # get the index of the roles\n",
    "            current_role_index = 0 if next_role else 1\n",
    "            next_role_index = 0 if following_role else 1\n",
    "\n",
    "            \n",
    "            tensor[current_index, next_role_index, current_role_index] += 1\n",
    "    return tensor, word_to_index\n",
    "\n",
    "def make_emission(host, guest):\n",
    "    emission = np.zeros((len(host), len(host), 2))\n",
    "    emission[:, :, 0] = host\n",
    "    emission[:, :, 1] = guest\n",
    "    return np.swapaxes(emission, 0, 1)\n",
    "\n",
    "def tensor_viterbi(obs, transition, emission, initial):\n",
    "    b_eps = 1e-25\n",
    "    start_index = obs[0]\n",
    "    obs = obs[1:]\n",
    "    n = len(obs)\n",
    "    d = transition.shape[0]\n",
    "    eta = np.zeros((n, 2))\n",
    "    backpointers = np.zeros((n, 2), dtype=int)\n",
    "    eta[0] = np.log(initial) + np.log(emission[start_index, obs[1], :])\n",
    "    obs = obs[1:]\n",
    "    for i in range(1, n - 1):\n",
    "        b = emission[obs[i - 1], obs[i], :]\n",
    "        if np.any(b == 0):\n",
    "            zero_index = np.where(b == 0)\n",
    "            b[zero_index] = b_eps\n",
    "        eta_candidate = np.log(transition[obs[i - 1], :, :]) + np.log(b)[:, np.newaxis] + eta[i - 1][np.newaxis, :]\n",
    "        eta[i] = np.max(eta_candidate, axis=1)\n",
    "        backpointers[i] = np.argmax(eta_candidate, axis=1)\n",
    "    state_sequence = np.zeros(n, dtype=int)\n",
    "    state_sequence[-1] = np.argmax(eta[-1])\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        state_sequence[i] = backpointers[i + 1, state_sequence[i + 1]]\n",
    "    return state_sequence\n",
    "\n",
    "def pad_to_match(a, b):\n",
    "    \"\"\"\n",
    "    Pads the shorter array with its last element to match the length of the longer array.\n",
    "    \n",
    "    Args:\n",
    "        a (np.array): First array for comparison.\n",
    "        b (np.array): Second array for comparison.\n",
    "        \n",
    "    Returns:\n",
    "        np.array, np.array: The two arrays modified to have equal lengths.\n",
    "    \"\"\"\n",
    "    if len(a) == len(b):\n",
    "        return a, b\n",
    "    elif len(a) > len(b):\n",
    "        padding = np.full(len(a) - len(b), b[-1])\n",
    "        b_padded = np.concatenate((b, padding))\n",
    "        return a, b_padded\n",
    "    else:\n",
    "        padding = np.full(len(b) - len(a), a[-1])\n",
    "        a_padded = np.concatenate((a, padding))\n",
    "        return a_padded, b\n",
    "    \n",
    "def filter_episodes(df, host_id=None):\n",
    "    df_filtered = df[df['host_id'] != -1]\n",
    "    if host_id is None:\n",
    "        top_host = df_filtered.groupby('host_id')['episode'].nunique().idxmax()\n",
    "    else:\n",
    "        top_host = host_id\n",
    "    top_host_episodes = df_filtered[df_filtered['host_id'] == top_host]['episode'].unique()\n",
    "    df_top_host_all_utterances = df[df['episode'].isin(top_host_episodes)]\n",
    "    utterance_counts = df_top_host_all_utterances.groupby('episode')['utterance'].count()\n",
    "    episodes_over_30 = utterance_counts[utterance_counts > 30].index\n",
    "    df_top_host_over_30 = df_top_host_all_utterances[df_top_host_all_utterances['episode'].isin(episodes_over_30)]\n",
    "    return df_top_host_over_30.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "embedding_path = 'archive/embed_df_with_hosts_filtered.csv'\n",
    "utterance_path = 'archive/processed_utterances-2sp.csv'\n",
    "umap_components = 10\n",
    "initial_state_probabilities = np.array([.5, .5])\n",
    "\n",
    "# Load the data\n",
    "episode_embedding_df = pd.read_csv('archive/embed_df_with_hosts.csv')\n",
    "\n",
    "# get the host_ids that have at least 100 episodes\n",
    "host_ids = episode_embedding_df['host_id'].value_counts()[episode_embedding_df['host_id'].value_counts() > 100].index\n",
    "\n",
    "episode_embedding_df = episode_embedding_df[episode_embedding_df['host_id'].isin(host_ids)]\n",
    "# Convert the string into a list and then into an array of floats\n",
    "episode_embedding_df['embedding'] = episode_embedding_df['embedding'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "utterance_df = pd.read_csv(utterance_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting GMM with 8 components\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the number of unique hosts\n",
    "host_ids = episode_embedding_df['host_id'].unique()\n",
    "n_components = len(host_ids) // 4\n",
    "\n",
    "episodes = episode_embedding_df['episode'].unique()\n",
    "np.random.shuffle(episodes)\n",
    "split_index = int(len(episodes) * 0.8)\n",
    "train_episodes = episodes[:split_index]\n",
    "test_episodes = episodes[split_index:]\n",
    "\n",
    "\n",
    "X_train = episode_embedding_df[episode_embedding_df['episode'].isin(train_episodes)]['embedding']\n",
    "X_test = episode_embedding_df[episode_embedding_df['episode'].isin(test_episodes)]['embedding']\n",
    "y_train = episode_embedding_df[episode_embedding_df['episode'].isin(train_episodes)]['host_id']\n",
    "y_test = episode_embedding_df[episode_embedding_df['episode'].isin(test_episodes)]['host_id']\n",
    "\n",
    "# Convert the labels to integers\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Convert the lists of floats into numpy arrays\n",
    "X_train = np.array([np.array(x) for x in X_train])\n",
    "X_test = np.array([np.array(x) for x in X_test])\n",
    "# Reduce dimensions to 20 with UMAP\n",
    "umap_reducer = umap.UMAP(n_components=umap_components)\n",
    "X_reduced = umap_reducer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "print(f\"Fitting GMM with {n_components} components\")\n",
    "gmm = GaussianMixture(n_components=n_components)\n",
    "gmm.fit(X_reduced)\n",
    "\n",
    "# Predict cluster labels\n",
    "train_cluster_labels = gmm.predict(X_reduced)\n",
    "\n",
    "# reduce the dimensions of the test set\n",
    "X_test_reduced = umap_reducer.transform(X_test)\n",
    "\n",
    "# Predict the cluster labels of the test set\n",
    "test_cluster_labels = gmm.predict(X_test_reduced)\n",
    "\n",
    "# # create dictionary to map train cluster labels to embeddings\n",
    "train_cluster_to_embedding = {cluster: [] for cluster in set(train_cluster_labels)}\n",
    "for cluster, embedding in zip(train_cluster_labels, X_train):\n",
    "    train_cluster_to_embedding[cluster].append(embedding)\n",
    "\n",
    "# create dictionary to map embeddings to episode ids\n",
    "embedding_to_episode = {tuple(embedding): episode for embedding, episode in zip(episode_embedding_df['embedding'], episode_embedding_df['episode'])}\n",
    "\n",
    "# create dictionary to map train cluster labels to episoded ids\n",
    "train_cluster_to_episode = {cluster: [] for cluster in set(train_cluster_labels)}\n",
    "for cluster, embedding in zip(train_cluster_labels, X_train):\n",
    "    train_cluster_to_episode[cluster].append(embedding_to_episode[tuple(embedding)])\n",
    "\n",
    "# create dictionary to map test cluster labels to embeddings\n",
    "test_cluster_to_embedding = {cluster: [] for cluster in set(test_cluster_labels)}\n",
    "for cluster, embedding in zip(test_cluster_labels, X_test):\n",
    "    test_cluster_to_embedding[cluster].append(embedding)\n",
    "\n",
    "\n",
    "# create dictionary to map test cluster labels to episoded ids\n",
    "test_cluster_to_episode = {cluster: [] for cluster in set(test_cluster_labels)}\n",
    "for cluster, embedding in zip(test_cluster_labels, X_test):\n",
    "    test_cluster_to_episode[cluster].append(embedding_to_episode[tuple(embedding)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "37692\n",
      "9164\n",
      "Cluster 1\n",
      "70125\n",
      "18422\n",
      "Cluster 2\n",
      "93467\n",
      "23342\n",
      "Cluster 3\n",
      "230424\n",
      "54414\n",
      "Cluster 4\n",
      "147726\n",
      "38167\n",
      "Cluster 5\n",
      "89648\n",
      "22568\n",
      "Cluster 6\n",
      "121046\n",
      "30746\n",
      "Cluster 7\n",
      "76375\n",
      "18539\n",
      "Host 0\n",
      "13654\n",
      "3116\n",
      "Host -1\n",
      "0\n",
      "0\n",
      "Host 1\n",
      "34460\n",
      "7894\n",
      "Host 2\n",
      "35\n",
      "0\n",
      "Host 3\n",
      "0\n",
      "0\n",
      "Host 4\n",
      "46\n",
      "0\n",
      "Host 5\n",
      "31313\n",
      "9111\n",
      "Host 6\n",
      "0\n",
      "0\n",
      "Host 7\n",
      "33473\n",
      "8864\n",
      "Host 8\n",
      "39116\n",
      "10812\n",
      "Host 10\n",
      "14860\n",
      "3891\n",
      "Host 11\n",
      "29394\n",
      "6408\n",
      "Duplicate or existing entry found for episode 115351 under Host method.\n",
      "Host 12\n",
      "68203\n",
      "15997\n",
      "Host 13\n",
      "12034\n",
      "4037\n",
      "Host 14\n",
      "24982\n",
      "6222\n",
      "Host 16\n",
      "49733\n",
      "11948\n",
      "Duplicate or existing entry found for episode 116993 under Host method.\n",
      "Host 17\n",
      "25238\n",
      "6526\n",
      "Duplicate or existing entry found for episode 63176 under Host method.\n",
      "Host 18\n",
      "6846\n",
      "2093\n",
      "Host 9\n",
      "24414\n",
      "5552\n",
      "Duplicate or existing entry found for episode 5090 under Host method.\n",
      "Duplicate or existing entry found for episode 81639 under Host method.\n",
      "Host 19\n",
      "47978\n",
      "10247\n",
      "Host 171\n",
      "161\n",
      "0\n",
      "Host 102\n",
      "3038\n",
      "1132\n",
      "Duplicate or existing entry found for episode 1912 under Host method.\n",
      "Duplicate or existing entry found for episode 2824 under Host method.\n",
      "Duplicate or existing entry found for episode 5084 under Host method.\n",
      "Duplicate or existing entry found for episode 31103 under Host method.\n",
      "Duplicate or existing entry found for episode 54905 under Host method.\n",
      "Duplicate or existing entry found for episode 71505 under Host method.\n",
      "Duplicate or existing entry found for episode 72640 under Host method.\n",
      "Duplicate or existing entry found for episode 72657 under Host method.\n",
      "Duplicate or existing entry found for episode 72999 under Host method.\n",
      "Duplicate or existing entry found for episode 73769 under Host method.\n",
      "Duplicate or existing entry found for episode 74881 under Host method.\n",
      "Duplicate or existing entry found for episode 75269 under Host method.\n",
      "Duplicate or existing entry found for episode 110359 under Host method.\n",
      "Duplicate or existing entry found for episode 117482 under Host method.\n",
      "Duplicate or existing entry found for episode 120275 under Host method.\n",
      "Duplicate or existing entry found for episode 120653 under Host method.\n",
      "Duplicate or existing entry found for episode 121353 under Host method.\n",
      "Duplicate or existing entry found for episode 124629 under Host method.\n",
      "Host 23\n",
      "31\n",
      "0\n",
      "Host 104\n",
      "1733\n",
      "478\n",
      "Duplicate or existing entry found for episode 105773 under Host method.\n",
      "Duplicate or existing entry found for episode 111903 under Host method.\n",
      "Duplicate or existing entry found for episode 112654 under Host method.\n",
      "Host 15\n",
      "36956\n",
      "8996\n",
      "Duplicate or existing entry found for episode 56526 under Host method.\n",
      "Duplicate or existing entry found for episode 105244 under Host method.\n",
      "Duplicate or existing entry found for episode 115299 under Host method.\n",
      "Host 24\n",
      "4422\n",
      "1643\n",
      "Duplicate or existing entry found for episode 32383 under Host method.\n",
      "Duplicate or existing entry found for episode 34245 under Host method.\n",
      "Duplicate or existing entry found for episode 34349 under Host method.\n",
      "Duplicate or existing entry found for episode 51337 under Host method.\n",
      "Duplicate or existing entry found for episode 51467 under Host method.\n",
      "Duplicate or existing entry found for episode 53564 under Host method.\n",
      "Duplicate or existing entry found for episode 55419 under Host method.\n",
      "Duplicate or existing entry found for episode 56540 under Host method.\n",
      "Duplicate or existing entry found for episode 56961 under Host method.\n",
      "Duplicate or existing entry found for episode 58034 under Host method.\n",
      "Duplicate or existing entry found for episode 58320 under Host method.\n",
      "Duplicate or existing entry found for episode 58447 under Host method.\n",
      "Duplicate or existing entry found for episode 60337 under Host method.\n",
      "Duplicate or existing entry found for episode 61353 under Host method.\n",
      "Duplicate or existing entry found for episode 64107 under Host method.\n",
      "Duplicate or existing entry found for episode 64561 under Host method.\n",
      "Duplicate or existing entry found for episode 64920 under Host method.\n",
      "Duplicate or existing entry found for episode 65071 under Host method.\n",
      "Duplicate or existing entry found for episode 65540 under Host method.\n",
      "Duplicate or existing entry found for episode 66352 under Host method.\n",
      "Host 176\n",
      "457\n",
      "87\n",
      "Host 26\n",
      "0\n",
      "0\n",
      "Host 27\n",
      "37\n",
      "0\n",
      "Host 21\n",
      "8771\n",
      "2102\n",
      "Host 157\n",
      "1061\n",
      "173\n",
      "Duplicate or existing entry found for episode 116490 under Host method.\n",
      "Host 180\n",
      "420\n",
      "237\n",
      "Duplicate or existing entry found for episode 73465 under Host method.\n",
      "Duplicate or existing entry found for episode 124990 under Host method.\n",
      "Duplicate or existing entry found for episode 135583 under Host method.\n",
      "Host 158\n",
      "138\n",
      "45\n",
      "Host 40\n",
      "119\n",
      "57\n",
      "Host 83\n",
      "218\n",
      "41\n",
      "Duplicate or existing entry found for episode 78163 under Host method.\n",
      "Host 29\n",
      "59333\n",
      "13084\n",
      "Duplicate or existing entry found for episode 2145 under Host method.\n",
      "Duplicate or existing entry found for episode 2904 under Host method.\n",
      "Duplicate or existing entry found for episode 32385 under Host method.\n",
      "Duplicate or existing entry found for episode 33960 under Host method.\n",
      "Host 36\n",
      "276\n",
      "151\n",
      "Duplicate or existing entry found for episode 2240 under Host method.\n",
      "Duplicate or existing entry found for episode 55946 under Host method.\n",
      "Host 30\n",
      "0\n",
      "71\n",
      "Duplicate or existing entry found for episode 2493 under Host method.\n",
      "Duplicate or existing entry found for episode 8664 under Host method.\n",
      "Host 22\n",
      "4984\n",
      "1547\n",
      "Host 98\n",
      "3210\n",
      "1044\n",
      "Duplicate or existing entry found for episode 126964 under Host method.\n",
      "Host 34\n",
      "102\n",
      "38\n",
      "Duplicate or existing entry found for episode 75307 under Host method.\n",
      "Host 213\n",
      "995\n",
      "273\n",
      "Duplicate or existing entry found for episode 3369 under Host method.\n",
      "Duplicate or existing entry found for episode 52881 under Host method.\n",
      "Duplicate or existing entry found for episode 69915 under Host method.\n",
      "Duplicate or existing entry found for episode 74034 under Host method.\n",
      "Duplicate or existing entry found for episode 76566 under Host method.\n",
      "Host 125\n",
      "247\n",
      "298\n",
      "Duplicate or existing entry found for episode 117786 under Host method.\n",
      "Duplicate or existing entry found for episode 125135 under Host method.\n",
      "Host 31\n",
      "0\n",
      "0\n",
      "Host 172\n",
      "17831\n",
      "4362\n",
      "Duplicate or existing entry found for episode 117129 under Host method.\n",
      "Duplicate or existing entry found for episode 129159 under Host method.\n",
      "Duplicate or existing entry found for episode 131091 under Host method.\n",
      "Host 39\n",
      "0\n",
      "0\n",
      "Host 251\n",
      "1144\n",
      "346\n",
      "Duplicate or existing entry found for episode 30026 under Host method.\n",
      "Duplicate or existing entry found for episode 35213 under Host method.\n",
      "Duplicate or existing entry found for episode 52697 under Host method.\n",
      "Duplicate or existing entry found for episode 105698 under Host method.\n",
      "Duplicate or existing entry found for episode 135834 under Host method.\n",
      "Host 227\n",
      "721\n",
      "218\n",
      "Duplicate or existing entry found for episode 4007 under Host method.\n",
      "Duplicate or existing entry found for episode 129027 under Host method.\n",
      "Host 41\n",
      "0\n",
      "0\n",
      "Host 42\n",
      "12256\n",
      "2854\n",
      "Duplicate or existing entry found for episode 111478 under Host method.\n",
      "Duplicate or existing entry found for episode 134830 under Host method.\n",
      "Duplicate or existing entry found for episode 135839 under Host method.\n",
      "Host 181\n",
      "182\n",
      "0\n",
      "Host 114\n",
      "388\n",
      "89\n",
      "Duplicate or existing entry found for episode 114198 under Host method.\n",
      "Duplicate or existing entry found for episode 132284 under Host method.\n",
      "Host 185\n",
      "729\n",
      "210\n",
      "Duplicate or existing entry found for episode 5375 under Host method.\n",
      "Duplicate or existing entry found for episode 113596 under Host method.\n",
      "Duplicate or existing entry found for episode 115770 under Host method.\n",
      "Duplicate or existing entry found for episode 133319 under Host method.\n",
      "Duplicate or existing entry found for episode 134312 under Host method.\n",
      "Host 28\n",
      "19365\n",
      "4717\n",
      "Duplicate or existing entry found for episode 8145 under Host method.\n",
      "Duplicate or existing entry found for episode 8380 under Host method.\n",
      "Duplicate or existing entry found for episode 31287 under Host method.\n",
      "Duplicate or existing entry found for episode 32149 under Host method.\n",
      "Duplicate or existing entry found for episode 49325 under Host method.\n",
      "Duplicate or existing entry found for episode 54286 under Host method.\n",
      "Duplicate or existing entry found for episode 57691 under Host method.\n",
      "Duplicate or existing entry found for episode 62697 under Host method.\n",
      "Host 44\n",
      "577\n",
      "159\n",
      "Duplicate or existing entry found for episode 70885 under Host method.\n",
      "Duplicate or existing entry found for episode 128714 under Host method.\n",
      "Host 45\n",
      "0\n",
      "0\n",
      "Host 25\n",
      "140\n",
      "44\n",
      "Host 47\n",
      "12521\n",
      "2878\n",
      "Duplicate or existing entry found for episode 116813 under Host method.\n",
      "Host 48\n",
      "0\n",
      "0\n",
      "Host 20\n",
      "0\n",
      "0\n",
      "Host 49\n",
      "0\n",
      "0\n",
      "Host 50\n",
      "0\n",
      "0\n",
      "Host 199\n",
      "717\n",
      "90\n",
      "Duplicate or existing entry found for episode 65854 under Host method.\n",
      "Duplicate or existing entry found for episode 122295 under Host method.\n",
      "Host 242\n",
      "123\n",
      "41\n",
      "Duplicate or existing entry found for episode 9102 under Host method.\n",
      "Host 216\n",
      "0\n",
      "0\n",
      "Host 234\n",
      "0\n",
      "109\n",
      "Duplicate or existing entry found for episode 9220 under Host method.\n",
      "Duplicate or existing entry found for episode 105982 under Host method.\n",
      "Host 109\n",
      "66\n",
      "0\n",
      "Host 52\n",
      "1008\n",
      "309\n",
      "Duplicate or existing entry found for episode 67260 under Host method.\n",
      "Duplicate or existing entry found for episode 68357 under Host method.\n",
      "Duplicate or existing entry found for episode 127619 under Host method.\n",
      "Host 116\n",
      "90\n",
      "0\n",
      "Host 53\n",
      "284\n",
      "0\n",
      "Host 54\n",
      "570\n",
      "174\n",
      "Duplicate or existing entry found for episode 60889 under Host method.\n",
      "Duplicate or existing entry found for episode 116486 under Host method.\n",
      "Duplicate or existing entry found for episode 119173 under Host method.\n",
      "Host 55\n",
      "50\n",
      "34\n",
      "Duplicate or existing entry found for episode 64838 under Host method.\n",
      "Host 43\n",
      "17537\n",
      "4400\n",
      "Duplicate or existing entry found for episode 125889 under Host method.\n",
      "Duplicate or existing entry found for episode 129443 under Host method.\n",
      "Duplicate or existing entry found for episode 131926 under Host method.\n",
      "Duplicate or existing entry found for episode 132065 under Host method.\n",
      "Duplicate or existing entry found for episode 132111 under Host method.\n",
      "Duplicate or existing entry found for episode 136698 under Host method.\n",
      "Host 58\n",
      "0\n",
      "0\n",
      "Host 51\n",
      "39\n",
      "0\n",
      "Host 59\n",
      "80\n",
      "0\n",
      "Host 32\n",
      "0\n",
      "0\n",
      "Host 111\n",
      "159\n",
      "0\n",
      "Host 62\n",
      "0\n",
      "0\n",
      "Host 265\n",
      "602\n",
      "81\n",
      "Duplicate or existing entry found for episode 108703 under Host method.\n",
      "Host 67\n",
      "0\n",
      "0\n",
      "Host 131\n",
      "822\n",
      "308\n",
      "Duplicate or existing entry found for episode 32972 under Host method.\n",
      "Duplicate or existing entry found for episode 33962 under Host method.\n",
      "Duplicate or existing entry found for episode 50490 under Host method.\n",
      "Duplicate or existing entry found for episode 57209 under Host method.\n",
      "Host 120\n",
      "38285\n",
      "8878\n",
      "Duplicate or existing entry found for episode 52803 under Host method.\n",
      "Duplicate or existing entry found for episode 53181 under Host method.\n",
      "Duplicate or existing entry found for episode 116875 under Host method.\n",
      "Duplicate or existing entry found for episode 119739 under Host method.\n",
      "Duplicate or existing entry found for episode 120025 under Host method.\n",
      "Duplicate or existing entry found for episode 120440 under Host method.\n",
      "Duplicate or existing entry found for episode 121278 under Host method.\n",
      "Duplicate or existing entry found for episode 122102 under Host method.\n",
      "Duplicate or existing entry found for episode 125702 under Host method.\n",
      "Duplicate or existing entry found for episode 130748 under Host method.\n",
      "Duplicate or existing entry found for episode 130751 under Host method.\n",
      "Host 87\n",
      "37\n",
      "0\n",
      "Host 73\n",
      "39\n",
      "0\n",
      "Host 76\n",
      "162\n",
      "45\n",
      "Duplicate or existing entry found for episode 69587 under Host method.\n",
      "Host 75\n",
      "0\n",
      "0\n",
      "Host 161\n",
      "38\n",
      "0\n",
      "Host 60\n",
      "182\n",
      "68\n",
      "Duplicate or existing entry found for episode 116931 under Host method.\n",
      "Duplicate or existing entry found for episode 129425 under Host method.\n",
      "Host 33\n",
      "665\n",
      "151\n",
      "Duplicate or existing entry found for episode 49494 under Host method.\n",
      "Duplicate or existing entry found for episode 57208 under Host method.\n",
      "Duplicate or existing entry found for episode 114903 under Host method.\n",
      "Duplicate or existing entry found for episode 126596 under Host method.\n",
      "Host 80\n",
      "0\n",
      "0\n",
      "Host 82\n",
      "0\n",
      "0\n",
      "Host 219\n",
      "685\n",
      "171\n",
      "Duplicate or existing entry found for episode 79598 under Host method.\n",
      "Duplicate or existing entry found for episode 113838 under Host method.\n",
      "Duplicate or existing entry found for episode 126770 under Host method.\n",
      "Duplicate or existing entry found for episode 134620 under Host method.\n",
      "Host 84\n",
      "71\n",
      "0\n",
      "Host 200\n",
      "98\n",
      "148\n",
      "Duplicate or existing entry found for episode 68130 under Host method.\n",
      "Duplicate or existing entry found for episode 79574 under Host method.\n",
      "Host 85\n",
      "0\n",
      "0\n",
      "Host 89\n",
      "588\n",
      "139\n",
      "Duplicate or existing entry found for episode 106168 under Host method.\n",
      "Duplicate or existing entry found for episode 114557 under Host method.\n",
      "Duplicate or existing entry found for episode 124349 under Host method.\n",
      "Host 91\n",
      "15647\n",
      "3589\n",
      "Host 92\n",
      "20482\n",
      "5955\n",
      "Host 93\n",
      "0\n",
      "0\n",
      "Host 217\n",
      "248\n",
      "71\n",
      "Duplicate or existing entry found for episode 56387 under Host method.\n",
      "Duplicate or existing entry found for episode 120752 under Host method.\n",
      "Host 78\n",
      "9630\n",
      "2355\n",
      "Duplicate or existing entry found for episode 80531 under Host method.\n",
      "Duplicate or existing entry found for episode 80532 under Host method.\n",
      "Duplicate or existing entry found for episode 130232 under Host method.\n",
      "Duplicate or existing entry found for episode 133450 under Host method.\n",
      "Host 99\n",
      "0\n",
      "0\n",
      "Host 197\n",
      "358\n",
      "85\n",
      "Duplicate or existing entry found for episode 107175 under Host method.\n",
      "Duplicate or existing entry found for episode 125908 under Host method.\n",
      "Host 72\n",
      "0\n",
      "0\n",
      "Host 140\n",
      "76\n",
      "0\n",
      "Host 220\n",
      "80\n",
      "144\n",
      "Duplicate or existing entry found for episode 63879 under Host method.\n",
      "Duplicate or existing entry found for episode 76270 under Host method.\n",
      "Duplicate or existing entry found for episode 78821 under Host method.\n",
      "Duplicate or existing entry found for episode 117388 under Host method.\n",
      "Host 110\n",
      "195\n",
      "122\n",
      "Duplicate or existing entry found for episode 73677 under Host method.\n",
      "Duplicate or existing entry found for episode 81197 under Host method.\n",
      "Duplicate or existing entry found for episode 107248 under Host method.\n",
      "Host 204\n",
      "0\n",
      "69\n",
      "Duplicate or existing entry found for episode 66198 under Host method.\n",
      "Host 124\n",
      "1485\n",
      "485\n",
      "Duplicate or existing entry found for episode 66215 under Host method.\n",
      "Duplicate or existing entry found for episode 66786 under Host method.\n",
      "Duplicate or existing entry found for episode 68140 under Host method.\n",
      "Duplicate or existing entry found for episode 68847 under Host method.\n",
      "Duplicate or existing entry found for episode 69902 under Host method.\n",
      "Duplicate or existing entry found for episode 104390 under Host method.\n",
      "Duplicate or existing entry found for episode 108506 under Host method.\n",
      "Duplicate or existing entry found for episode 121996 under Host method.\n",
      "Host 108\n",
      "373\n",
      "81\n",
      "Duplicate or existing entry found for episode 115014 under Host method.\n",
      "Duplicate or existing entry found for episode 136742 under Host method.\n",
      "Host 115\n",
      "11908\n",
      "3295\n",
      "Duplicate or existing entry found for episode 108205 under Host method.\n",
      "Duplicate or existing entry found for episode 114425 under Host method.\n",
      "Host 179\n",
      "217\n",
      "113\n",
      "Duplicate or existing entry found for episode 67713 under Host method.\n",
      "Duplicate or existing entry found for episode 106260 under Host method.\n",
      "Duplicate or existing entry found for episode 106967 under Host method.\n",
      "Host 271\n",
      "146\n",
      "0\n",
      "Host 205\n",
      "0\n",
      "0\n",
      "Host 70\n",
      "97\n",
      "82\n",
      "Duplicate or existing entry found for episode 77034 under Host method.\n",
      "Duplicate or existing entry found for episode 127353 under Host method.\n",
      "Host 117\n",
      "14221\n",
      "3369\n",
      "Duplicate or existing entry found for episode 68119 under Host method.\n",
      "Duplicate or existing entry found for episode 69038 under Host method.\n",
      "Duplicate or existing entry found for episode 70445 under Host method.\n",
      "Duplicate or existing entry found for episode 71822 under Host method.\n",
      "Duplicate or existing entry found for episode 71846 under Host method.\n",
      "Duplicate or existing entry found for episode 72793 under Host method.\n",
      "Duplicate or existing entry found for episode 73284 under Host method.\n",
      "Duplicate or existing entry found for episode 74270 under Host method.\n",
      "Duplicate or existing entry found for episode 77785 under Host method.\n",
      "Host 94\n",
      "0\n",
      "0\n",
      "Host 105\n",
      "47462\n",
      "12874\n",
      "Duplicate or existing entry found for episode 121573 under Host method.\n",
      "Duplicate or existing entry found for episode 121631 under Host method.\n",
      "Duplicate or existing entry found for episode 123391 under Host method.\n",
      "Duplicate or existing entry found for episode 127032 under Host method.\n",
      "Duplicate or existing entry found for episode 128798 under Host method.\n",
      "Duplicate or existing entry found for episode 134969 under Host method.\n",
      "Duplicate or existing entry found for episode 136479 under Host method.\n",
      "Duplicate or existing entry found for episode 137275 under Host method.\n",
      "Host 126\n",
      "0\n",
      "0\n",
      "Host 79\n",
      "0\n",
      "0\n",
      "Host 128\n",
      "765\n",
      "68\n",
      "Duplicate or existing entry found for episode 116779 under Host method.\n",
      "Duplicate or existing entry found for episode 126952 under Host method.\n",
      "Host 132\n",
      "0\n",
      "0\n",
      "Host 96\n",
      "0\n",
      "0\n",
      "Host 143\n",
      "83\n",
      "0\n",
      "Host 133\n",
      "276\n",
      "43\n",
      "Duplicate or existing entry found for episode 132644 under Host method.\n",
      "Host 136\n",
      "0\n",
      "0\n",
      "Host 138\n",
      "0\n",
      "0\n",
      "Host 139\n",
      "0\n",
      "0\n",
      "Host 215\n",
      "106\n",
      "0\n",
      "Host 191\n",
      "428\n",
      "0\n",
      "Host 63\n",
      "0\n",
      "0\n",
      "Host 146\n",
      "0\n",
      "0\n",
      "Host 77\n",
      "174\n",
      "0\n",
      "Host 147\n",
      "0\n",
      "0\n",
      "Host 148\n",
      "0\n",
      "0\n",
      "Host 151\n",
      "0\n",
      "0\n",
      "Host 152\n",
      "0\n",
      "0\n",
      "Host 162\n",
      "0\n",
      "0\n",
      "Host 212\n",
      "153\n",
      "49\n",
      "Duplicate or existing entry found for episode 93370 under Host method.\n",
      "Host 164\n",
      "0\n",
      "0\n",
      "Host 95\n",
      "49\n",
      "0\n",
      "Host 166\n",
      "0\n",
      "0\n",
      "Host 167\n",
      "0\n",
      "0\n",
      "Host 209\n",
      "33\n",
      "130\n",
      "Duplicate or existing entry found for episode 121461 under Host method.\n",
      "Duplicate or existing entry found for episode 121695 under Host method.\n",
      "Host 175\n",
      "0\n",
      "0\n",
      "Host 195\n",
      "0\n",
      "0\n",
      "Host 210\n",
      "0\n",
      "0\n",
      "Host 112\n",
      "0\n",
      "0\n",
      "Host 90\n",
      "0\n",
      "0\n",
      "Host 224\n",
      "0\n",
      "0\n",
      "Host 236\n",
      "0\n",
      "0\n",
      "Host 218\n",
      "0\n",
      "0\n",
      "Host 121\n",
      "99\n",
      "0\n",
      "Host 61\n",
      "0\n",
      "0\n",
      "Host 253\n",
      "66\n",
      "0\n",
      "Host 165\n",
      "0\n",
      "0\n",
      "Host 201\n",
      "60\n",
      "0\n",
      "Host 239\n",
      "0\n",
      "0\n",
      "Host 232\n",
      "0\n",
      "0\n",
      "Host 198\n",
      "0\n",
      "0\n",
      "Host 261\n",
      "0\n",
      "0\n",
      "Host 245\n",
      "0\n",
      "0\n",
      "Host 153\n",
      "0\n",
      "0\n",
      "Host 281\n",
      "0\n",
      "0\n",
      "Host 254\n",
      "0\n",
      "0\n",
      "Host 267\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "results_dict = {episode: [] for episode in episode_embedding_df[episode_embedding_df['episode'].isin(test_episodes)]['episode'].unique()}\n",
    "\n",
    "# Iterate through each unique test cluster label\n",
    "for cluster in set(test_cluster_labels):\n",
    "    print(f\"Cluster {cluster}\")\n",
    "    # Get the embeddings of the cluster\n",
    "    cluster_embeddings = test_cluster_to_embedding[cluster]\n",
    "    # Get the episode ids of the test episodes in the cluster\n",
    "    cluster_test_episodes = test_cluster_to_episode[cluster]\n",
    "\n",
    "    # Get the episode ids of the train episodes in the cluster\n",
    "    cluster_train_episodes = train_cluster_to_episode[cluster]\n",
    "\n",
    "    # Filter the utterance df to only include the episodes in the cluster for training and testing\n",
    "    filtered_df = utterance_df[utterance_df['episode'].isin(cluster_train_episodes + cluster_test_episodes)][['is_host', 'episode', 'utterance']]\n",
    "\n",
    "    # Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "    words_series = filtered_df['utterance'].str.split().explode()\n",
    "    unique_words = set(words_series)\n",
    "    word_frequencies = words_series.value_counts().to_dict()\n",
    "    word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "    # Filter the filter df to only include the episodes in the cluster for training and testing\n",
    "    training_df = filtered_df[filtered_df['episode'].isin(cluster_train_episodes)]\n",
    "    testing_df = filtered_df[filtered_df['episode'].isin(cluster_test_episodes)]\n",
    "\n",
    "    print(len(training_df))\n",
    "    print(len(testing_df))\n",
    "\n",
    "    # Get the transition matrix for the training data\n",
    "    transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(training_df[training_df['is_host'] == True], word_indices, len(unique_words))\n",
    "    transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(training_df[training_df['is_host'] == False], word_indices, len(unique_words))\n",
    "\n",
    "    # Get the tensor for the training data\n",
    "    tensor, word_to_index = tensor_trouble(training_df, word_indices, len(unique_words))\n",
    "\n",
    "    # Normalize the tensor\n",
    "    tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "    # Get the emission matrix for the training data\n",
    "    emission_matrix = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "    for episode in cluster_test_episodes:\n",
    "        test_episode_df = testing_df[testing_df['episode'] == episode]\n",
    "        test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "        test_episode_df = test_episode_df.explode('utterance')\n",
    "        test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "        test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "        test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "        test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "        obs = [0] + test_episode_word_indices  # Add start token index (e.g., 0) if your Viterbi expects it\n",
    "\n",
    "\n",
    "\n",
    "        state_sequence = tensor_viterbi(obs, tensor_normalized, emission_matrix, initial_state_probabilities)\n",
    "        # print(len(state_sequence))\n",
    "        test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "        # Now, calculate the accuracy\n",
    "        accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "\n",
    "        # Switch the labels if accuracy is less than 0.5\n",
    "        if accuracy < 0.5:\n",
    "            accuracy = 1 - accuracy\n",
    "            state_sequence_padded = 1 - state_sequence_padded\n",
    "        \n",
    "        assume_0 = (test_label_padded == 1).astype(int)\n",
    "        assume_0_accuracy = np.mean(assume_0)\n",
    "\n",
    "        # Switch the labels if accuracy is less than 0.5\n",
    "        if assume_0_accuracy < 0.5:\n",
    "            assume_0_accuracy = 1 - assume_0_accuracy\n",
    "            assume_0 = 1 - assume_0\n",
    "\n",
    "        # Calculate the confusion matrices\n",
    "        confusion = confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "        assume_0_confusion = confusion_matrix(test_label_padded, assume_0)\n",
    "\n",
    "        # Append the confusion matrices to the results dictionary\n",
    "        results_dict[episode].append({\n",
    "            'accuracy': accuracy,\n",
    "            'confusion': confusion,\n",
    "            'assume_0_accuracy': assume_0_accuracy,\n",
    "            'assume_0_confusion': assume_0_confusion,\n",
    "            'method': 'Clustering'\n",
    "        })\n",
    "\n",
    "for host_id in utterance_df['host_id'].unique():\n",
    "    print(f\"Host {host_id}\")\n",
    "    # Filter the utterance and embedding dataframes for this specific host\n",
    "    host_utterance_df = filter_episodes(utterance_df, host_id=host_id)\n",
    "\n",
    "    # Get the episodes related to this host\n",
    "    host_episodes = host_utterance_df['episode'].unique()\n",
    "\n",
    "    # Process the utterances as before\n",
    "    words_series = host_utterance_df['utterance'].str.split().explode()\n",
    "    unique_words = set(words_series)\n",
    "    word_frequencies = words_series.value_counts().to_dict()\n",
    "    word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "    \n",
    "    \n",
    "\n",
    "    training_df = host_utterance_df[host_utterance_df['episode'].isin(train_episodes)]\n",
    "    print(len(training_df))\n",
    "    testing_df = host_utterance_df[host_utterance_df['episode'].isin(test_episodes)]\n",
    "    print(len(testing_df))\n",
    "\n",
    "    # Calculate transition matrices and tensor for training data\n",
    "    transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(training_df[training_df['is_host'] == True], word_indices, len(unique_words))\n",
    "    transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(training_df[training_df['is_host'] == False], word_indices, len(unique_words))\n",
    "    tensor, word_to_index = tensor_trouble(training_df, word_indices, len(unique_words))\n",
    "    tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "    # Emission matrix for training data\n",
    "    emission_matrix = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "\n",
    "    for episode in testing_df['episode'].unique():\n",
    "        test_episode_df = testing_df[testing_df['episode'] == episode][['is_host', 'utterance']]\n",
    "        test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "        test_episode_df = test_episode_df.explode('utterance')\n",
    "        test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "        test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "        test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "        test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "        if not test_episode_word_indices:  # If the list is empty, skip to the next iteration\n",
    "            print(f\"No words processed for episode {episode}, skipping...\")\n",
    "            continue\n",
    "        obs = [0] + test_episode_word_indices  # Include start token\n",
    "\n",
    "        state_sequence = tensor_viterbi(obs, tensor_normalized, emission_matrix, initial_state_probabilities)\n",
    "        test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "        accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "\n",
    "        if accuracy < 0.5:\n",
    "            accuracy = 1 - accuracy\n",
    "            state_sequence_padded = 1 - state_sequence_padded\n",
    "\n",
    "        assume_0 = (test_label_padded == 1).astype(int)\n",
    "        assume_0_accuracy = np.mean(assume_0)\n",
    "\n",
    "        if assume_0_accuracy < 0.5:\n",
    "            assume_0_accuracy = 1 - assume_0_accuracy\n",
    "            assume_0 = 1 - assume_0\n",
    "\n",
    "        confusion = confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "        assume_0_confusion = confusion_matrix(test_label_padded, assume_0)\n",
    "\n",
    "        if episode not in results_dict or 'Host' not in [result['method'] for result in results_dict[episode]]:\n",
    "            results_dict[episode].append({\n",
    "                'accuracy': accuracy,\n",
    "                'confusion': confusion,\n",
    "                'assume_0_accuracy': assume_0_accuracy,\n",
    "                'assume_0_confusion': assume_0_confusion,\n",
    "                'method': 'Host'\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Duplicate or existing entry found for episode {episode} under Host method.\")\n",
    "\n",
    "# Save the results dictionary to a file\n",
    "with open('archive/results_dict.json', 'w') as f:\n",
    "    f.write(str(results_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = results_dict.keys()\n",
    "cluster_confusion = np.array([[0, 0], [0, 0]])\n",
    "host_confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "cluster_accuracy = 0\n",
    "host_accuracy = 0\n",
    "\n",
    "for e in eps:\n",
    "    cluster = results_dict[e][0]\n",
    "    if len(results_dict[e]) > 1:\n",
    "        host = results_dict[e][1]\n",
    "        host_confusion += host['confusion']\n",
    "        host_accuracy += host['accuracy']\n",
    "\n",
    "    cluster_confusion += cluster['confusion']\n",
    "    cluster_accuracy += cluster['accuracy']\n",
    "# results_dict[421][0]['confusion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster accuracy: 0.7724927742299321\n",
      "Host accuracy: 0.6200589370405237\n",
      "Cluster confusion matrix: [[0.68894026 0.01399151]\n",
      " [0.22163485 0.07543338]]\n",
      "Host confusion matrix: [[0.68463869 0.01546578]\n",
      " [0.22365618 0.07623935]]\n"
     ]
    }
   ],
   "source": [
    "# normalize the confusion matrices\n",
    "cluster_confusion = cluster_confusion / cluster_confusion.sum()\n",
    "host_confusion = host_confusion / host_confusion.sum()\n",
    "\n",
    "print(f\"Cluster accuracy: {cluster_accuracy / len(eps)}\")\n",
    "print(f\"Host accuracy: {host_accuracy / len(eps)}\")\n",
    "print(f\"Cluster confusion matrix: {cluster_confusion}\")\n",
    "print(f\"Host confusion matrix: {host_confusion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
