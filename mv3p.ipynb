{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hosts: 32\n",
      "Host ID: 0\n",
      "Number of episodes: 309\n",
      "Number of utterances: 16770\n",
      "Number of unique words: 8465\n",
      "Average Accuracy: 0.7917487313432423\n",
      "Average Assume 0 Accuracy: 0.7612979921955829\n",
      "Variance of Accuracy: 0.004104496459254812\n",
      "Variance of Assume 0 Accuracy: 0.0054934787426084925\n",
      "Host ID: 1\n",
      "Number of episodes: 865\n",
      "Number of utterances: 42497\n",
      "Number of unique words: 12071\n",
      "Average Accuracy: 0.7679164191821449\n",
      "Average Assume 0 Accuracy: 0.7200589101993823\n",
      "Variance of Accuracy: 0.008089686746889502\n",
      "Variance of Assume 0 Accuracy: 0.009269286067876461\n",
      "Host ID: 5\n",
      "Number of episodes: 897\n",
      "Number of utterances: 40424\n",
      "Number of unique words: 12054\n",
      "Average Accuracy: 0.7700893449521964\n",
      "Average Assume 0 Accuracy: 0.738697375682983\n",
      "Variance of Accuracy: 0.008703079825261624\n",
      "Variance of Assume 0 Accuracy: 0.008851673674149303\n",
      "Host ID: 7\n",
      "Number of episodes: 464\n",
      "Number of utterances: 42337\n",
      "Number of unique words: 11948\n",
      "Average Accuracy: 0.7483269364102682\n",
      "Average Assume 0 Accuracy: 0.7038002640396046\n",
      "Variance of Accuracy: 0.00797276948430464\n",
      "Variance of Assume 0 Accuracy: 0.009728613454346294\n",
      "Host ID: 8\n",
      "Number of episodes: 463\n",
      "Number of utterances: 49928\n",
      "Number of unique words: 10089\n",
      "Average Accuracy: 0.7758635336037202\n",
      "Average Assume 0 Accuracy: 0.6970413217950286\n",
      "Variance of Accuracy: 0.00466390683513198\n",
      "Variance of Assume 0 Accuracy: 0.00580206295635051\n",
      "Host ID: 9\n",
      "Number of episodes: 597\n",
      "Number of utterances: 30006\n",
      "Number of unique words: 10696\n",
      "Average Accuracy: 0.7774029325234609\n",
      "Average Assume 0 Accuracy: 0.747979602247623\n",
      "Variance of Accuracy: 0.006141119355580688\n",
      "Variance of Assume 0 Accuracy: 0.007136665920039809\n",
      "Host ID: 10\n",
      "Number of episodes: 351\n",
      "Number of utterances: 18751\n",
      "Number of unique words: 8919\n",
      "Average Accuracy: 0.7415739057376434\n",
      "Average Assume 0 Accuracy: 0.6934947658855952\n",
      "Variance of Accuracy: 0.005980467824261063\n",
      "Variance of Assume 0 Accuracy: 0.005974798932522307\n",
      "Host ID: 11\n",
      "Number of episodes: 764\n",
      "Number of utterances: 35802\n",
      "Number of unique words: 11502\n",
      "Average Accuracy: 0.7915607650424527\n",
      "Average Assume 0 Accuracy: 0.750823963977172\n",
      "Variance of Accuracy: 0.006602805466679293\n",
      "Variance of Assume 0 Accuracy: 0.007740279442997236\n",
      "Host ID: 12\n",
      "Number of episodes: 1685\n",
      "Number of utterances: 84260\n",
      "Number of unique words: 13672\n",
      "Average Accuracy: 0.771731449008788\n",
      "Average Assume 0 Accuracy: 0.7321847247598902\n",
      "Variance of Accuracy: 0.006531960330605404\n",
      "Variance of Assume 0 Accuracy: 0.007594895204306942\n",
      "Host ID: 13\n",
      "Number of episodes: 230\n",
      "Number of utterances: 16246\n",
      "Number of unique words: 8509\n",
      "Average Accuracy: 0.7706158508944796\n",
      "Average Assume 0 Accuracy: 0.7236754017614736\n",
      "Variance of Accuracy: 0.008666041661995822\n",
      "Variance of Assume 0 Accuracy: 0.01141930714077513\n",
      "Host ID: 14\n",
      "Number of episodes: 577\n",
      "Number of utterances: 31204\n",
      "Number of unique words: 10671\n",
      "Average Accuracy: 0.7332689766525416\n",
      "Average Assume 0 Accuracy: 0.6891630519517282\n",
      "Variance of Accuracy: 0.007125399742075711\n",
      "Variance of Assume 0 Accuracy: 0.00813304012612896\n",
      "Host ID: 15\n",
      "Number of episodes: 822\n",
      "Number of utterances: 45952\n",
      "Number of unique words: 11869\n",
      "Average Accuracy: 0.7684910053117192\n",
      "Average Assume 0 Accuracy: 0.7240648169427029\n",
      "Variance of Accuracy: 0.0074252584519805065\n",
      "Variance of Assume 0 Accuracy: 0.008346404345799988\n",
      "Host ID: 16\n",
      "Number of episodes: 1164\n",
      "Number of utterances: 61862\n",
      "Number of unique words: 12722\n",
      "Average Accuracy: 0.7468571520499868\n",
      "Average Assume 0 Accuracy: 0.7166755281348993\n",
      "Variance of Accuracy: 0.009695766324840436\n",
      "Variance of Assume 0 Accuracy: 0.010327552748728172\n",
      "Host ID: 17\n",
      "Number of episodes: 685\n",
      "Number of utterances: 31881\n",
      "Number of unique words: 10702\n",
      "Average Accuracy: 0.799968988967242\n",
      "Average Assume 0 Accuracy: 0.7577790033551306\n",
      "Variance of Accuracy: 0.007196373387077671\n",
      "Variance of Assume 0 Accuracy: 0.00881420244099439\n",
      "Host ID: 18\n",
      "Number of episodes: 202\n",
      "Number of utterances: 8971\n",
      "Number of unique words: 6921\n",
      "Average Accuracy: 0.7782346123424522\n",
      "Average Assume 0 Accuracy: 0.7352039668990747\n",
      "Variance of Accuracy: 0.006289388192483732\n",
      "Variance of Assume 0 Accuracy: 0.010562462155994918\n",
      "Host ID: 19\n",
      "Number of episodes: 1034\n",
      "Number of utterances: 58225\n",
      "Number of unique words: 12152\n",
      "Average Accuracy: 0.7755669273819654\n",
      "Average Assume 0 Accuracy: 0.6754632733145535\n",
      "Variance of Accuracy: 0.006076885994341833\n",
      "Variance of Assume 0 Accuracy: 0.00963290489045625\n",
      "Host ID: 21\n",
      "Number of episodes: 216\n",
      "Number of utterances: 10873\n",
      "Number of unique words: 7271\n",
      "Average Accuracy: 0.7657869735962263\n",
      "Average Assume 0 Accuracy: 0.7338915138047765\n",
      "Variance of Accuracy: 0.007876514109810247\n",
      "Variance of Assume 0 Accuracy: 0.012053877675945197\n",
      "Host ID: 22\n",
      "Number of episodes: 128\n",
      "Number of utterances: 6619\n",
      "Number of unique words: 6087\n",
      "Average Accuracy: 0.7719135597996379\n",
      "Average Assume 0 Accuracy: 0.7285229092379885\n",
      "Variance of Accuracy: 0.0075492391504402906\n",
      "Variance of Assume 0 Accuracy: 0.009831817531479088\n",
      "Host ID: 28\n",
      "Number of episodes: 524\n",
      "Number of utterances: 24121\n",
      "Number of unique words: 9936\n",
      "Average Accuracy: 0.7507381796763651\n",
      "Average Assume 0 Accuracy: 0.6976547140076675\n",
      "Variance of Accuracy: 0.00903101920745757\n",
      "Variance of Assume 0 Accuracy: 0.009868251367135653\n",
      "Host ID: 29\n",
      "Number of episodes: 1039\n",
      "Number of utterances: 72462\n",
      "Number of unique words: 12549\n",
      "Average Accuracy: 0.7743465094400472\n",
      "Average Assume 0 Accuracy: 0.7296222922741644\n",
      "Variance of Accuracy: 0.009148014610574589\n",
      "Variance of Assume 0 Accuracy: 0.009677252427116013\n",
      "Host ID: 42\n",
      "Number of episodes: 280\n",
      "Number of utterances: 15110\n",
      "Number of unique words: 9040\n",
      "Average Accuracy: 0.787113197253093\n",
      "Average Assume 0 Accuracy: 0.7628399666169414\n",
      "Variance of Accuracy: 0.007948283936776534\n",
      "Variance of Assume 0 Accuracy: 0.010015931321017144\n",
      "Host ID: 43\n",
      "Number of episodes: 385\n",
      "Number of utterances: 22002\n",
      "Number of unique words: 10391\n",
      "Average Accuracy: 0.739386689308906\n",
      "Average Assume 0 Accuracy: 0.6898058598822807\n",
      "Variance of Accuracy: 0.0064966042548786265\n",
      "Variance of Assume 0 Accuracy: 0.007733351248869906\n",
      "Host ID: 47\n",
      "Number of episodes: 334\n",
      "Number of utterances: 15441\n",
      "Number of unique words: 9199\n",
      "Average Accuracy: 0.7656512283249616\n",
      "Average Assume 0 Accuracy: 0.7299988386709771\n",
      "Variance of Accuracy: 0.006621780313954219\n",
      "Variance of Assume 0 Accuracy: 0.00818490659567247\n",
      "Host ID: 78\n",
      "Number of episodes: 242\n",
      "Number of utterances: 12157\n",
      "Number of unique words: 8045\n",
      "Average Accuracy: 0.7566436687584166\n",
      "Average Assume 0 Accuracy: 0.7272193639982854\n",
      "Variance of Accuracy: 0.007364128211750513\n",
      "Variance of Assume 0 Accuracy: 0.007640077271744113\n",
      "Host ID: 91\n",
      "Number of episodes: 310\n",
      "Number of utterances: 19236\n",
      "Number of unique words: 9133\n",
      "Average Accuracy: 0.7757859451277623\n",
      "Average Assume 0 Accuracy: 0.7253254952502728\n",
      "Variance of Accuracy: 0.011020002680870682\n",
      "Variance of Assume 0 Accuracy: 0.014457079636399363\n",
      "Host ID: 92\n",
      "Number of episodes: 512\n",
      "Number of utterances: 26608\n",
      "Number of unique words: 10630\n",
      "Average Accuracy: 0.7505595286198316\n",
      "Average Assume 0 Accuracy: 0.7244219149177327\n",
      "Variance of Accuracy: 0.007731612381288908\n",
      "Variance of Assume 0 Accuracy: 0.008280873545267846\n",
      "Host ID: 98\n",
      "Number of episodes: 94\n",
      "Number of utterances: 4287\n",
      "Number of unique words: 5146\n",
      "Average Accuracy: 0.7779865001122277\n",
      "Average Assume 0 Accuracy: 0.7512825618265585\n",
      "Variance of Accuracy: 0.004073936374793999\n",
      "Variance of Assume 0 Accuracy: 0.006578461115412781\n",
      "Host ID: 105\n",
      "Number of episodes: 1277\n",
      "Number of utterances: 60380\n",
      "Number of unique words: 12994\n",
      "Average Accuracy: 0.7442536319550788\n",
      "Average Assume 0 Accuracy: 0.6941040489321001\n",
      "Variance of Accuracy: 0.009577881590896911\n",
      "Variance of Assume 0 Accuracy: 0.0110739143256436\n",
      "Host ID: 115\n",
      "Number of episodes: 297\n",
      "Number of utterances: 15203\n",
      "Number of unique words: 8247\n",
      "Average Accuracy: 0.7690350763689433\n",
      "Average Assume 0 Accuracy: 0.7422712921792738\n",
      "Variance of Accuracy: 0.009936056074015865\n",
      "Variance of Assume 0 Accuracy: 0.009603669655060515\n",
      "Host ID: 117\n",
      "Number of episodes: 356\n",
      "Number of utterances: 17763\n",
      "Number of unique words: 9469\n",
      "Average Accuracy: 0.7790314082810894\n",
      "Average Assume 0 Accuracy: 0.7372301094696043\n",
      "Variance of Accuracy: 0.007919640410559769\n",
      "Variance of Assume 0 Accuracy: 0.010613665553687219\n",
      "Host ID: 120\n",
      "Number of episodes: 751\n",
      "Number of utterances: 47163\n",
      "Number of unique words: 12192\n",
      "Average Accuracy: 0.7266966627540009\n",
      "Average Assume 0 Accuracy: 0.663926503399323\n",
      "Variance of Accuracy: 0.007414761415016434\n",
      "Variance of Assume 0 Accuracy: 0.009864051844676614\n",
      "Host ID: 172\n",
      "Number of episodes: 459\n",
      "Number of utterances: 22193\n",
      "Number of unique words: 10139\n",
      "Average Accuracy: 0.7469824916100112\n",
      "Average Assume 0 Accuracy: 0.7053211345423985\n",
      "Variance of Accuracy: 0.009036463991590905\n",
      "Variance of Assume 0 Accuracy: 0.009072078420037927\n",
      "Weighted Average Accuracy: 0.7639531074740017\n",
      "Weighted Average Assume 0 Accuracy: 0.7170610260931807\n",
      "Total Episodes: 18313\n",
      "Total Words: 14928569\n"
     ]
    }
   ],
   "source": [
    "# Load the spacy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_threshold = 25\n",
    "\n",
    "\n",
    "def aggressive_cleaning(text):\n",
    "    \"\"\"\n",
    "    Cleans the text by handling NaN values and 'nan' strings, normalizing dashes, lowercasing,\n",
    "    and removing non-alphabetic characters except spaces.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    if text.lower() == 'nan' or pd.isna(text):\n",
    "        return ''\n",
    "    else:\n",
    "        text = text.replace('-', ' ').lower().strip()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "\n",
    "def tokenize_and_lemmatize(text, word_frequencies):\n",
    "    \"\"\"\n",
    "    Tokenizes and lemmatizes the given text, applying a word frequency filter.\n",
    "    Words below the threshold are replaced with 'xxxxx'.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "    filtered_tokens = [word if word_frequencies.get(word, 0) >= word_threshold else 'xxxxx' for word in lemmatized_tokens]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "def preprocess_dataframe(dataframe, batch_size=60000):\n",
    "    \"\"\"\n",
    "    Applies preprocessing to the entire DataFrame, including cleaning text, tokenizing,\n",
    "    lemmatizing, and filtering based on word frequencies.\n",
    "    \"\"\"\n",
    "    # Apply aggressive cleaning\n",
    "    dataframe['utterance'] = dataframe['utterance'].apply(aggressive_cleaning)\n",
    "    \n",
    "    all_words = ' '.join(dataframe['utterance']).split()\n",
    "    word_frequencies = Counter(all_words)\n",
    "    \n",
    "    processed_batches = []\n",
    "    progress = tqdm(total=len(dataframe), desc=\"Processing batches\")\n",
    "    \n",
    "    for start_row in range(0, len(dataframe), batch_size):\n",
    "        end_row = start_row + batch_size\n",
    "        batch = dataframe.iloc[start_row:end_row]\n",
    "        batch['utterance'] = batch['utterance'].apply(lambda x: tokenize_and_lemmatize(x, word_frequencies))\n",
    "        processed_batches.append(batch)\n",
    "        progress.update(len(batch))\n",
    "    \n",
    "    progress.close()\n",
    "    return pd.concat(processed_batches)\n",
    "\n",
    "\n",
    "def filter_episodes(df, host_id=None):\n",
    "    \"\"\"Filter episodes based on the host_id with \n",
    "    the most episodes or a specific host_id.\"\"\"\n",
    "    df_filtered = df[df['host_id'] != -1]\n",
    "    if host_id is None:\n",
    "        top_host = df_filtered.groupby('host_id')['episode'].nunique().idxmax()\n",
    "    else:\n",
    "        top_host = host_id\n",
    "    top_host_episodes = df_filtered[df_filtered['host_id'] == top_host]['episode'].unique()\n",
    "    df_top_host_all_utterances = df[df['episode'].isin(top_host_episodes)]\n",
    "    utterance_counts = df_top_host_all_utterances.groupby('episode')['utterance'].count()\n",
    "    episodes_over_30 = utterance_counts[utterance_counts > 30].index\n",
    "    df_top_host_over_30 = df_top_host_all_utterances[df_top_host_all_utterances['episode'].isin(episodes_over_30)]\n",
    "    return df_top_host_over_30.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_transition_matrix_efficient(df, word_to_index, d):\n",
    "    \"\"\"\n",
    "    Get the transition matrix for the given DataFrame.\n",
    "    \"\"\"\n",
    "    transition_counts = np.ones((d, d), dtype=int)\n",
    "    for utterance in df['utterance']:\n",
    "        if type(utterance) != str:\n",
    "            try:\n",
    "                utterance = str(utterance)\n",
    "            except:\n",
    "                continue\n",
    "        words = utterance.split()\n",
    "        for i in range(1, len(words)):\n",
    "            word1 = words[i - 1]\n",
    "            word2 = words[i]\n",
    "            if word1 in word_to_index and word2 in word_to_index:\n",
    "                index1 = word_to_index[word1]\n",
    "                index2 = word_to_index[word2]\n",
    "                transition_counts[index2, index1] += 1\n",
    "    column_sums = transition_counts.sum(axis=0, keepdims=True)\n",
    "    transition_probabilities = transition_counts / column_sums\n",
    "    return transition_probabilities, word_to_index\n",
    "\n",
    "\n",
    "def tensor_trouble(df, word_to_index, d):\n",
    "    \"\"\"\n",
    "    Get the emission matrix for the given DataFrame.\n",
    "    \"\"\"\n",
    "    tensor = np.ones((d, 2, 2), dtype=int)\n",
    "    for episode, group in df.groupby('episode'):\n",
    "        if type(group['utterance']) != str:\n",
    "            group['utterance'] = group['utterance'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "        text = \" \".join(group['utterance'].astype(str))\n",
    "        words = text.split()\n",
    "        roles = np.concatenate(group.apply(lambda row: [row['is_host']] * len(str(row['utterance']).split()), axis=1).values)\n",
    "        for i in range(2, len(words)):\n",
    "            word = words[i - 2]\n",
    "            if word not in word_to_index:  # Check if the word exists in the dictionary\n",
    "                continue  # Skip this iteration if the word is not found\n",
    "            next_role = roles[i - 1]\n",
    "            following_role = roles[i]\n",
    "            current_index = word_to_index[word]\n",
    "            current_role_index = 0 if next_role else 1\n",
    "            next_role_index = 0 if following_role else 1\n",
    "            tensor[current_index, next_role_index, current_role_index] += 1\n",
    "    return tensor, word_to_index\n",
    "\n",
    "\n",
    "def make_emission(host, guest):\n",
    "    \"\"\"\n",
    "    Put the emission matrix together from the host and guest matrices.\n",
    "    \"\"\"\n",
    "    emission = np.zeros((len(host), len(host), 2))\n",
    "    emission[:, :, 0] = host\n",
    "    emission[:, :, 1] = guest\n",
    "    return np.swapaxes(emission, 0, 1)\n",
    "\n",
    "\n",
    "def tensor_viterbi(obs, transition, emission, initial):\n",
    "    \"\"\"\n",
    "    Perform the Viterbi algorithm, modified to work with the conditional HMM.\n",
    "    \"\"\"\n",
    "    b_eps = 1e-25\n",
    "    start_index = obs[0]\n",
    "    obs = obs[1:]\n",
    "    n = len(obs)\n",
    "    d = transition.shape[0]\n",
    "    eta = np.zeros((n, 2))\n",
    "    backpointers = np.zeros((n, 2), dtype=int)\n",
    "    eta[0] = np.log(initial) + np.log(emission[start_index, obs[1], :])\n",
    "    obs = obs[1:]\n",
    "    for i in range(1, n - 1):\n",
    "        b = emission[obs[i - 1], obs[i], :]\n",
    "        if np.any(b == 0):\n",
    "            zero_index = np.where(b == 0)\n",
    "            b[zero_index] = b_eps\n",
    "        eta_candidate = np.log(transition[obs[i - 1], :, :]) + np.log(b)[:, np.newaxis] + eta[i - 1][np.newaxis, :]\n",
    "        eta[i] = np.max(eta_candidate, axis=1)\n",
    "        backpointers[i] = np.argmax(eta_candidate, axis=1)\n",
    "    state_sequence = np.zeros(n, dtype=int)\n",
    "    state_sequence[-1] = np.argmax(eta[-1])\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        state_sequence[i] = backpointers[i + 1, state_sequence[i + 1]]\n",
    "    return state_sequence\n",
    "\n",
    "\n",
    "def pad_to_match(a, b):\n",
    "    \"\"\"\n",
    "    Pads the shorter array with its last element to match the length of the longer array.\n",
    "    \n",
    "    Args:\n",
    "        a (np.array): First array for comparison.\n",
    "        b (np.array): Second array for comparison.\n",
    "        \n",
    "    Returns:\n",
    "        np.array, np.array: The two arrays modified to have equal lengths.\n",
    "    \"\"\"\n",
    "    if len(a) == len(b):\n",
    "        return a, b\n",
    "    elif len(a) > len(b):\n",
    "        padding = np.full(len(a) - len(b), b[-1])\n",
    "        b_padded = np.concatenate((b, padding))\n",
    "        return a, b_padded\n",
    "    else:\n",
    "        padding = np.full(len(b) - len(a), a[-1])\n",
    "        a_padded = np.concatenate((a, padding))\n",
    "        return a_padded, b\n",
    "    \n",
    "    \n",
    "# Assume df is your DataFrame after reading the CSV file\n",
    "# df = pd.read_csv('archive/utterances-2sp.csv')\n",
    "\n",
    "# # Shift relevant columns to compare with the next row\n",
    "# df['next_episode_order'] = df['episode_order'].shift(-1)\n",
    "# df['next_host_id'] = df['host_id'].shift(-1)\n",
    "\n",
    "# # Define the condition to keep the last utterance where episode_order = 1 and host_id changes\n",
    "# keep_last_condition = (df['episode_order'] == 1) & (df['host_id'] != df['next_host_id'])\n",
    "\n",
    "# # Update your filtering condition to exclude rows where episode_order = 1 unless it meets the new condition\n",
    "# df = df[(df['episode_order'] != 1) | keep_last_condition]\n",
    "\n",
    "# # Remove the auxiliary columns if no longer needed\n",
    "# df.drop(columns=['next_episode_order', 'next_host_id'], inplace=True)\n",
    "\n",
    "# # Continue with your preprocessing\n",
    "# df = preprocess_dataframe(df)\n",
    "# df.loc[(df['episode_order'] == 1) & (df['turn_order'] == 0), 'utterance'] = df.loc[(df['episode_order'] == 1) & (df['turn_order'] == 0), 'utterance'].apply(lambda x: 'yyyyy ' + ' '.join(x.split()))\n",
    "\n",
    "df = pd.read_csv('archive/processed_utterances-2sp.csv')\n",
    "\n",
    "# get all of the unique host ids that are not -1 with at least 500 episodes\n",
    "host_ids = df[(df['host_id'] != -1) & (df['is_host'] == True)].groupby('host_id')['episode'].nunique()\n",
    "host_ids = host_ids[host_ids >= 100].index\n",
    "\n",
    "print(f'Number of hosts: {len(host_ids)}')\n",
    "\n",
    "\n",
    "weighted_accuracy_sum = 0\n",
    "weighted_assume_0_accuracy_sum = 0\n",
    "total_words = 0\n",
    "total_episodes = 0\n",
    "\n",
    "confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "# select 5 random hosts\n",
    "# host_ids = np.random.choice(host_ids, 5)\n",
    "\n",
    "# host_ids = [12]\n",
    "\n",
    "for host_id in host_ids:\n",
    "\n",
    "    print(f'Host ID: {host_id}')\n",
    "\n",
    "\n",
    "    filtered_df = filter_episodes(df, host_id=host_id)\n",
    "\n",
    "    total_episodes += len(filtered_df['episode'].unique())\n",
    "\n",
    "    print(f'Number of episodes: {len(filtered_df[\"episode\"].unique())}')\n",
    "\n",
    "\n",
    "    print(f'Number of utterances: {len(filtered_df)}')\n",
    "\n",
    "    # load the preprocessed data\n",
    "    # filtered_df = pd.read_csv('archive/most_episodes_host.csv')\n",
    "\n",
    "    # Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "    words_series = filtered_df['utterance'].str.split().explode()\n",
    "    unique_words = set(words_series)\n",
    "    word_frequencies = words_series.value_counts().to_dict()\n",
    "    word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "    # print the number of unique words\n",
    "    print(f'Number of unique words: {len(unique_words)}')\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    episodes = filtered_df['episode'].unique()\n",
    "    np.random.shuffle(episodes)\n",
    "    split_index = int(len(episodes) * 0.8)\n",
    "    train_episodes = episodes[:split_index]\n",
    "    test_episodes = episodes[split_index:]\n",
    "\n",
    "    test = filtered_df[filtered_df['episode'].isin(test_episodes)]\n",
    "\n",
    "    # Get the transition matrix for the host and guest\n",
    "    transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(filtered_df[(filtered_df['is_host'] == True) & (filtered_df['episode'].isin(train_episodes))], word_indices, len(unique_words))\n",
    "    transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(filtered_df[(filtered_df['is_host'] == False) & (filtered_df['episode'].isin(train_episodes))], word_indices, len(unique_words))\n",
    "    tensor, word_to_index = tensor_trouble(filtered_df[filtered_df['episode'].isin(train_episodes)], word_indices, len(unique_words))\n",
    "    tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "    # Initialize the emission matrix\n",
    "    initial = np.array([0.5, 0.5])\n",
    "    emission = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "    # Calculate the accuracy for each test episode\n",
    "    average_accuracy = 0\n",
    "    average_assume_0_accuracy = 0\n",
    "    accuracy_list = []\n",
    "    assume_0_accuracy_list = []\n",
    "\n",
    "    total_host_words = filtered_df['utterance'].str.split().explode().shape[0]\n",
    "\n",
    "    num_test_episodes = len(test_episodes)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_episode = None\n",
    "\n",
    "    # Calculate the accuracy for each test episode\n",
    "    for test_episode in test_episodes:\n",
    "        test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "        test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "        test_episode_df = test_episode_df.explode('utterance')\n",
    "        test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "        test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "        test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "        test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "        obs = [0] + test_episode_word_indices  # Add start token index (e.g., 0) if your Viterbi expects it\n",
    "\n",
    "        state_sequence = tensor_viterbi(obs, tensor_normalized, emission, initial)\n",
    "        # print(len(state_sequence))\n",
    "        test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "        # Now, calculate the accuracy\n",
    "        accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "        if accuracy < 0.5:\n",
    "            accuracy = 1 - accuracy\n",
    "            state_sequence_padded = 1 - state_sequence_padded\n",
    "\n",
    "        assume_0 = (test_label_padded == 1).astype(int)\n",
    "        assume_0_accuracy = np.mean(assume_0)\n",
    "\n",
    "        if assume_0_accuracy < 0.5:\n",
    "            assume_0_accuracy = 1 - assume_0_accuracy\n",
    "            assume_0 = 1 - assume_0\n",
    "\n",
    "        average_accuracy += accuracy\n",
    "        average_assume_0_accuracy += assume_0_accuracy\n",
    "        accuracy_list.append(accuracy)\n",
    "        assume_0_accuracy_list.append(assume_0_accuracy)\n",
    "\n",
    "        confusion += confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "\n",
    "    print(f'Average Accuracy: {average_accuracy / num_test_episodes}')\n",
    "    print(f'Average Assume 0 Accuracy: {average_assume_0_accuracy / num_test_episodes}')\n",
    "    print(f'Variance of Accuracy: {np.var(accuracy_list)}')\n",
    "    print(f'Variance of Assume 0 Accuracy: {np.var(assume_0_accuracy_list)}')\n",
    "\n",
    "# Update weighted sums using total_host_words as the weight\n",
    "    weighted_accuracy_sum += (average_accuracy / num_test_episodes) * total_host_words\n",
    "    weighted_assume_0_accuracy_sum += (average_assume_0_accuracy / num_test_episodes) * total_host_words\n",
    "    total_words += total_host_words\n",
    "\n",
    "# Calculate the final weighted averages\n",
    "weighted_average_accuracy = weighted_accuracy_sum / total_words\n",
    "weighted_average_assume_0_accuracy = weighted_assume_0_accuracy_sum / total_words\n",
    "\n",
    "print(f'Weighted Average Accuracy: {weighted_average_accuracy}')\n",
    "print(f'Weighted Average Assume 0 Accuracy: {weighted_average_assume_0_accuracy}')\n",
    "print(f'Total Episodes: {total_episodes}')\n",
    "print(f'Total Words: {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2061201,   47680],\n",
       "       [ 671811,  234072]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68370227, 0.0158155 ],\n",
       "       [0.22284033, 0.0776419 ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion / confusion.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = best_episode\n",
    "\n",
    "# Create a DataFrame for the best episode\n",
    "test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "test_episode_df = test_episode_df.explode('utterance')\n",
    "test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "obs = [0] + test_episode_word_indices  # Add start token index (e.g., 0) if your Viterbi expects it\n",
    "\n",
    "state_sequence = tensor_viterbi(obs, tensor_normalized, emission, initial)\n",
    "# print(len(state_sequence))\n",
    "test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "# Now, calculate the accuracy\n",
    "accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "assume_0_accuracy = np.mean((test_label_padded == 1).astype(int))\n",
    "accuracy = max(accuracy, 1 - accuracy)  # Adjust based on expected behavior\n",
    "assume_0_accuracy = max(assume_0_accuracy, 1 - assume_0_accuracy)  # Adjust based on expected behavior\n",
    "\n",
    "if accuracy > best_accuracy and len(test_episode_word_indices) > 20:\n",
    "    best_accuracy = accuracy\n",
    "    best_episode = test_episode\n",
    "\n",
    "average_accuracy += accuracy\n",
    "average_assume_0_accuracy += assume_0_accuracy\n",
    "accuracy_list.append(accuracy)\n",
    "assume_0_accuracy_list.append(assume_0_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9301587301587302"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all',\n",
       " 'along',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'xxxxx',\n",
       " 'interstate',\n",
       " 'ha',\n",
       " 'helped',\n",
       " 'shape',\n",
       " 'the',\n",
       " 'daily',\n",
       " 'life',\n",
       " 'and',\n",
       " 'vacation',\n",
       " 'dream',\n",
       " 'of',\n",
       " 'the',\n",
       " 'roughly',\n",
       " 'million',\n",
       " 'people',\n",
       " 'who',\n",
       " 'reside',\n",
       " 'alongside',\n",
       " 'it',\n",
       " 'they',\n",
       " 'live',\n",
       " 'in',\n",
       " 'dense',\n",
       " 'city',\n",
       " 'and',\n",
       " 'rural',\n",
       " 'county',\n",
       " 'in',\n",
       " 'the',\n",
       " 'state',\n",
       " 'through',\n",
       " 'which',\n",
       " 'i',\n",
       " 'pass',\n",
       " 'on',\n",
       " 'it',\n",
       " 'north',\n",
       " 'south',\n",
       " 'path',\n",
       " 'the',\n",
       " 'traffic',\n",
       " 'peak',\n",
       " 'in',\n",
       " 'august',\n",
       " 'so',\n",
       " 'beginning',\n",
       " 'today',\n",
       " 'continuing',\n",
       " 'for',\n",
       " 'the',\n",
       " 'next',\n",
       " 'two',\n",
       " 'weekend',\n",
       " 'npr',\n",
       " 'will',\n",
       " 'take',\n",
       " 'a',\n",
       " 'closer',\n",
       " 'look',\n",
       " 'at',\n",
       " 'this',\n",
       " 'great',\n",
       " 'river',\n",
       " 'of',\n",
       " 'traffic',\n",
       " 'in',\n",
       " 'a',\n",
       " 'series',\n",
       " 'were',\n",
       " 'calling',\n",
       " 'i',\n",
       " 'the',\n",
       " 'road',\n",
       " 'most',\n",
       " 'traveled',\n",
       " 'nprs',\n",
       " 'senior',\n",
       " 'business',\n",
       " 'editor',\n",
       " 'marilyn',\n",
       " 'geewax',\n",
       " 'is',\n",
       " 'here',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'u',\n",
       " 'more',\n",
       " 'marilyn',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'being',\n",
       " 'with',\n",
       " 'u',\n",
       " 'hi',\n",
       " 'scott',\n",
       " 'i',\n",
       " 'ha',\n",
       " 'been',\n",
       " 'around',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'so',\n",
       " 'why',\n",
       " 'a',\n",
       " 'series',\n",
       " 'scott',\n",
       " 'i',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'see',\n",
       " 'this',\n",
       " 'article',\n",
       " 'that',\n",
       " 'xxxxx',\n",
       " 'saying',\n",
       " 'after',\n",
       " 'all',\n",
       " 'these',\n",
       " 'year',\n",
       " 'and',\n",
       " 'year',\n",
       " 'of',\n",
       " 'delay',\n",
       " 'construction',\n",
       " 'finally',\n",
       " 'going',\n",
       " 'to',\n",
       " 'start',\n",
       " 'in',\n",
       " 'september',\n",
       " 'on',\n",
       " 'this',\n",
       " 'part',\n",
       " 'thats',\n",
       " 'called',\n",
       " 'the',\n",
       " 'missing',\n",
       " 'link',\n",
       " 'of',\n",
       " 'i',\n",
       " 'it',\n",
       " 'a',\n",
       " 'short',\n",
       " 'stretch',\n",
       " 'where',\n",
       " 'i',\n",
       " 'xxxxx',\n",
       " 'never',\n",
       " 'completed',\n",
       " 'around',\n",
       " 'xxxxx',\n",
       " 'new',\n",
       " 'jersey',\n",
       " 'and',\n",
       " 'in',\n",
       " 'theory',\n",
       " 'i',\n",
       " 'should',\n",
       " 'be',\n",
       " 'this',\n",
       " 'xxxxx',\n",
       " 'highway',\n",
       " 'that',\n",
       " 'go',\n",
       " 'from',\n",
       " 'maine',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'down',\n",
       " 'to',\n",
       " 'miami',\n",
       " 'but',\n",
       " 'it',\n",
       " 'not',\n",
       " 'it',\n",
       " 'got',\n",
       " 'this',\n",
       " 'gap',\n",
       " 'so',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'that',\n",
       " 'missing',\n",
       " 'link',\n",
       " 'in',\n",
       " 'a',\n",
       " 'piece',\n",
       " 'thats',\n",
       " 'coming',\n",
       " 'up',\n",
       " 'in',\n",
       " 'just',\n",
       " 'a',\n",
       " 'few',\n",
       " 'moment',\n",
       " 'and',\n",
       " 'what',\n",
       " 'make',\n",
       " 'i',\n",
       " 'distinctive',\n",
       " 'compared',\n",
       " 'with',\n",
       " 'other',\n",
       " 'interstate',\n",
       " 'around',\n",
       " 'the',\n",
       " 'country',\n",
       " 'well',\n",
       " 'it',\n",
       " 'really',\n",
       " 'a',\n",
       " 'key',\n",
       " 'transportation',\n",
       " 'link',\n",
       " 'for',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'east',\n",
       " 'coast',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'most',\n",
       " 'densely',\n",
       " 'populated',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'country',\n",
       " 'and',\n",
       " 'it',\n",
       " 'also',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'historic',\n",
       " 'the',\n",
       " 'original',\n",
       " 'colony',\n",
       " 'are',\n",
       " 'included',\n",
       " 'on',\n",
       " 'this',\n",
       " 'road',\n",
       " 'and',\n",
       " 'it',\n",
       " 'pass',\n",
       " 'through',\n",
       " 'more',\n",
       " 'state',\n",
       " 'than',\n",
       " 'any',\n",
       " 'other',\n",
       " 'interstate',\n",
       " 'in',\n",
       " 'the',\n",
       " 'country',\n",
       " 'the',\n",
       " 'department',\n",
       " 'of',\n",
       " 'transportation',\n",
       " 'say',\n",
       " 'it',\n",
       " 'get',\n",
       " 'xxxxx',\n",
       " 'thats',\n",
       " 'vehicle',\n",
       " 'mile',\n",
       " 'traveled',\n",
       " 'more',\n",
       " 'than',\n",
       " 'any',\n",
       " 'other',\n",
       " 'road',\n",
       " 'in',\n",
       " 'the',\n",
       " 'country',\n",
       " 'each',\n",
       " 'year',\n",
       " 'it',\n",
       " 'our',\n",
       " 'infrastructure',\n",
       " 'xxxxx',\n",
       " 'and',\n",
       " 'it',\n",
       " 'allows',\n",
       " 'u',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'our',\n",
       " 'job',\n",
       " 'take',\n",
       " 'our',\n",
       " 'export',\n",
       " 'over',\n",
       " 'to',\n",
       " 'ship',\n",
       " 'and',\n",
       " 'maybe',\n",
       " 'the',\n",
       " 'most',\n",
       " 'important',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'it',\n",
       " 'get',\n",
       " 'our',\n",
       " 'little',\n",
       " 'body',\n",
       " 'out',\n",
       " 'onto',\n",
       " 'the',\n",
       " 'beach',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'tan',\n",
       " 'this',\n",
       " 'time',\n",
       " 'of',\n",
       " 'year',\n",
       " 'tell',\n",
       " 'u',\n",
       " 'about',\n",
       " 'what',\n",
       " 'youve',\n",
       " 'got',\n",
       " 'coming',\n",
       " 'up',\n",
       " 'well',\n",
       " 'after',\n",
       " 'today',\n",
       " 'report',\n",
       " 'on',\n",
       " 'that',\n",
       " 'missing',\n",
       " 'link',\n",
       " 'weve',\n",
       " 'got',\n",
       " 'kathy',\n",
       " 'xxxxx',\n",
       " 'coming',\n",
       " 'up',\n",
       " 'reporter',\n",
       " 'in',\n",
       " 'georgia',\n",
       " 'who',\n",
       " 'spent',\n",
       " 'some',\n",
       " 'time',\n",
       " 'talking',\n",
       " 'to',\n",
       " 'the',\n",
       " 'official',\n",
       " 'there',\n",
       " 'who',\n",
       " 'say',\n",
       " 'they',\n",
       " 'have',\n",
       " 'spent',\n",
       " 'a',\n",
       " 'fortune',\n",
       " 'expanding',\n",
       " 'i',\n",
       " 'to',\n",
       " 'accommodate',\n",
       " 'all',\n",
       " 'that',\n",
       " 'freight',\n",
       " 'traffic',\n",
       " 'thats',\n",
       " 'now',\n",
       " 'coming',\n",
       " 'in',\n",
       " 'through',\n",
       " 'the',\n",
       " 'port',\n",
       " 'of',\n",
       " 'savannah',\n",
       " 'this',\n",
       " 'ha',\n",
       " 'really',\n",
       " 'transformed',\n",
       " 'the',\n",
       " 'economy',\n",
       " 'in',\n",
       " 'south',\n",
       " 'georgia',\n",
       " 'because',\n",
       " 'it',\n",
       " 'made',\n",
       " 'that',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'really',\n",
       " 'a',\n",
       " 'global',\n",
       " 'link',\n",
       " 'where',\n",
       " 'export',\n",
       " 'and',\n",
       " 'import',\n",
       " 'come',\n",
       " 'in',\n",
       " 'and',\n",
       " 'out',\n",
       " 'and',\n",
       " 'then',\n",
       " 'next',\n",
       " 'weekend',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'head',\n",
       " 'down',\n",
       " 'to',\n",
       " 'south',\n",
       " 'florida',\n",
       " 'and',\n",
       " 'up',\n",
       " 'to',\n",
       " 'maine',\n",
       " 'of',\n",
       " 'course',\n",
       " 'florida',\n",
       " 'and',\n",
       " 'maine',\n",
       " 'are',\n",
       " 'two',\n",
       " 'very',\n",
       " 'different',\n",
       " 'state',\n",
       " 'but',\n",
       " 'they',\n",
       " 'both',\n",
       " 'depend',\n",
       " 'very',\n",
       " 'heavily',\n",
       " 'on',\n",
       " 'i',\n",
       " 'to',\n",
       " 'deliver',\n",
       " 'tourist',\n",
       " 'and',\n",
       " 'create',\n",
       " 'job',\n",
       " 'speaking',\n",
       " 'of',\n",
       " 'tourist',\n",
       " 'and',\n",
       " 'job',\n",
       " 'for',\n",
       " 'that',\n",
       " 'matter',\n",
       " 'labor',\n",
       " 'day',\n",
       " 'weekend',\n",
       " 'will',\n",
       " 'find',\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'tourist',\n",
       " 'on',\n",
       " 'i',\n",
       " 'but',\n",
       " 'there',\n",
       " 'another',\n",
       " 'type',\n",
       " 'of',\n",
       " 'xxxxx',\n",
       " 'who',\n",
       " 'doesnt',\n",
       " 'get',\n",
       " 'a',\n",
       " 'break',\n",
       " 'on',\n",
       " 'the',\n",
       " 'holiday',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'more',\n",
       " 'about',\n",
       " 'them',\n",
       " 'too',\n",
       " 'right',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'end',\n",
       " 'this',\n",
       " 'series',\n",
       " 'on',\n",
       " 'labor',\n",
       " 'day',\n",
       " 'weekend',\n",
       " 'by',\n",
       " 'learning',\n",
       " 'more',\n",
       " 'about',\n",
       " 'the',\n",
       " 'migrant',\n",
       " 'worker',\n",
       " 'who',\n",
       " 'drive',\n",
       " 'up',\n",
       " 'and',\n",
       " 'down',\n",
       " 'i',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'they',\n",
       " 'follow',\n",
       " 'the',\n",
       " 'crop',\n",
       " 'that',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'picked',\n",
       " 'along',\n",
       " 'the',\n",
       " 'eastern',\n",
       " 'xxxxx',\n",
       " 'in',\n",
       " 'our',\n",
       " 'very',\n",
       " 'last',\n",
       " 'piece',\n",
       " 'well',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'transportation',\n",
       " 'xxxxx',\n",
       " 'who',\n",
       " 'offer',\n",
       " 'some',\n",
       " 'suggestion',\n",
       " 'for',\n",
       " 'how',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'traffic',\n",
       " 'flowing',\n",
       " 'on',\n",
       " 'what',\n",
       " 'is',\n",
       " 'already',\n",
       " 'a',\n",
       " 'very',\n",
       " 'over',\n",
       " 'crowded',\n",
       " 'road',\n",
       " 'you',\n",
       " 'know',\n",
       " 'we',\n",
       " 'use',\n",
       " 'our',\n",
       " 'interstate',\n",
       " 'for',\n",
       " 'all',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'short',\n",
       " 'xxxxx',\n",
       " 'you',\n",
       " 'know',\n",
       " 'you',\n",
       " 'might',\n",
       " 'get',\n",
       " 'on',\n",
       " 'i',\n",
       " 'just',\n",
       " 'to',\n",
       " 'run',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'mall',\n",
       " 'or',\n",
       " 'to',\n",
       " 'get',\n",
       " 'to',\n",
       " 'your',\n",
       " 'office',\n",
       " 'whatever',\n",
       " 'but',\n",
       " 'these',\n",
       " 'thing',\n",
       " 'were',\n",
       " 'really',\n",
       " 'intended',\n",
       " 'to',\n",
       " 'be',\n",
       " 'interstate',\n",
       " 'youre',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'drive',\n",
       " 'quickly',\n",
       " 'from',\n",
       " 'one',\n",
       " 'state',\n",
       " 'to',\n",
       " 'another',\n",
       " 'not',\n",
       " 'just',\n",
       " 'run',\n",
       " 'to',\n",
       " 'the',\n",
       " 'mall',\n",
       " 'but',\n",
       " 'if',\n",
       " 'we',\n",
       " 'want',\n",
       " 'to',\n",
       " 'preserve',\n",
       " 'that',\n",
       " 'interstate',\n",
       " 'system',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'have',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'more',\n",
       " 'creative',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'increase',\n",
       " 'the',\n",
       " 'capacity',\n",
       " 'nprs',\n",
       " 'senior',\n",
       " 'business',\n",
       " 'editor',\n",
       " 'marilyn',\n",
       " 'geewax',\n",
       " 'thanks',\n",
       " 'so',\n",
       " 'much',\n",
       " 'oh',\n",
       " 'youre',\n",
       " 'welcome',\n",
       " 'scott']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_episode_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - state_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('archive/processed_utterances-2sp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('archive/most_episodes_host.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
