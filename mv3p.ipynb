{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import warnings\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hosts: 32\n",
      "Host ID: 0\n",
      "Number of episodes: 318\n",
      "Number of utterances: 18419\n",
      "Number of unique words: 9019\n",
      "Average Accuracy: 0.7516475456730863\n",
      "Average Assume 0 Accuracy: 0.686472524965357\n",
      "Variance of Accuracy: 0.004116241661218167\n",
      "Variance of Assume 0 Accuracy: 0.006263564690280386\n",
      "Host ID: 1\n",
      "Number of episodes: 907\n",
      "Number of utterances: 46765\n",
      "Number of unique words: 12754\n",
      "Average Accuracy: 0.7555795368516068\n",
      "Average Assume 0 Accuracy: 0.6927065532948183\n",
      "Variance of Accuracy: 0.00848623272205961\n",
      "Variance of Assume 0 Accuracy: 0.011098463922067632\n",
      "Host ID: 5\n",
      "Number of episodes: 971\n",
      "Number of utterances: 45880\n",
      "Number of unique words: 12761\n",
      "Average Accuracy: 0.7397882405791776\n",
      "Average Assume 0 Accuracy: 0.6845936757944583\n",
      "Variance of Accuracy: 0.007485634787151783\n",
      "Variance of Assume 0 Accuracy: 0.007242588561502564\n",
      "Host ID: 7\n",
      "Number of episodes: 476\n",
      "Number of utterances: 44304\n",
      "Number of unique words: 12416\n",
      "Average Accuracy: 0.7566031781982212\n",
      "Average Assume 0 Accuracy: 0.6876464317520282\n",
      "Variance of Accuracy: 0.006092966963605846\n",
      "Variance of Assume 0 Accuracy: 0.0077496799067127515\n",
      "Host ID: 8\n",
      "Number of episodes: 463\n",
      "Number of utterances: 51500\n",
      "Number of unique words: 10390\n",
      "Average Accuracy: 0.761349583244828\n",
      "Average Assume 0 Accuracy: 0.6694131017738302\n",
      "Variance of Accuracy: 0.0037128672868337494\n",
      "Variance of Assume 0 Accuracy: 0.006293553399945393\n",
      "Host ID: 9\n",
      "Number of episodes: 612\n",
      "Number of utterances: 32730\n",
      "Number of unique words: 11196\n",
      "Average Accuracy: 0.7203669979692904\n",
      "Average Assume 0 Accuracy: 0.6709189387731707\n",
      "Variance of Accuracy: 0.007388633466095316\n",
      "Variance of Assume 0 Accuracy: 0.008316949596578674\n",
      "Host ID: 10\n",
      "Number of episodes: 356\n",
      "Number of utterances: 20311\n",
      "Number of unique words: 9316\n",
      "Average Accuracy: 0.7175396057733896\n",
      "Average Assume 0 Accuracy: 0.644300687027661\n",
      "Variance of Accuracy: 0.005102020648763797\n",
      "Variance of Assume 0 Accuracy: 0.006470674193106394\n",
      "Host ID: 11\n",
      "Number of episodes: 806\n",
      "Number of utterances: 40021\n",
      "Number of unique words: 12196\n",
      "Average Accuracy: 0.7580316038099876\n",
      "Average Assume 0 Accuracy: 0.6940845635878834\n",
      "Variance of Accuracy: 0.006585171904255473\n",
      "Variance of Assume 0 Accuracy: 0.007329910440511038\n",
      "Host ID: 12\n",
      "Number of episodes: 1793\n",
      "Number of utterances: 94514\n",
      "Number of unique words: 14451\n",
      "Average Accuracy: 0.7548094850908728\n",
      "Average Assume 0 Accuracy: 0.6812420635107409\n",
      "Variance of Accuracy: 0.004892158682108011\n",
      "Variance of Assume 0 Accuracy: 0.006267064014809347\n",
      "Host ID: 13\n",
      "Number of episodes: 239\n",
      "Number of utterances: 17287\n",
      "Number of unique words: 8838\n",
      "Average Accuracy: 0.7716653074250788\n",
      "Average Assume 0 Accuracy: 0.7065306597394088\n",
      "Variance of Accuracy: 0.006875850581281891\n",
      "Variance of Assume 0 Accuracy: 0.00906009254759899\n",
      "Host ID: 14\n",
      "Number of episodes: 593\n",
      "Number of utterances: 34128\n",
      "Number of unique words: 11227\n",
      "Average Accuracy: 0.729771208547909\n",
      "Average Assume 0 Accuracy: 0.652068356749879\n",
      "Variance of Accuracy: 0.005994762399154978\n",
      "Variance of Assume 0 Accuracy: 0.007412738805901075\n",
      "Host ID: 15\n",
      "Number of episodes: 864\n",
      "Number of utterances: 51036\n",
      "Number of unique words: 12555\n",
      "Average Accuracy: 0.7347229873727784\n",
      "Average Assume 0 Accuracy: 0.6636517446734859\n",
      "Variance of Accuracy: 0.004591325260601587\n",
      "Variance of Assume 0 Accuracy: 0.005274044519600443\n",
      "Host ID: 16\n",
      "Number of episodes: 1209\n",
      "Number of utterances: 67375\n",
      "Number of unique words: 13284\n",
      "Average Accuracy: 0.7290648328484636\n",
      "Average Assume 0 Accuracy: 0.6749503946085145\n",
      "Variance of Accuracy: 0.00862832756572428\n",
      "Variance of Assume 0 Accuracy: 0.009741129947747826\n",
      "Host ID: 17\n",
      "Number of episodes: 724\n",
      "Number of utterances: 35451\n",
      "Number of unique words: 11249\n",
      "Average Accuracy: 0.7808032857286215\n",
      "Average Assume 0 Accuracy: 0.7138745535263636\n",
      "Variance of Accuracy: 0.004417149240031852\n",
      "Variance of Assume 0 Accuracy: 0.005834372979720581\n",
      "Host ID: 18\n",
      "Number of episodes: 223\n",
      "Number of utterances: 10393\n",
      "Number of unique words: 7381\n",
      "Average Accuracy: 0.7736163955628821\n",
      "Average Assume 0 Accuracy: 0.7111834058123649\n",
      "Variance of Accuracy: 0.005670630287139316\n",
      "Variance of Assume 0 Accuracy: 0.006219803722102983\n",
      "Host ID: 19\n",
      "Number of episodes: 1069\n",
      "Number of utterances: 64112\n",
      "Number of unique words: 12775\n",
      "Average Accuracy: 0.7731380538829415\n",
      "Average Assume 0 Accuracy: 0.6178241585910446\n",
      "Variance of Accuracy: 0.005690863228822396\n",
      "Variance of Assume 0 Accuracy: 0.006876006617559331\n",
      "Host ID: 21\n",
      "Number of episodes: 222\n",
      "Number of utterances: 11992\n",
      "Number of unique words: 7707\n",
      "Average Accuracy: 0.7349172165633381\n",
      "Average Assume 0 Accuracy: 0.6535784819993432\n",
      "Variance of Accuracy: 0.004849231332730722\n",
      "Variance of Assume 0 Accuracy: 0.005772431030369882\n",
      "Host ID: 22\n",
      "Number of episodes: 136\n",
      "Number of utterances: 7368\n",
      "Number of unique words: 6586\n",
      "Average Accuracy: 0.7342644388046712\n",
      "Average Assume 0 Accuracy: 0.6550991907973074\n",
      "Variance of Accuracy: 0.005846536093170173\n",
      "Variance of Assume 0 Accuracy: 0.006258619476266486\n",
      "Host ID: 28\n",
      "Number of episodes: 559\n",
      "Number of utterances: 26923\n",
      "Number of unique words: 10409\n",
      "Average Accuracy: 0.723427415573549\n",
      "Average Assume 0 Accuracy: 0.6509149048237776\n",
      "Variance of Accuracy: 0.009888601500214662\n",
      "Variance of Assume 0 Accuracy: 0.010467244111403488\n",
      "Host ID: 29\n",
      "Number of episodes: 1058\n",
      "Number of utterances: 76051\n",
      "Number of unique words: 13014\n",
      "Average Accuracy: 0.7839007164145202\n",
      "Average Assume 0 Accuracy: 0.7156555958703696\n",
      "Variance of Accuracy: 0.008119341830756514\n",
      "Variance of Assume 0 Accuracy: 0.008895758260893059\n",
      "Host ID: 42\n",
      "Number of episodes: 304\n",
      "Number of utterances: 16648\n",
      "Number of unique words: 9573\n",
      "Average Accuracy: 0.7528110458367283\n",
      "Average Assume 0 Accuracy: 0.695887031743084\n",
      "Variance of Accuracy: 0.005926475614065347\n",
      "Variance of Assume 0 Accuracy: 0.008065635453504775\n",
      "Host ID: 43\n",
      "Number of episodes: 408\n",
      "Number of utterances: 23829\n",
      "Number of unique words: 10848\n",
      "Average Accuracy: 0.7429090681825625\n",
      "Average Assume 0 Accuracy: 0.6587407020799134\n",
      "Variance of Accuracy: 0.005448197800316975\n",
      "Variance of Assume 0 Accuracy: 0.004429043951251019\n",
      "Host ID: 47\n",
      "Number of episodes: 357\n",
      "Number of utterances: 17504\n",
      "Number of unique words: 9847\n",
      "Average Accuracy: 0.7252375297210317\n",
      "Average Assume 0 Accuracy: 0.6679002977407253\n",
      "Variance of Accuracy: 0.005963974676047056\n",
      "Variance of Assume 0 Accuracy: 0.007075090380320694\n",
      "Host ID: 78\n",
      "Number of episodes: 252\n",
      "Number of utterances: 13213\n",
      "Number of unique words: 8430\n",
      "Average Accuracy: 0.7549075294172107\n",
      "Average Assume 0 Accuracy: 0.6959258708199133\n",
      "Variance of Accuracy: 0.004276299964062259\n",
      "Variance of Assume 0 Accuracy: 0.005877272874188412\n",
      "Host ID: 91\n",
      "Number of episodes: 341\n",
      "Number of utterances: 20950\n",
      "Number of unique words: 9714\n",
      "Average Accuracy: 0.7768548496792386\n",
      "Average Assume 0 Accuracy: 0.7025468051045709\n",
      "Variance of Accuracy: 0.009694893378291172\n",
      "Variance of Assume 0 Accuracy: 0.012700239591857747\n",
      "Host ID: 92\n",
      "Number of episodes: 546\n",
      "Number of utterances: 29448\n",
      "Number of unique words: 11146\n",
      "Average Accuracy: 0.7425592303535812\n",
      "Average Assume 0 Accuracy: 0.6880900316795009\n",
      "Variance of Accuracy: 0.006583877196886228\n",
      "Variance of Assume 0 Accuracy: 0.007213688980971435\n",
      "Host ID: 98\n",
      "Number of episodes: 104\n",
      "Number of utterances: 4925\n",
      "Number of unique words: 5561\n",
      "Average Accuracy: 0.7221914111257443\n",
      "Average Assume 0 Accuracy: 0.6751612882789007\n",
      "Variance of Accuracy: 0.0038565896423726955\n",
      "Variance of Assume 0 Accuracy: 0.00693153604973805\n",
      "Host ID: 105\n",
      "Number of episodes: 1341\n",
      "Number of utterances: 66684\n",
      "Number of unique words: 13682\n",
      "Average Accuracy: 0.7341227339425676\n",
      "Average Assume 0 Accuracy: 0.643450061656287\n",
      "Variance of Accuracy: 0.007982641946378854\n",
      "Variance of Assume 0 Accuracy: 0.009249864421596867\n",
      "Host ID: 115\n",
      "Number of episodes: 314\n",
      "Number of utterances: 16952\n",
      "Number of unique words: 8767\n",
      "Average Accuracy: 0.7337467922218187\n",
      "Average Assume 0 Accuracy: 0.6736615770147343\n",
      "Variance of Accuracy: 0.003037061360499507\n",
      "Variance of Assume 0 Accuracy: 0.0044443216783501506\n",
      "Host ID: 117\n",
      "Number of episodes: 394\n",
      "Number of utterances: 20123\n",
      "Number of unique words: 10152\n",
      "Average Accuracy: 0.765731119752987\n",
      "Average Assume 0 Accuracy: 0.6953152669661739\n",
      "Variance of Accuracy: 0.0070301774984700815\n",
      "Variance of Assume 0 Accuracy: 0.009866114189449618\n",
      "Host ID: 120\n",
      "Number of episodes: 774\n",
      "Number of utterances: 49609\n",
      "Number of unique words: 12658\n",
      "Average Accuracy: 0.7273444839001006\n",
      "Average Assume 0 Accuracy: 0.6381521607104762\n",
      "Variance of Accuracy: 0.006601060422667987\n",
      "Variance of Assume 0 Accuracy: 0.007035264423865663\n",
      "Host ID: 172\n",
      "Number of episodes: 487\n",
      "Number of utterances: 24526\n",
      "Number of unique words: 10704\n",
      "Average Accuracy: 0.7493224140972021\n",
      "Average Assume 0 Accuracy: 0.6940314845989068\n",
      "Variance of Accuracy: 0.008417922636294218\n",
      "Variance of Assume 0 Accuracy: 0.015041203981274474\n",
      "Weighted Average Accuracy: 0.7497865709549778\n",
      "Weighted Average Assume 0 Accuracy: 0.6750882768502973\n",
      "Total Episodes: 19220\n",
      "Total Words: 16411928\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_threshold = 25\n",
    "\n",
    "def aggressive_cleaning(text):\n",
    "    \"\"\"\n",
    "    Cleans the text by handling NaN values and 'nan' strings, normalizing dashes, lowercasing,\n",
    "    and removing non-alphabetic characters except spaces.\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    if text.lower() == 'nan' or pd.isna(text):\n",
    "        return ''\n",
    "    else:\n",
    "        text = text.replace('-', ' ').lower().strip()\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "def tokenize_and_lemmatize(text, word_frequencies):\n",
    "    \"\"\"\n",
    "    Tokenizes and lemmatizes the given text, applying a word frequency filter.\n",
    "    Words below the threshold are replaced with 'xxxxx'.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "    filtered_tokens = [word if word_frequencies.get(word, 0) >= word_threshold else 'xxxxx' for word in lemmatized_tokens]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def preprocess_dataframe(dataframe, batch_size=60000):\n",
    "    \"\"\"\n",
    "    Applies preprocessing to the entire DataFrame, including cleaning text, tokenizing,\n",
    "    lemmatizing, and filtering based on word frequencies.\n",
    "    \"\"\"\n",
    "    # Apply aggressive cleaning\n",
    "    dataframe['utterance'] = dataframe['utterance'].apply(aggressive_cleaning)\n",
    "    \n",
    "    all_words = ' '.join(dataframe['utterance']).split()\n",
    "    word_frequencies = Counter(all_words)\n",
    "    \n",
    "    processed_batches = []\n",
    "    progress = tqdm(total=len(dataframe), desc=\"Processing batches\")\n",
    "    \n",
    "    for start_row in range(0, len(dataframe), batch_size):\n",
    "        end_row = start_row + batch_size\n",
    "        batch = dataframe.iloc[start_row:end_row]\n",
    "        batch['utterance'] = batch['utterance'].apply(lambda x: tokenize_and_lemmatize(x, word_frequencies))\n",
    "        processed_batches.append(batch)\n",
    "        progress.update(len(batch))\n",
    "    \n",
    "    progress.close()\n",
    "    return pd.concat(processed_batches)\n",
    "\n",
    "\n",
    "def filter_episodes(df, host_id=None):\n",
    "    df_filtered = df[df['host_id'] != -1]\n",
    "    if host_id is None:\n",
    "        top_host = df_filtered.groupby('host_id')['episode'].nunique().idxmax()\n",
    "    else:\n",
    "        top_host = host_id\n",
    "    top_host_episodes = df_filtered[df_filtered['host_id'] == top_host]['episode'].unique()\n",
    "    df_top_host_all_utterances = df[df['episode'].isin(top_host_episodes)]\n",
    "    utterance_counts = df_top_host_all_utterances.groupby('episode')['utterance'].count()\n",
    "    episodes_over_30 = utterance_counts[utterance_counts > 30].index\n",
    "    df_top_host_over_30 = df_top_host_all_utterances[df_top_host_all_utterances['episode'].isin(episodes_over_30)]\n",
    "    return df_top_host_over_30.reset_index(drop=True)\n",
    "\n",
    "def get_transition_matrix_efficient(df, word_to_index, d):\n",
    "    transition_counts = np.ones((d, d), dtype=int)\n",
    "    for utterance in df['utterance']:\n",
    "        if type(utterance) != str:\n",
    "            try:\n",
    "                utterance = str(utterance)\n",
    "            except:\n",
    "                continue\n",
    "        words = utterance.split()\n",
    "        for i in range(1, len(words)):\n",
    "            word1 = words[i - 1]\n",
    "            word2 = words[i]\n",
    "            if word1 in word_to_index and word2 in word_to_index:\n",
    "                index1 = word_to_index[word1]\n",
    "                index2 = word_to_index[word2]\n",
    "                transition_counts[index2, index1] += 1\n",
    "    column_sums = transition_counts.sum(axis=0, keepdims=True)\n",
    "    transition_probabilities = transition_counts / column_sums\n",
    "    return transition_probabilities, word_to_index\n",
    "\n",
    "def tensor_trouble(df, word_to_index, d):\n",
    "    tensor = np.ones((d, 2, 2), dtype=int)\n",
    "    for episode, group in df.groupby('episode'):\n",
    "        if type(group['utterance']) != str:\n",
    "            group['utterance'] = group['utterance'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "        text = \" \".join(group['utterance'].astype(str))\n",
    "        words = text.split()\n",
    "        roles = np.concatenate(group.apply(lambda row: [row['is_host']] * len(str(row['utterance']).split()), axis=1).values)\n",
    "        for i in range(2, len(words)):\n",
    "            word = words[i - 2]\n",
    "            if word not in word_to_index:  # Check if the word exists in the dictionary\n",
    "                continue  # Skip this iteration if the word is not found\n",
    "            next_role = roles[i - 1]\n",
    "            following_role = roles[i]\n",
    "            current_index = word_to_index[word]\n",
    "            current_role_index = 0 if next_role else 1\n",
    "            next_role_index = 0 if following_role else 1\n",
    "            tensor[current_index, next_role_index, current_role_index] += 1\n",
    "    return tensor, word_to_index\n",
    "\n",
    "def make_emission(host, guest):\n",
    "    emission = np.zeros((len(host), len(host), 2))\n",
    "    emission[:, :, 0] = host\n",
    "    emission[:, :, 1] = guest\n",
    "    return np.swapaxes(emission, 0, 1)\n",
    "\n",
    "def tensor_viterbi(obs, transition, emission, initial):\n",
    "    b_eps = 1e-25\n",
    "    start_index = obs[0]\n",
    "    obs = obs[1:]\n",
    "    n = len(obs)\n",
    "    d = transition.shape[0]\n",
    "    eta = np.zeros((n, 2))\n",
    "    backpointers = np.zeros((n, 2), dtype=int)\n",
    "    eta[0] = np.log(initial) + np.log(emission[start_index, obs[1], :])\n",
    "    obs = obs[1:]\n",
    "    for i in range(1, n - 1):\n",
    "        b = emission[obs[i - 1], obs[i], :]\n",
    "        if np.any(b == 0):\n",
    "            zero_index = np.where(b == 0)\n",
    "            b[zero_index] = b_eps\n",
    "        eta_candidate = np.log(transition[obs[i - 1], :, :]) + np.log(b)[:, np.newaxis] + eta[i - 1][np.newaxis, :]\n",
    "        eta[i] = np.max(eta_candidate, axis=1)\n",
    "        backpointers[i] = np.argmax(eta_candidate, axis=1)\n",
    "    state_sequence = np.zeros(n, dtype=int)\n",
    "    state_sequence[-1] = np.argmax(eta[-1])\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        state_sequence[i] = backpointers[i + 1, state_sequence[i + 1]]\n",
    "    return state_sequence\n",
    "\n",
    "def pad_to_match(a, b):\n",
    "    \"\"\"\n",
    "    Pads the shorter array with its last element to match the length of the longer array.\n",
    "    \n",
    "    Args:\n",
    "        a (np.array): First array for comparison.\n",
    "        b (np.array): Second array for comparison.\n",
    "        \n",
    "    Returns:\n",
    "        np.array, np.array: The two arrays modified to have equal lengths.\n",
    "    \"\"\"\n",
    "    if len(a) == len(b):\n",
    "        return a, b\n",
    "    elif len(a) > len(b):\n",
    "        padding = np.full(len(a) - len(b), b[-1])\n",
    "        b_padded = np.concatenate((b, padding))\n",
    "        return a, b_padded\n",
    "    else:\n",
    "        padding = np.full(len(b) - len(a), a[-1])\n",
    "        a_padded = np.concatenate((a, padding))\n",
    "        return a_padded, b\n",
    "\n",
    "# Load and process the DataFrame\n",
    "# df = pd.read_csv('archive/utterances-2sp.csv')\n",
    "# df = preprocess_dataframe(df)\n",
    "# df.loc[(df['episode_order'] == 1) & (df['turn_order'] == 0), 'utterance'] = df.loc[(df['episode_order'] == 1) & (df['turn_order'] == 0), 'utterance'].apply(lambda x: 'yyyyy ' + ' '.join(x.split()))\n",
    "\n",
    "df = pd.read_csv('archive/processed_utterances-2sp.csv')\n",
    "\n",
    "# get all of the unique host ids that are not -1 with at least 500 episodes\n",
    "host_ids = df[(df['host_id'] != -1) & (df['is_host'] == True)].groupby('host_id')['episode'].nunique()\n",
    "host_ids = host_ids[host_ids >= 100].index\n",
    "\n",
    "print(f'Number of hosts: {len(host_ids)}')\n",
    "\n",
    "\n",
    "weighted_accuracy_sum = 0\n",
    "weighted_assume_0_accuracy_sum = 0\n",
    "total_words = 0\n",
    "total_episodes = 0\n",
    "\n",
    "for host_id in host_ids:\n",
    "\n",
    "    print(f'Host ID: {host_id}')\n",
    "\n",
    "\n",
    "    filtered_df = filter_episodes(df, host_id=host_id)\n",
    "\n",
    "    total_episodes += len(filtered_df['episode'].unique())\n",
    "\n",
    "    print(f'Number of episodes: {len(filtered_df[\"episode\"].unique())}')\n",
    "\n",
    "\n",
    "    print(f'Number of utterances: {len(filtered_df)}')\n",
    "\n",
    "    # load the preprocessed data\n",
    "    # filtered_df = pd.read_csv('archive/most_episodes_host.csv')\n",
    "\n",
    "    # Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "    words_series = filtered_df['utterance'].str.split().explode()\n",
    "    unique_words = set(words_series)\n",
    "    word_frequencies = words_series.value_counts().to_dict()\n",
    "    word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "    # print the number of unique words\n",
    "    print(f'Number of unique words: {len(unique_words)}')\n",
    "\n",
    "\n",
    "    episodes = filtered_df['episode'].unique()\n",
    "    np.random.shuffle(episodes)\n",
    "    split_index = int(len(episodes) * 0.8)\n",
    "    train_episodes = episodes[:split_index]\n",
    "    test_episodes = episodes[split_index:]\n",
    "\n",
    "    test = filtered_df[filtered_df['episode'].isin(test_episodes)]\n",
    "\n",
    "    transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(filtered_df[(filtered_df['is_host'] == True) & (filtered_df['episode'].isin(train_episodes))], word_indices, len(unique_words))\n",
    "    transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(filtered_df[(filtered_df['is_host'] == False) & (filtered_df['episode'].isin(train_episodes))], word_indices, len(unique_words))\n",
    "    tensor, word_to_index = tensor_trouble(filtered_df[filtered_df['episode'].isin(train_episodes)], word_indices, len(unique_words))\n",
    "    tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "    initial = np.array([0.5, 0.5])\n",
    "    emission = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "    average_accuracy = 0\n",
    "    average_assume_0_accuracy = 0\n",
    "    accuracy_list = []\n",
    "    assume_0_accuracy_list = []\n",
    "    iters = len(test['episode'].unique())\n",
    "\n",
    "    total_host_words = filtered_df['utterance'].str.split().explode().shape[0]\n",
    "    if iters == 0:\n",
    "        continue\n",
    "\n",
    "    for i in range(iters):\n",
    "        test_episode = np.random.choice(test['episode'].unique())\n",
    "        test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "        test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "        test_episode_df = test_episode_df.explode('utterance')\n",
    "        test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "        test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "        test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "        test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "        obs = [0] + test_episode_word_indices  # Add start token index (e.g., 0) if your Viterbi expects it\n",
    "\n",
    "        state_sequence = tensor_viterbi(obs, tensor_normalized, emission, initial)\n",
    "        # print(len(state_sequence))\n",
    "        test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "        # Now, calculate the accuracy\n",
    "        accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "        assume_0_accuracy = np.mean((test_label_padded == 0).astype(int))\n",
    "        accuracy = max(accuracy, 1 - accuracy)  # Adjust based on expected behavior\n",
    "        assume_0_accuracy = max(assume_0_accuracy, 1 - assume_0_accuracy)\n",
    "\n",
    "        average_accuracy += accuracy\n",
    "        average_assume_0_accuracy += assume_0_accuracy\n",
    "        accuracy_list.append(accuracy)\n",
    "        assume_0_accuracy_list.append(assume_0_accuracy)\n",
    "\n",
    "    print(f'Average Accuracy: {average_accuracy / iters}')\n",
    "    print(f'Average Assume 0 Accuracy: {average_assume_0_accuracy / iters}')\n",
    "    print(f'Variance of Accuracy: {np.var(accuracy_list)}')\n",
    "    print(f'Variance of Assume 0 Accuracy: {np.var(assume_0_accuracy_list)}')\n",
    "\n",
    "# Update weighted sums using total_host_words as the weight\n",
    "    weighted_accuracy_sum += (average_accuracy / iters) * total_host_words\n",
    "    weighted_assume_0_accuracy_sum += (average_assume_0_accuracy / iters) * total_host_words\n",
    "    total_words += total_host_words\n",
    "\n",
    "# Calculate the final weighted averages\n",
    "weighted_average_accuracy = weighted_accuracy_sum / total_words\n",
    "weighted_average_assume_0_accuracy = weighted_assume_0_accuracy_sum / total_words\n",
    "\n",
    "print(f'Weighted Average Accuracy: {weighted_average_accuracy}')\n",
    "print(f'Weighted Average Assume 0 Accuracy: {weighted_average_assume_0_accuracy}')\n",
    "print(f'Total Episodes: {total_episodes}')\n",
    "print(f'Total Words: {total_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('archive/processed_utterances-2sp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('archive/most_episodes_host.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
