{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import warnings\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import umap\n",
    "import ast\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix_efficient(df, word_to_index, d, initial_values=1):\n",
    "    \n",
    "    # Initialize the transition counts \n",
    "    transition_counts = np.ones((d, d), dtype=int) * initial_values\n",
    "    \n",
    "    # Loop through each utterance\n",
    "    for utterance in df['utterance']:\n",
    "        if type(utterance) != str:\n",
    "            try:\n",
    "                utterance = str(utterance)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "        words = utterance.split()\n",
    "        \n",
    "        # Loop through each word and the previous word in the utterance\n",
    "        for i in range(1, len(words)):\n",
    "            word1 = words[i - 1]\n",
    "            word2 = words[i]\n",
    "            \n",
    "            # Add to transition counts\n",
    "            if word1 in word_to_index and word2 in word_to_index:\n",
    "                index1 = word_to_index[word1]\n",
    "                index2 = word_to_index[word2]\n",
    "                transition_counts[index2, index1] += 1\n",
    "                \n",
    "    # Normalize the columns to construct the transition matrix\n",
    "    column_sums = transition_counts.sum(axis=0, keepdims=True)\n",
    "    transition_probabilities = transition_counts / column_sums\n",
    "    \n",
    "    return transition_probabilities, word_to_index\n",
    "\n",
    "\n",
    "def tensor_trouble(df, word_to_index, d, initial_values=1):\n",
    "    # Construct your emission matrix \n",
    "    \n",
    "    # Initialize the tensor\n",
    "    tensor = np.ones((d, 2, 2), dtype=int) * initial_values\n",
    "    \n",
    "    # For each episode \n",
    "    for episode, group in df.groupby('episode'):\n",
    "        if type(group['utterance']) != str:\n",
    "            group['utterance'] = group['utterance'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "            \n",
    "        # Get words by utterance\n",
    "        text = \" \".join(group['utterance'].astype(str))\n",
    "        words = text.split()\n",
    "        \n",
    "        # Get label for if it is host or not host\n",
    "        roles = np.concatenate(group.apply(lambda row: [row['is_host']] * len(str(row['utterance']).split()), axis=1).values)\n",
    "        \n",
    "        # Construct dictionary \n",
    "        for i in range(2, len(words)):\n",
    "            word = words[i - 2]\n",
    "            if word not in word_to_index:  # Check if the word exists in the dictionary\n",
    "                continue  # Skip this iteration if the word is not found\n",
    "\n",
    "            # get the roles of the current word and the next two words\n",
    "            next_role = roles[i - 1]\n",
    "            following_role = roles[i]\n",
    "\n",
    "            # get the index of the current word\n",
    "            current_index = word_to_index[word]\n",
    "\n",
    "            # get the index of the roles\n",
    "            current_role_index = 0 if next_role else 1\n",
    "            next_role_index = 0 if following_role else 1\n",
    "            \n",
    "            tensor[current_index, next_role_index, current_role_index] += 1\n",
    "            \n",
    "    return tensor, word_to_index\n",
    "\n",
    "\n",
    "def make_emission(host, guest):\n",
    "    emission = np.zeros((len(host), len(host), 2))\n",
    "    emission[:, :, 0] = host\n",
    "    emission[:, :, 1] = guest\n",
    "    return np.swapaxes(emission, 0, 1)\n",
    "\n",
    "\n",
    "def tensor_viterbi(obs, transition, emission, initial):\n",
    "    \n",
    "    # Initialize all parameters\n",
    "    b_eps = 1e-25\n",
    "    start_index = obs[0]\n",
    "    obs = obs[1:]\n",
    "    n = len(obs)\n",
    "    d = transition.shape[0]\n",
    "    eta = np.zeros((n, 2))\n",
    "    backpointers = np.zeros((n, 2), dtype=int)\n",
    "    eta[0] = np.log(initial) + np.log(emission[start_index, obs[1], :])\n",
    "    obs = obs[1:]\n",
    "    \n",
    "    # Perform the viterbi algorith modified to work on tensors \n",
    "    for i in range(1, n - 1):\n",
    "        \n",
    "        b = emission[obs[i - 1], obs[i], :]\n",
    "        \n",
    "        if np.any(b == 0):\n",
    "            \n",
    "            zero_index = np.where(b == 0)\n",
    "            b[zero_index] = b_eps\n",
    "            \n",
    "        eta_candidate = np.log(transition[obs[i - 1], :, :]) + np.log(b)[:, np.newaxis] + eta[i - 1][np.newaxis, :]\n",
    "        eta[i] = np.max(eta_candidate, axis=1)\n",
    "        backpointers[i] = np.argmax(eta_candidate, axis=1)\n",
    "        \n",
    "    state_sequence = np.zeros(n, dtype=int)\n",
    "    state_sequence[-1] = np.argmax(eta[-1])\n",
    "    \n",
    "    for i in range(n - 2, -1, -1):\n",
    "        state_sequence[i] = backpointers[i + 1, state_sequence[i + 1]]\n",
    "        \n",
    "    return state_sequence\n",
    "\n",
    "\n",
    "def pad_to_match(a, b):\n",
    "    \"\"\"\n",
    "    Pads the shorter array with its last element to match the length of the longer array.\n",
    "    \n",
    "    Args:\n",
    "        a (np.array): First array for comparison.\n",
    "        b (np.array): Second array for comparison.\n",
    "        \n",
    "    Returns:\n",
    "        np.array, np.array: The two arrays modified to have equal lengths.\n",
    "    \"\"\"\n",
    "    # If they are already the same size\n",
    "    if len(a) == len(b):\n",
    "        return a, b\n",
    "    \n",
    "    # If a needs padding\n",
    "    elif len(a) > len(b):\n",
    "        padding = np.full(len(a) - len(b), b[-1])\n",
    "        b_padded = np.concatenate((b, padding))\n",
    "        return a, b_padded\n",
    "    \n",
    "    # If b needs padding\n",
    "    else:\n",
    "        padding = np.full(len(b) - len(a), a[-1])\n",
    "        a_padded = np.concatenate((a, padding))\n",
    "        return a_padded, b\n",
    "    \n",
    "    \n",
    "def filter_episodes(df, host_id=None):\n",
    "    \"\"\"\n",
    "    Filters the episodes by host\n",
    "    \"\"\"\n",
    "    \n",
    "    df_filtered = df[df['host_id'] != -1]\n",
    "    \n",
    "    if host_id is None:  \n",
    "        top_host = df_filtered.groupby('host_id')['episode'].nunique().idxmax()\n",
    "        \n",
    "    else:\n",
    "        top_host = host_id\n",
    "    \n",
    "    # Filter the episodes by the specific host\n",
    "    top_host_episodes = df_filtered[df_filtered['host_id'] == top_host]['episode'].unique()\n",
    "    df_top_host_all_utterances = df[df['episode'].isin(top_host_episodes)]\n",
    "    utterance_counts = df_top_host_all_utterances.groupby('episode')['utterance'].count()\n",
    "    episodes_over_30 = utterance_counts[utterance_counts > 30].index\n",
    "    df_top_host_over_30 = df_top_host_all_utterances[df_top_host_all_utterances['episode'].isin(episodes_over_30)]\n",
    "    \n",
    "    return df_top_host_over_30.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "embedding_path = 'archive/embed_df_with_hosts_filtered.csv'\n",
    "utterance_path = 'archive/processed_utterances-2sp.csv'\n",
    "umap_components = 10\n",
    "initial_state_probabilities = np.array([.5, .5])\n",
    "\n",
    "# Load the data\n",
    "episode_embedding_df = pd.read_csv('archive/embed_df_with_hosts.csv')\n",
    "\n",
    "# get the host_ids that have at least 100 episodes\n",
    "host_ids = episode_embedding_df['host_id'].value_counts()[episode_embedding_df['host_id'].value_counts() > 100].index\n",
    "\n",
    "episode_embedding_df = episode_embedding_df[episode_embedding_df['host_id'].isin(host_ids)]\n",
    "# Convert the string into a list and then into an array of floats\n",
    "episode_embedding_df['embedding'] = episode_embedding_df['embedding'].apply(lambda x: np.array(ast.literal_eval(x)))\n",
    "utterance_df = pd.read_csv(utterance_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>is_host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good morning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good morning lulu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all right</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whats the latest</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>well the latest is that the lawyer for the first whistleblower is tweeting that he is now representing a second whistleblower who he say ha firsthand knowledge of these event around the president and ukraine</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the first whistleblower only had second and xxxxx knowledge</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>we also know that there are subpoena for white house document and state department document and personnel</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>we dont know how cooperative the administration will be with those request</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>we know that the former ambassador to ukraine marie xxxxx is expected to testify thursday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shes xxxxx removed because the president personal lawyer rudy giuliani and others did not feel she xxxxx loyal enough to the president</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         utterance  \\\n",
       "0                                                                                                                                                                                                     good morning   \n",
       "1                                                                                                                                                                                                good morning lulu   \n",
       "2                                                                                                                                                                                                        all right   \n",
       "3                                                                                                                                                                                                 whats the latest   \n",
       "4  well the latest is that the lawyer for the first whistleblower is tweeting that he is now representing a second whistleblower who he say ha firsthand knowledge of these event around the president and ukraine   \n",
       "5                                                                                                                                                      the first whistleblower only had second and xxxxx knowledge   \n",
       "6                                                                                                        we also know that there are subpoena for white house document and state department document and personnel   \n",
       "7                                                                                                                                       we dont know how cooperative the administration will be with those request   \n",
       "8                                                                                                                        we know that the former ambassador to ukraine marie xxxxx is expected to testify thursday   \n",
       "9                                                                           shes xxxxx removed because the president personal lawyer rudy giuliani and others did not feel she xxxxx loyal enough to the president   \n",
       "\n",
       "   is_host  \n",
       "0     True  \n",
       "1    False  \n",
       "2     True  \n",
       "3     True  \n",
       "4    False  \n",
       "5    False  \n",
       "6    False  \n",
       "7    False  \n",
       "8    False  \n",
       "9    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "display(utterance_df[['utterance', 'is_host']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>is_host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The impeachment inquiry picks up tomorrow where it left off Friday, with a subpoena sent to a White House that's used to ignoring congressional requests, a State Department that missed its own subpoena deadline and investigators poring over text messages between U.S. diplomats discussing what exactly the president wanted from Ukraine.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just this morning, the lawyer for the whistleblower, whose complaint is at the base of this inquiry, says he's representing another whistleblower.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There's are a lot of moving parts.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fortunately, NPR's Mara Liasson is here to help.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good morning.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Good morning, Lulu.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All right.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What's the latest?</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Well, the latest is that the lawyer for the first whistleblower is tweeting that he is now representing a second whistleblower, who he says has firsthand knowledge of these events around the president and Ukraine.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The first whistleblower only had second and thirdhand knowledge.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>We also know that there are subpoenas for White House documents and State Department documents and personnel.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We don't know how cooperative the administration will be with those requests.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>We know that the former ambassador to Ukraine, Marie Yovanovitch, is expected to testify Thursday.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>She's was removed because the president's personal lawyer Rudy Giuliani and others did not feel she was loyal enough to the president.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>And we that the latest bombshell - the text messages between the three U.S. diplomats - have already led to more requests for information.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                           utterance  \\\n",
       "0   The impeachment inquiry picks up tomorrow where it left off Friday, with a subpoena sent to a White House that's used to ignoring congressional requests, a State Department that missed its own subpoena deadline and investigators poring over text messages between U.S. diplomats discussing what exactly the president wanted from Ukraine.   \n",
       "1                                                                                                                                                                                                 Just this morning, the lawyer for the whistleblower, whose complaint is at the base of this inquiry, says he's representing another whistleblower.   \n",
       "2                                                                                                                                                                                                                                                                                                                 There's are a lot of moving parts.   \n",
       "3                                                                                                                                                                                                                                                                                                   Fortunately, NPR's Mara Liasson is here to help.   \n",
       "4                                                                                                                                                                                                                                                                                                                                      Good morning.   \n",
       "5                                                                                                                                                                                                                                                                                                                                Good morning, Lulu.   \n",
       "6                                                                                                                                                                                                                                                                                                                                         All right.   \n",
       "7                                                                                                                                                                                                                                                                                                                                 What's the latest?   \n",
       "8                                                                                                                              Well, the latest is that the lawyer for the first whistleblower is tweeting that he is now representing a second whistleblower, who he says has firsthand knowledge of these events around the president and Ukraine.   \n",
       "9                                                                                                                                                                                                                                                                                   The first whistleblower only had second and thirdhand knowledge.   \n",
       "10                                                                                                                                                                                                                                     We also know that there are subpoenas for White House documents and State Department documents and personnel.   \n",
       "11                                                                                                                                                                                                                                                                     We don't know how cooperative the administration will be with those requests.   \n",
       "12                                                                                                                                                                                                                                                We know that the former ambassador to Ukraine, Marie Yovanovitch, is expected to testify Thursday.   \n",
       "13                                                                                                                                                                                                            She's was removed because the president's personal lawyer Rudy Giuliani and others did not feel she was loyal enough to the president.   \n",
       "14                                                                                                                                                                                                        And we that the latest bombshell - the text messages between the three U.S. diplomats - have already led to more requests for information.   \n",
       "\n",
       "    is_host  \n",
       "0      True  \n",
       "1      True  \n",
       "2      True  \n",
       "3      True  \n",
       "4      True  \n",
       "5     False  \n",
       "6      True  \n",
       "7      True  \n",
       "8     False  \n",
       "9     False  \n",
       "10    False  \n",
       "11    False  \n",
       "12    False  \n",
       "13    False  \n",
       "14    False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.read_csv('archive/utterances-2sp.csv')[['utterance', 'is_host']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting GMM with 6 components\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get the number of unique hosts\n",
    "host_ids = episode_embedding_df['host_id'].unique()\n",
    "n_components = 6\n",
    "\n",
    "episodes = episode_embedding_df['episode'].unique()\n",
    "np.random.shuffle(episodes)\n",
    "split_index = int(len(episodes) * 0.8)\n",
    "train_episodes = episodes[:split_index]\n",
    "test_episodes = episodes[split_index:]\n",
    "\n",
    "\n",
    "X_train = episode_embedding_df[episode_embedding_df['episode'].isin(train_episodes)]['embedding']\n",
    "X_test = episode_embedding_df[episode_embedding_df['episode'].isin(test_episodes)]['embedding']\n",
    "y_train = episode_embedding_df[episode_embedding_df['episode'].isin(train_episodes)]['host_id']\n",
    "y_test = episode_embedding_df[episode_embedding_df['episode'].isin(test_episodes)]['host_id']\n",
    "\n",
    "# Convert the labels to integers\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "# Convert the lists of floats into numpy arrays\n",
    "X_train = np.array([np.array(x) for x in X_train])\n",
    "X_test = np.array([np.array(x) for x in X_test])\n",
    "# Reduce dimensions to 20 with UMAP\n",
    "umap_reducer = umap.UMAP(n_components=umap_components)\n",
    "X_reduced = umap_reducer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "print(f\"Fitting GMM with {n_components} components\")\n",
    "gmm = GaussianMixture(n_components=n_components)\n",
    "gmm.fit(X_reduced)\n",
    "\n",
    "# Predict cluster labels\n",
    "train_cluster_labels = gmm.predict(X_reduced)\n",
    "\n",
    "# reduce the dimensions of the test set\n",
    "X_test_reduced = umap_reducer.transform(X_test)\n",
    "\n",
    "# Predict the cluster labels of the test set\n",
    "test_cluster_labels = gmm.predict(X_test_reduced)\n",
    "\n",
    "# # create dictionary to map train cluster labels to embeddings\n",
    "train_cluster_to_embedding = {cluster: [] for cluster in set(train_cluster_labels)}\n",
    "for cluster, embedding in zip(train_cluster_labels, X_train):\n",
    "    train_cluster_to_embedding[cluster].append(embedding)\n",
    "\n",
    "# create dictionary to map embeddings to episode ids\n",
    "embedding_to_episode = {tuple(embedding): episode for embedding, episode in zip(episode_embedding_df['embedding'], episode_embedding_df['episode'])}\n",
    "\n",
    "# create dictionary to map train cluster labels to episoded ids\n",
    "train_cluster_to_episode = {cluster: [] for cluster in set(train_cluster_labels)}\n",
    "for cluster, embedding in zip(train_cluster_labels, X_train):\n",
    "    train_cluster_to_episode[cluster].append(embedding_to_episode[tuple(embedding)])\n",
    "\n",
    "# create dictionary to map test cluster labels to embeddings\n",
    "test_cluster_to_embedding = {cluster: [] for cluster in set(test_cluster_labels)}\n",
    "for cluster, embedding in zip(test_cluster_labels, X_test):\n",
    "    test_cluster_to_embedding[cluster].append(embedding)\n",
    "\n",
    "\n",
    "# create dictionary to map test cluster labels to episoded ids\n",
    "test_cluster_to_episode = {cluster: [] for cluster in set(test_cluster_labels)}\n",
    "for cluster, embedding in zip(test_cluster_labels, X_test):\n",
    "    test_cluster_to_episode[cluster].append(embedding_to_episode[tuple(embedding)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "266939\n",
      "63963\n",
      "Cluster 1\n",
      "128443\n",
      "31958\n",
      "Cluster 2\n",
      "132281\n",
      "34792\n",
      "Cluster 3\n",
      "43765\n",
      "10686\n",
      "Cluster 4\n",
      "136757\n",
      "34008\n",
      "Cluster 5\n",
      "158971\n",
      "39340\n"
     ]
    }
   ],
   "source": [
    "results_dict = {episode: [] for episode in episode_embedding_df[episode_embedding_df['episode'].isin(test_episodes)]['episode'].unique()}\n",
    "\n",
    "# Iterate through each unique test cluster label\n",
    "for cluster in set(test_cluster_labels):\n",
    "    print(f\"Cluster {cluster}\")\n",
    "    # Get the embeddings of the cluster\n",
    "    cluster_embeddings = test_cluster_to_embedding[cluster]\n",
    "    # Get the episode ids of the test episodes in the cluster\n",
    "    cluster_test_episodes = test_cluster_to_episode[cluster]\n",
    "\n",
    "    # Get the episode ids of the train episodes in the cluster\n",
    "    cluster_train_episodes = train_cluster_to_episode[cluster]\n",
    "\n",
    "    # Filter the utterance df to only include the episodes in the cluster for training and testing\n",
    "    filtered_df = utterance_df[utterance_df['episode'].isin(cluster_train_episodes + cluster_test_episodes)][['is_host', 'episode', 'utterance']]\n",
    "\n",
    "    # Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "    words_series = filtered_df['utterance'].str.split().explode()\n",
    "    unique_words = set(words_series)\n",
    "    word_frequencies = words_series.value_counts().to_dict()\n",
    "    word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "    # Filter the filter df to only include the episodes in the cluster for training and testing\n",
    "    training_df = filtered_df[filtered_df['episode'].isin(cluster_train_episodes)]\n",
    "    testing_df = filtered_df[filtered_df['episode'].isin(cluster_test_episodes)]\n",
    "\n",
    "    print(len(training_df))\n",
    "    print(len(testing_df))\n",
    "\n",
    "    # Get the transition matrix for the training data\n",
    "    transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(training_df[training_df['is_host'] == True], word_indices, len(unique_words))\n",
    "    transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(training_df[training_df['is_host'] == False], word_indices, len(unique_words))\n",
    "\n",
    "    # Get the tensor for the training data\n",
    "    tensor, word_to_index = tensor_trouble(training_df, word_indices, len(unique_words))\n",
    "\n",
    "    # Normalize the tensor\n",
    "    tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "    # Get the emission matrix for the training data\n",
    "    emission_matrix = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "    for episode in cluster_test_episodes:\n",
    "        if episode not in results_dict:\n",
    "            results_dict[episode] = []\n",
    "        test_episode_df = testing_df[testing_df['episode'] == episode]\n",
    "        test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "        test_episode_df = test_episode_df.explode('utterance')\n",
    "        test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "        test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "        test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "        test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "        obs = [0] + test_episode_word_indices  # Add start token index (e.g., 0) if your Viterbi expects it\n",
    "\n",
    "\n",
    "\n",
    "        state_sequence = tensor_viterbi(obs, tensor_normalized, emission_matrix, initial_state_probabilities)\n",
    "        # print(len(state_sequence))\n",
    "        test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "        # Now, calculate the accuracy\n",
    "        accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "\n",
    "        # Switch the labels if accuracy is less than 0.5\n",
    "        if accuracy < 0.5:\n",
    "            accuracy = 1 - accuracy\n",
    "            state_sequence_padded = 1 - state_sequence_padded\n",
    "        \n",
    "        assume_0 = (test_label_padded == 1).astype(int)\n",
    "        assume_0_accuracy = np.mean(assume_0)\n",
    "\n",
    "        # Switch the labels if accuracy is less than 0.5\n",
    "        if assume_0_accuracy < 0.5:\n",
    "            assume_0_accuracy = 1 - assume_0_accuracy\n",
    "            assume_0 = 1 - assume_0\n",
    "\n",
    "        # Calculate the confusion matrices\n",
    "        confusion = confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "        assume_0_confusion = confusion_matrix(test_label_padded, assume_0)\n",
    "\n",
    "        # Append the confusion matrices to the results dictionary\n",
    "        results_dict[episode].append({\n",
    "            'accuracy': accuracy,\n",
    "            'confusion': confusion,\n",
    "            'assume_0_accuracy': assume_0_accuracy,\n",
    "            'assume_0_confusion': assume_0_confusion,\n",
    "            'method': 'Clustering'\n",
    "        })\n",
    "\n",
    "# for host_id in utterance_df['host_id'].unique():\n",
    "#     print(f\"Host {host_id}\")\n",
    "#     # Filter the utterance and embedding dataframes for this specific host\n",
    "#     host_utterance_df = filter_episodes(utterance_df, host_id=host_id)\n",
    "\n",
    "#     # Get the episodes related to this host\n",
    "#     host_episodes = host_utterance_df['episode'].unique()\n",
    "\n",
    "#     # Process the utterances as before\n",
    "#     words_series = host_utterance_df['utterance'].str.split().explode()\n",
    "#     unique_words = set(words_series)\n",
    "#     word_frequencies = words_series.value_counts().to_dict()\n",
    "#     word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "    \n",
    "    \n",
    "\n",
    "#     training_df = host_utterance_df[host_utterance_df['episode'].isin(train_episodes)]\n",
    "#     print(len(training_df))\n",
    "#     testing_df = host_utterance_df[host_utterance_df['episode'].isin(test_episodes)]\n",
    "#     print(len(testing_df))\n",
    "\n",
    "#     # Calculate transition matrices and tensor for training data\n",
    "#     transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(training_df[training_df['is_host'] == True], word_indices, len(unique_words))\n",
    "#     transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(training_df[training_df['is_host'] == False], word_indices, len(unique_words))\n",
    "#     tensor, word_to_index = tensor_trouble(training_df, word_indices, len(unique_words))\n",
    "#     tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "#     # Emission matrix for training data\n",
    "#     emission_matrix = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "#     # Iterate through each episode to get all test words\n",
    "#     for episode in testing_df['episode'].unique():\n",
    "#         test_episode_df = testing_df[testing_df['episode'] == episode][['is_host', 'utterance']]\n",
    "#         test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "#         test_episode_df = test_episode_df.explode('utterance')\n",
    "#         test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "#         test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "#         test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "#         test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "        \n",
    "#         if not test_episode_word_indices:  # If the list is empty, skip to the next iteration\n",
    "#             print(f\"No words processed for episode {episode}, skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         obs = [0] + test_episode_word_indices  # Include start token\n",
    "\n",
    "#         # Get state sequence from viterbi algorithm \n",
    "#         state_sequence = tensor_viterbi(obs, tensor_normalized, emission_matrix, initial_state_probabilities)\n",
    "#         test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "#         accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "\n",
    "#         # Calculate the accuracy \n",
    "#         if accuracy < 0.5:\n",
    "#             accuracy = 1 - accuracy\n",
    "#             state_sequence_padded = 1 - state_sequence_padded\n",
    "\n",
    "#         assume_0 = (test_label_padded == 1).astype(int)\n",
    "#         assume_0_accuracy = np.mean(assume_0)\n",
    "\n",
    "#         # Take the larger of the accuracies since binary \n",
    "#         if assume_0_accuracy < 0.5: \n",
    "#             assume_0_accuracy = 1 - assume_0_accuracy\n",
    "#             assume_0 = 1 - assume_0\n",
    "\n",
    "#         # Create the confusion matrix\n",
    "#         confusion = confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "#         assume_0_confusion = confusion_matrix(test_label_padded, assume_0)\n",
    "\n",
    "#         # For new episode add to results dict \n",
    "#         if episode not in results_dict or 'Host' not in [result['method'] for result in results_dict[episode]]:\n",
    "#             results_dict[episode].append({\n",
    "#                 'accuracy': accuracy,\n",
    "#                 'confusion': confusion,\n",
    "#                 'assume_0_accuracy': assume_0_accuracy,\n",
    "#                 'assume_0_confusion': assume_0_confusion,\n",
    "#                 'method': 'Host'\n",
    "#             })\n",
    "#         else:\n",
    "#             print(f\"Duplicate or existing entry found for episode {episode} under Host method.\")\n",
    "\n",
    "# # Save the results dictionary to a file\n",
    "# with open('archive/results_dict.json', 'w') as f:\n",
    "#     f.write(str(results_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = 1\n",
    "\n",
    "# cluster_embeddings = test_cluster_to_embedding[cluster]\n",
    "# # Get the episode ids of the test episodes in the cluster\n",
    "# cluster_test_episodes = test_cluster_to_episode[cluster]\n",
    "\n",
    "# # Get the episode ids of the train episodes in the cluster\n",
    "# cluster_train_episodes = train_cluster_to_episode[cluster]\n",
    "\n",
    "# # Filter the utterance df to only include the episodes in the cluster for training and testing\n",
    "# filtered_df = utterance_df[utterance_df['episode'].isin(cluster_train_episodes + cluster_test_episodes)][['is_host', 'episode', 'utterance']]\n",
    "\n",
    "# # Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "# words_series = filtered_df['utterance'].str.split().explode()\n",
    "# unique_words = set(words_series)\n",
    "# word_frequencies = words_series.value_counts().to_dict()\n",
    "# word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "# # Filter the filter df to only include the episodes in the cluster for training and testing\n",
    "# training_df = filtered_df[filtered_df['episode'].isin(cluster_train_episodes)]\n",
    "# testing_df = filtered_df[filtered_df['episode'].isin(cluster_test_episodes)]\n",
    "\n",
    "# print(len(training_df))\n",
    "# print(len(testing_df))\n",
    "\n",
    "# # Get the transition matrix for the training data\n",
    "# transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(training_df[training_df['is_host'] == True], word_indices, len(unique_words))\n",
    "# transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(training_df[training_df['is_host'] == False], word_indices, len(unique_words))\n",
    "\n",
    "# # Get the tensor for the training data\n",
    "# tensor, word_to_index = tensor_trouble(training_df, word_indices, len(unique_words))\n",
    "\n",
    "# # Normalize the tensor\n",
    "# tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "# # Get the emission matrix for the training data\n",
    "# emission_matrix = make_emission(transition_matrix_host, transition_matrix_guest)\n",
    "\n",
    "episode = cluster_test_episodes[6]\n",
    "\n",
    "\n",
    "test_episode_df = testing_df[testing_df['episode'] == episode]\n",
    "test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "test_episode_df = test_episode_df.explode('utterance')\n",
    "test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "test_episode_word_indices = [word_indices.get(word, -1) for word in test_episode_words if word in word_indices]\n",
    "obs = [0] + test_episode_word_indices  # Add start token index (e.g., 0) if your Viterbi expects it\n",
    "\n",
    "\n",
    "\n",
    "state_sequence = tensor_viterbi(obs, tensor_normalized, emission_matrix, initial_state_probabilities)\n",
    "# print(len(state_sequence))\n",
    "test_label_padded, state_sequence_padded = pad_to_match(test_label, state_sequence)\n",
    "\n",
    "# Now, calculate the accuracy\n",
    "accuracy = np.mean((state_sequence_padded == test_label_padded).astype(int))\n",
    "\n",
    "# Switch the labels if accuracy is less than 0.5\n",
    "if accuracy < 0.5:\n",
    "    accuracy = 1 - accuracy\n",
    "    state_sequence_padded = 1 - state_sequence_padded\n",
    "\n",
    "assume_0 = (test_label_padded == 1).astype(int)\n",
    "assume_0_accuracy = np.mean(assume_0)\n",
    "\n",
    "# Switch the labels if accuracy is less than 0.5\n",
    "if assume_0_accuracy < 0.5:\n",
    "    assume_0_accuracy = 1 - assume_0_accuracy\n",
    "    assume_0 = 1 - assume_0\n",
    "\n",
    "# Calculate the confusion matrices\n",
    "confusion = confusion_matrix(test_label_padded, state_sequence_padded)\n",
    "assume_0_confusion = confusion_matrix(test_label_padded, assume_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462897526501767"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60267 not found in results_dict\n"
     ]
    }
   ],
   "source": [
    "eps = results_dict.keys()\n",
    "cluster_confusion = np.array([[0, 0], [0, 0]])\n",
    "# host_confusion = np.array([[0, 0], [0, 0]])\n",
    "\n",
    "cluster_accuracy = 0\n",
    "# host_accuracy = 0\n",
    "\n",
    "# Find the confusion matrices for cluster and host methods \n",
    "for e in eps:  \n",
    "    if e not in results_dict or len(results_dict[e]) == 0:\n",
    "        print(f\"Episode {e} not found in results_dict\")\n",
    "        continue\n",
    "    cluster = results_dict[e][0]\n",
    "    # if len(results_dict[e]) > 1:\n",
    "    #     host = results_dict[e][1]\n",
    "        # host_confusion += host['confusion']\n",
    "        # host_accuracy += host['accuracy']\n",
    "\n",
    "    cluster_confusion += cluster['confusion']\n",
    "    cluster_accuracy += cluster['accuracy']\n",
    "# results_dict[421][0]['confusion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster accuracy: 0.7758748904872238\n",
      "Cluster confusion matrix: [[0.69107441 0.01406547]\n",
      " [0.21694967 0.07791046]]\n"
     ]
    }
   ],
   "source": [
    "# normalize the confusion matrices\n",
    "cluster_confusion = cluster_confusion / cluster_confusion.sum()\n",
    "# host_confusion = host_confusion / host_confusion.sum()\n",
    "\n",
    "print(f\"Cluster accuracy: {cluster_accuracy / len(eps)}\")\n",
    "# print(f\"Host accuracy: {host_accuracy / len(eps)}\")\n",
    "print(f\"Cluster confusion matrix: {cluster_confusion}\")\n",
    "# print(f\"Host confusion matrix: {host_confusion}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
