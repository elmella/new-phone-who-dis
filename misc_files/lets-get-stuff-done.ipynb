{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import warnings\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('archive/utterances-2sp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_with_most_utterances(df):\n",
    "    \"\"\"\n",
    "    Identifies the episode with the most rows (utterances).\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the columns:\n",
    "                       'episode', 'episode_order', 'turn_order',\n",
    "                       'speaker_order', 'host_id', 'is_host', 'utterance'\n",
    "\n",
    "    Returns:\n",
    "    str: The episode identifier with the most utterances.\n",
    "    \"\"\"\n",
    "    # Group by 'episode', count the rows in each group, then find the episode with the maximum count\n",
    "    episode_counts = df.groupby('episode').size()\n",
    "    max_utterances_episode = episode_counts.idxmax()\n",
    "    \n",
    "    return max_utterances_episode\n",
    "\n",
    "largest_idx = episode_with_most_utterances(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the largest episode, column \"episode\" with index largest_idx\n",
    "largest_episode = df[df['episode'] == largest_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the function for tokenizing and lemmatizing\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Define the function to preprocess the data and create a new DataFrame of tokens\n",
    "def create_token_dataframe(dataframe):\n",
    "    # Process each utterance to tokenize and lemmatize\n",
    "    dataframe['utterance'] = dataframe['utterance'].str.lower().apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    dataframe['tokens'] = dataframe['utterance'].apply(tokenize_and_lemmatize)\n",
    "    # dataframe['num_tokens'] = [len(tokens) for tokens in dataframe['tokens']]\n",
    "    \n",
    "    # Explode the DataFrame to have each token as a separate row\n",
    "    tokens_df = dataframe.explode('tokens').rename(columns={'tokens': 'token'})\n",
    "    tokens_df = tokens_df.drop(columns=['utterance'])  # Optionally remove the original 'utterance' column\n",
    "\n",
    "    return tokens_df\n",
    "\n",
    "def build_vocabulary(tokens_df):\n",
    "    \"\"\"Builds a vocabulary from the tokens DataFrame.\"\"\"\n",
    "    vocabulary = {word: idx for idx, word in enumerate(tokens_df['token'].unique())}\n",
    "    return vocabulary\n",
    "\n",
    "def one_hot_encode(tokens_df, vocabulary):\n",
    "    \"\"\"\n",
    "    One-hot encodes the tokens based on the provided vocabulary.\n",
    "    \n",
    "    Parameters:\n",
    "    - tokens_df (pd.DataFrame): DataFrame where each row contains a token.\n",
    "    - vocabulary (dict): A dictionary mapping words to indices.\n",
    "    \n",
    "    Returns:\n",
    "    - csr_matrix: The one-hot encoded representation of the tokens.\n",
    "    - list: A list of words corresponding to each row of the one-hot encoded matrix.\n",
    "    \"\"\"\n",
    "    # Initialize a matrix of zeros with shape (number of tokens, vocabulary size)\n",
    "    # data = np.zeros((len(tokens_df), len(vocabulary)), dtype=int)\n",
    "    \n",
    "    # Prepare a list to store the labels (words) in order\n",
    "    labels = []\n",
    "    \n",
    "    # For each token, set the appropriate element to 1\n",
    "    for i, token in enumerate(tokens_df['token']):\n",
    "        # index = vocabulary[token]\n",
    "        # data[i, index] = 1\n",
    "        labels.append(token)  # Add the word to the labels list\n",
    "\n",
    "    # Convert to a sparse CSR matrix for efficiency\n",
    "    # one_hot_encoded_data = csr_matrix(data)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "# Load the spaCy model for English. This needs to be installed via spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase and remove punctuation from text using vectorized operations.\"\"\"\n",
    "    text = text.str.lower()\n",
    "    text = text.str.replace('[{}]'.format(string.punctuation), '', regex=True)\n",
    "    return text\n",
    "\n",
    "def tokenize_and_lemmatize_batch(text_series):\n",
    "    \"\"\"Tokenize and lemmatize a series of texts using spaCy.\"\"\"\n",
    "    # Process the texts as a batch using spaCy\n",
    "    docs = list(nlp.pipe(text_series))\n",
    "    # Extract lemmatized tokens for each document\n",
    "    tokens = [[token.lemma_ for token in doc] for doc in docs]\n",
    "    return tokens\n",
    "\n",
    "def create_token_dataframe_optimized(dataframe):\n",
    "    \"\"\"Optimized DataFrame creation with tokenization and lemmatization.\"\"\"\n",
    "    # Preprocess the utterance column\n",
    "    dataframe['utterance'] = preprocess_text(dataframe['utterance'])\n",
    "    # Tokenize and lemmatize in batch mode\n",
    "    dataframe['tokens'] = tokenize_and_lemmatize_batch(dataframe['utterance'])\n",
    "    # Explode the DataFrame to have each token as a separate row\n",
    "    tokens_df = dataframe.explode('tokens').rename(columns={'tokens': 'token'})\n",
    "    tokens_df = tokens_df.drop(columns=['utterance'])  # Optionally remove the original 'utterance' column\n",
    "    return tokens_df\n",
    "\n",
    "def build_vocabulary(tokens_df):\n",
    "    \"\"\"Builds a vocabulary from the tokens DataFrame, using efficient pandas operations.\"\"\"\n",
    "    vocabulary = tokens_df['token'].drop_duplicates().reset_index(drop=True).to_dict()\n",
    "    vocabulary = {v: k for k, v in vocabulary.items()}\n",
    "    return vocabulary\n",
    "\n",
    "def get_token_indices(tokens, vocabulary):\n",
    "    \"\"\"Get the indices of tokens based on the vocabulary.\"\"\"\n",
    "    indices = [vocabulary[token] for token in tokens]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function that will be used for tokenizing and lemmatizing\n",
    "def tokenize_and_lemmatize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize each token\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Modify the preprocess_and_vectorize_batch function\n",
    "def preprocess_and_vectorize_batch(dataframe, vectorizer=None, batch_size=10000):\n",
    "    if vectorizer is None:\n",
    "        # Directly integrate tokenize_and_lemmatize with the vectorizer\n",
    "        vectorizer = CountVectorizer(tokenizer=tokenize_and_lemmatize, preprocessor=None, lowercase=True)\n",
    "        is_fit = False\n",
    "    else:\n",
    "        is_fit = True\n",
    "    \n",
    "    processed_batches = []\n",
    "    for start_row in range(0, dataframe.shape[0], batch_size):\n",
    "        batch = dataframe.iloc[start_row:start_row+batch_size].copy()\n",
    "        batch['utterance'] = batch['utterance'].str.lower().apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "        if not is_fit:\n",
    "            vectorized_batch = vectorizer.fit_transform(batch['utterance'])\n",
    "            is_fit = True\n",
    "        else:\n",
    "            vectorized_batch = vectorizer.transform(batch['utterance'])\n",
    "        \n",
    "        processed_batches.append(vectorized_batch)\n",
    "    \n",
    "    vectorized_data = vstack(processed_batches)\n",
    "    return vectorized_data, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data, vectorizer = preprocess_and_vectorize_batch(largest_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 5, 9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 5, 9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 5, 9, 10, 11, 12, 13, 14, 15, 16, 11, 17, 10, 18, 19, 20, 21, 22, 15, 16, 11, 17, 10, 18, 19, 20, 21, 22, 15, 16, 11, 17, 10, 18, 19, 20, 21, 22, 15, 23, 10, 19, 24, 25, 26, 27, 28, 29, 11, 13, 30, 31, 32, 33, 15, 23, 10, 19, 24, 25, 26, 27, 28, 29, 11, 13, 30, 31, 32, 33, 15, 23, 10, 19, 24, 25, 26, 27, 28, 29, 11, 13, 30, 31, 32, 33, 7, 34, 35, 36, 37, 38, 39, 40, 32, 5, 41, 42, 43, 7, 34, 35, 36, 37, 38, 39, 40, 32, 5, 41, 42, 43, 7, 34, 35, 36, 37, 38, 39, 40, 32, 5, 41, 42, 43, 44, 45, 46, 47, 48, 11, 49, 50, 44, 45, 46, 47, 48, 11, 49, 50, 44, 45, 46, 47, 48, 11, 49, 50, 51, 52, 53, 54, 55, 56, 51, 52, 53, 54, 55, 56, 51, 52, 53, 54, 55, 56, 57, 57, 57, 58, 59, 58, 59, 58, 59, 5, 6, 60, 61, 62, 63, 15, 64, 55, 44, 45, 5, 6, 60, 61, 62, 63, 15, 64, 55, 44, 45, 5, 6, 60, 61, 62, 63, 15, 64, 55, 44, 45, 65, 44, 45, 65, 44, 45, 65, 44, 45, 66, 55, 67, 68, 37, 69, 70, 66, 55, 67, 68, 37, 69, 70, 66, 55, 67, 68, 37, 69, 70, 48, 71, 51, 72, 32, 5, 73, 74, 75, 7, 34, 76, 77, 55, 78, 32, 79, 48, 71, 51, 72, 32, 5, 73, 74, 75, 7, 34, 76, 77, 55, 78, 32, 79, 48, 71, 51, 72, 32, 5, 73, 74, 75, 7, 34, 76, 77, 55, 78, 32, 79, 10, 80, 15, 4, 55, 81, 82, 83, 84, 85, 32, 86, 73, 68, 10, 80, 15, 4, 55, 81, 82, 83, 84, 85, 32, 86, 73, 68, 10, 80, 15, 4, 55, 81, 82, 83, 84, 85, 32, 86, 73, 68, 87, 88, 89, 54, 55, 90, 91, 92, 37, 93, 87, 88, 89, 54, 55, 90, 91, 92, 37, 93, 87, 88, 89, 54, 55, 90, 91, 92, 37, 93, 87, 94, 60, 11, 95, 68, 88, 96, 97, 98, 69, 99, 87, 94, 60, 11, 95, 68, 88, 96, 97, 98, 69, 99, 87, 94, 60, 11, 95, 68, 88, 96, 97, 98, 69, 99, 100, 53, 101, 88, 71, 100, 53, 101, 88, 71, 100, 53, 101, 88, 71, 102, 103, 96, 104, 55, 105, 102, 103, 96, 104, 55, 105, 102, 103, 96, 104, 55, 105, 102, 102, 102, 11, 106, 11, 6, 44, 45, 107, 108, 11, 106, 11, 6, 44, 45, 107, 108, 11, 106, 11, 6, 44, 45, 107, 108, 40, 88, 51, 109, 55, 54, 110, 111, 50, 11, 112, 113, 40, 88, 51, 109, 55, 54, 110, 111, 50, 11, 112, 113, 40, 88, 51, 109, 55, 54, 110, 111, 50, 11, 112, 113, 57, 57, 57, 63, 99, 114, 94, 11, 95, 72, 60, 63, 99, 114, 94, 11, 95, 72, 60, 63, 99, 114, 94, 11, 95, 72, 60, 115, 116, 115, 116, 115, 116, 7, 51, 117, 11, 95, 40, 51, 118, 119, 37, 120, 121, 122, 51, 117, 88, 7, 51, 117, 11, 95, 40, 51, 118, 119, 37, 120, 121, 122, 51, 117, 88, 7, 51, 117, 11, 95, 40, 51, 118, 119, 37, 120, 121, 122, 51, 117, 88, 51, 88, 11, 123, 124, 37, 120, 87, 125, 7, 99, 119, 54, 55, 126, 32, 127, 37, 128, 51, 51, 88, 11, 123, 124, 37, 120, 87, 125, 7, 99, 119, 54, 55, 126, 32, 127, 37, 128, 51, 51, 88, 11, 123, 124, 37, 120, 87, 125, 7, 99, 119, 54, 55, 126, 32, 127, 37, 128, 51, 63, 11, 129, 10, 130, 68, 60, 53, 54, 11, 83, 123, 60, 54, 131, 51, 54, 37, 120, 55, 129, 10, 130, 63, 11, 129, 10, 130, 68, 60, 53, 54, 11, 83, 123, 60, 54, 131, 51, 54, 37, 120, 55, 129, 10, 130, 63, 11, 129, 10, 130, 68, 60, 53, 54, 11, 83, 123, 60, 54, 131, 51, 54, 37, 120, 55, 129, 10, 130, 103, 51, 132, 99, 101, 133, 122, 134, 55, 129, 10, 130, 103, 51, 132, 99, 101, 133, 122, 134, 55, 129, 10, 130, 103, 51, 132, 99, 101, 133, 122, 134, 55, 129, 10, 130, 51, 135, 136, 40, 137, 138, 91, 123, 51, 135, 136, 40, 137, 138, 91, 123, 51, 135, 136, 40, 137, 138, 91, 123, 70, 70, 70, 103, 139, 60, 11, 140, 32, 116, 141, 142, 143, 144, 68, 103, 139, 60, 11, 140, 32, 116, 141, 142, 143, 144, 68, 103, 139, 60, 11, 140, 32, 116, 141, 142, 143, 144, 68, 145, 138, 145, 138, 145, 138, 146, 53, 104, 37, 93, 147, 146, 53, 104, 37, 93, 147, 146, 53, 104, 37, 93, 147, 143, 70, 143, 70, 143, 70, 148, 149, 117, 69, 139, 148, 149, 117, 69, 139, 148, 149, 117, 69, 139, 148, 149, 150, 151, 11, 130, 7, 137, 55, 152, 37, 150, 151, 40, 139, 22, 60, 55, 153, 32, 55, 154, 148, 149, 150, 151, 11, 130, 7, 137, 55, 152, 37, 150, 151, 40, 139, 22, 60, 55, 153, 32, 55, 154, 148, 149, 150, 151, 11, 130, 7, 137, 55, 152, 37, 150, 151, 40, 139, 22, 60, 55, 153, 32, 55, 154, 137, 155, 156, 137, 155, 156, 137, 155, 156, 7, 44, 45, 157, 5, 158, 159, 19, 72, 160, 19, 161, 162, 163, 164, 54, 165, 7, 166, 167, 7, 44, 45, 157, 5, 158, 159, 19, 72, 160, 19, 161, 162, 163, 164, 54, 165, 7, 166, 167, 7, 44, 45, 157, 5, 158, 159, 19, 72, 160, 19, 161, 162, 163, 164, 54, 165, 7, 166, 167, 168, 11, 110, 169, 168, 11, 110, 169, 168, 11, 110, 169, 44, 45, 46, 170, 171, 172, 170, 171, 173, 99, 174, 44, 45, 46, 170, 171, 172, 170, 171, 173, 99, 174, 44, 45, 46, 170, 171, 172, 170, 171, 173, 99, 174, 87, 175, 10, 55, 78, 176, 37, 120, 87, 177, 87, 175, 10, 55, 78, 176, 37, 120, 87, 177, 87, 175, 10, 55, 78, 176, 37, 120, 87, 177, 7, 178, 96, 179, 99, 108, 110, 180, 181, 182, 11, 183, 184, 10, 11, 185, 186, 187, 188, 116, 7, 178, 96, 179, 99, 108, 110, 180, 181, 182, 11, 183, 184, 10, 11, 185, 186, 187, 188, 116, 7, 178, 96, 179, 99, 108, 110, 180, 181, 182, 11, 183, 184, 10, 11, 185, 186, 187, 188, 116, 57, 15, 53, 189, 190, 191, 57, 15, 53, 189, 190, 191, 57, 15, 53, 189, 190, 191, 192, 192, 192, 96, 174, 88, 99, 193, 26, 99, 194, 88, 96, 174, 88, 99, 193, 26, 99, 194, 88, 96, 174, 88, 99, 193, 26, 99, 194, 88, 145, 145, 145, 145, 145, 145, 51, 195, 196, 51, 195, 196, 51, 195, 196, 72, 197, 32, 51, 198, 60, 88, 96, 174, 199, 200, 201, 55, 54, 55, 202, 32, 203, 63, 204, 40, 205, 76, 144, 206, 207, 168, 72, 197, 32, 51, 198, 60, 88, 96, 174, 199, 200, 201, 55, 54, 55, 202, 32, 203, 63, 204, 40, 205, 76, 144, 206, 207, 168, 72, 197, 32, 51, 198, 60, 88, 96, 174, 199, 200, 201, 55, 54, 55, 202, 32, 203, 63, 204, 40, 205, 76, 144, 206, 207, 168, 7, 208, 195, 209, 19, 116, 210, 88, 197, 7, 208, 195, 209, 19, 116, 210, 88, 197, 7, 208, 195, 209, 19, 116, 210, 88, 197, 57, 57, 57, 63, 51, 60, 55, 211, 10, 88, 43, 63, 51, 60, 55, 211, 10, 88, 43, 63, 51, 60, 55, 211, 10, 88, 43, 7, 212, 213, 139, 60, 11, 111, 47, 69, 63, 212, 214, 37, 69, 215, 40, 51, 60, 139, 216, 217, 218, 111, 7, 212, 213, 139, 60, 11, 111, 47, 69, 63, 212, 214, 37, 69, 215, 40, 51, 60, 139, 216, 217, 218, 111, 7, 212, 213, 139, 60, 11, 111, 47, 69, 63, 212, 214, 37, 69, 215, 40, 51, 60, 139, 216, 217, 218, 111, 7, 143, 32, 55, 219, 137, 55, 220, 221, 7, 143, 32, 55, 219, 137, 55, 220, 221, 7, 143, 32, 55, 219, 137, 55, 220, 221, 103, 51, 60, 103, 51, 60, 103, 51, 60, 63, 96, 204, 222, 54, 88, 202, 51, 223, 142, 87, 224, 63, 96, 204, 222, 54, 88, 202, 51, 223, 142, 87, 224, 63, 96, 204, 222, 54, 88, 202, 51, 223, 142, 87, 224, 54, 51, 223, 142, 143, 11, 225, 88, 99, 104, 19, 226, 186, 99, 174, 54, 54, 51, 223, 142, 143, 11, 225, 88, 99, 104, 19, 226, 186, 99, 174, 54, 54, 51, 223, 142, 143, 11, 225, 88, 99, 104, 19, 226, 186, 99, 174, 54, 51, 55, 202, 32, 203, 51, 55, 202, 32, 203, 51, 55, 202, 32, 203, 96, 227, 87, 96, 227, 87, 96, 227, 87, 51, 228, 32, 229, 99, 230, 37, 53, 54, 11, 231, 232, 51, 228, 32, 229, 99, 230, 37, 53, 54, 11, 231, 232, 51, 228, 32, 229, 99, 230, 37, 53, 54, 11, 231, 232, 7, 51, 228, 32, 233, 54, 99, 114, 116, 7, 96, 114, 99, 7, 51, 228, 32, 233, 54, 99, 114, 116, 7, 96, 114, 99, 7, 51, 228, 32, 233, 54, 99, 114, 116, 7, 96, 114, 99, 57, 137, 60, 190, 234, 10, 51, 235, 57, 137, 60, 190, 234, 10, 51, 235, 57, 137, 60, 190, 234, 10, 51, 235, 57, 96, 227, 87, 57, 96, 227, 87, 57, 96, 227, 87, 44, 45, 46, 182, 11, 183, 184, 10, 11, 185, 186, 187, 236, 116, 44, 45, 46, 182, 11, 183, 184, 10, 11, 185, 186, 187, 236, 116, 44, 45, 46, 182, 11, 183, 184, 10, 11, 185, 186, 187, 236, 116, 96, 104, 237, 55, 238, 96, 104, 237, 55, 238, 96, 104, 237, 55, 238, 139, 22, 239, 222, 54, 55, 240, 197, 32, 241, 242, 139, 22, 239, 222, 54, 55, 240, 197, 32, 241, 242, 139, 22, 239, 222, 54, 55, 240, 197, 32, 241, 242, 96, 132, 137, 243, 55, 152, 32, 111, 88, 107, 37, 244, 26, 47, 227, 122, 186, 99, 243, 7, 94, 241, 162, 3, 245, 117, 55, 246, 152, 32, 247, 117, 55, 246, 152, 32, 248, 249, 96, 132, 137, 243, 55, 152, 32, 111, 88, 107, 37, 244, 26, 47, 227, 122, 186, 99, 243, 7, 94, 241, 162, 3, 245, 117, 55, 246, 152, 32, 247, 117, 55, 246, 152, 32, 248, 249, 96, 132, 137, 243, 55, 152, 32, 111, 88, 107, 37, 244, 26, 47, 227, 122, 186, 99, 243, 7, 94, 241, 162, 3, 245, 117, 55, 246, 152, 32, 247, 117, 55, 246, 152, 32, 248, 249, 7, 66, 224, 250, 251, 7, 66, 224, 250, 251, 7, 66, 224, 250, 251, 96, 252, 88, 96, 53, 253, 37, 254, 255, 19, 11, 256, 96, 252, 88, 96, 53, 253, 37, 254, 255, 19, 11, 256, 96, 252, 88, 96, 53, 253, 37, 254, 255, 19, 11, 256, 7, 96, 257, 55, 258, 259, 260, 261, 163, 262, 263, 186, 264, 139, 265, 163, 116, 7, 96, 257, 55, 258, 259, 260, 261, 163, 262, 263, 186, 264, 139, 265, 163, 116, 7, 96, 257, 55, 258, 259, 260, 261, 163, 262, 263, 186, 264, 139, 265, 163, 116, 7, 51, 195, 250, 266, 155, 11, 110, 267, 268, 269, 270, 71, 7, 271, 7, 51, 195, 250, 266, 155, 11, 110, 267, 268, 269, 270, 71, 7, 271, 7, 51, 195, 250, 266, 155, 11, 110, 267, 268, 269, 270, 71, 7, 271, 7, 148, 272, 230, 7, 148, 273, 53, 274, 275, 276, 277, 88, 195, 214, 278, 10, 0, 279, 7, 148, 272, 230, 7, 148, 273, 53, 274, 275, 276, 277, 88, 195, 214, 278, 10, 0, 279, 7, 148, 272, 230, 7, 148, 273, 53, 274, 275, 276, 277, 88, 195, 214, 278, 10, 0, 279, 7, 148, 280, 281, 7, 282, 53, 228, 32, 4, 51, 54, 55, 283, 284, 37, 53, 254, 19, 11, 285, 7, 286, 287, 43, 147, 11, 288, 289, 290, 7, 148, 280, 281, 7, 282, 53, 228, 32, 4, 51, 54, 55, 283, 284, 37, 53, 254, 19, 11, 285, 7, 286, 287, 43, 147, 11, 288, 289, 290, 7, 148, 280, 281, 7, 282, 53, 228, 32, 4, 51, 54, 55, 283, 284, 37, 53, 254, 19, 11, 285, 7, 286, 287, 43, 147, 11, 288, 289, 290, 44, 45, 46, 96, 187, 117, 291, 11, 292, 108, 293, 7, 148, 294, 230, 108, 11, 295, 44, 45, 46, 96, 187, 117, 291, 11, 292, 108, 293, 7, 148, 294, 230, 108, 11, 295, 44, 45, 46, 96, 187, 117, 291, 11, 292, 108, 293, 7, 148, 294, 230, 108, 11, 295, 87, 94, 296, 88, 132, 26, 99, 297, 137, 195, 55, 283, 284, 88, 148, 298, 53, 214, 37, 254, 19, 11, 288, 285, 87, 94, 296, 88, 132, 26, 99, 297, 137, 195, 55, 283, 284, 88, 148, 298, 53, 214, 37, 254, 19, 11, 288, 285, 87, 94, 296, 88, 132, 26, 99, 297, 137, 195, 55, 283, 284, 88, 148, 298, 53, 214, 37, 254, 19, 11, 288, 285, 296, 51, 132, 134, 250, 299, 296, 51, 132, 134, 250, 299, 296, 51, 132, 134, 250, 299, 57, 88, 300, 60, 190, 301, 37, 51, 57, 88, 300, 60, 190, 301, 37, 51, 57, 88, 300, 60, 190, 301, 37, 51, 287, 43, 60, 55, 90, 302, 96, 303, 233, 54, 304, 287, 43, 60, 55, 90, 302, 96, 303, 233, 54, 304, 287, 43, 60, 55, 90, 302, 96, 303, 233, 54, 304, 19, 116, 88, 195, 305, 32, 11, 111, 88, 96, 257, 117, 306, 11, 305, 32, 307, 11, 308, 7, 309, 71, 228, 32, 43, 96, 257, 117, 306, 19, 116, 88, 195, 305, 32, 11, 111, 88, 96, 257, 117, 306, 11, 305, 32, 307, 11, 308, 7, 309, 71, 228, 32, 43, 96, 257, 117, 306, 19, 116, 88, 195, 305, 32, 11, 111, 88, 96, 257, 117, 306, 11, 305, 32, 307, 11, 308, 7, 309, 71, 228, 32, 43, 96, 257, 117, 306, 7, 87, 310, 3, 245, 55, 242, 108, 134, 135, 224, 10, 144, 311, 37, 312, 313, 230, 314, 7, 314, 7, 314, 7, 87, 310, 3, 245, 55, 242, 108, 134, 135, 224, 10, 144, 311, 37, 312, 313, 230, 314, 7, 314, 7, 314, 7, 87, 310, 3, 245, 55, 242, 108, 134, 135, 224, 10, 144, 311, 37, 312, 313, 230, 314, 7, 314, 7, 314, 7, 96, 303, 233, 88, 11, 315, 32, 316, 108, 139, 265, 60, 54, 11, 317, 318, 319, 11, 320, 266, 11, 311, 7, 96, 303, 233, 88, 11, 315, 32, 316, 108, 139, 265, 60, 54, 11, 317, 318, 319, 11, 320, 266, 11, 311, 7, 96, 303, 233, 88, 11, 315, 32, 316, 108, 139, 265, 60, 54, 11, 317, 318, 319, 11, 320, 266, 11, 311, 44, 45, 46, 35, 96, 321, 10, 11, 322, 323, 324, 72, 325, 326, 149, 327, 10, 144, 328, 44, 45, 46, 35, 96, 321, 10, 11, 322, 323, 324, 72, 325, 326, 149, 327, 10, 144, 328, 44, 45, 46, 35, 96, 321, 10, 11, 322, 323, 324, 72, 325, 326, 149, 327, 10, 144, 328, 7, 26, 99, 329, 116, 71, 96, 330, 7, 146, 331, 100, 51, 332, 100, 51, 332, 100, 51, 332, 7, 26, 99, 329, 116, 71, 96, 330, 7, 146, 331, 100, 51, 332, 100, 51, 332, 100, 51, 332, 7, 26, 99, 329, 116, 71, 96, 330, 7, 146, 331, 100, 51, 332, 100, 51, 332, 100, 51, 332, 139, 60, 226, 186, 10, 55, 333, 92, 139, 60, 226, 186, 10, 55, 333, 92, 139, 60, 226, 186, 10, 55, 333, 92, 139, 60, 226, 186, 10, 55, 334, 32, 335, 336, 139, 60, 226, 186, 10, 55, 334, 32, 335, 336, 139, 60, 226, 186, 10, 55, 334, 32, 335, 336, 7, 51, 337, 37, 338, 10, 339, 7, 99, 340, 341, 88, 29, 143, 7, 51, 337, 37, 338, 10, 339, 7, 99, 340, 341, 88, 29, 143, 7, 51, 337, 37, 338, 10, 339, 7, 99, 340, 341, 88, 29, 143, 99, 254, 19, 11, 285, 99, 254, 19, 11, 285, 99, 254, 19, 11, 285, 342, 221, 342, 221, 342, 221, 88, 195, 55, 343, 111, 37, 338, 88, 195, 55, 343, 111, 37, 338, 88, 195, 55, 343, 111, 37, 338, 192, 344, 192, 344, 192, 344, 96, 53, 76, 50, 241, 82, 345, 96, 53, 76, 50, 241, 82, 345, 96, 53, 76, 50, 241, 82, 345, 182, 54, 88, 149, 120, 68, 346, 347, 19, 144, 348, 68, 32, 143, 267, 182, 54, 88, 149, 120, 68, 346, 347, 19, 144, 348, 68, 32, 143, 267, 182, 54, 88, 149, 120, 68, 346, 347, 19, 144, 348, 68, 32, 143, 267, 192, 138, 192, 138, 192, 138, 117, 29, 143, 117, 29, 143, 117, 29, 143, 51, 195, 87, 349, 51, 195, 87, 349, 51, 195, 87, 349, 51, 195, 350, 51, 195, 54, 55, 258, 54, 349, 228, 32, 54, 55, 351, 88, 144, 352, 303, 69, 11, 68, 51, 195, 350, 51, 195, 54, 55, 258, 54, 349, 228, 32, 54, 55, 351, 88, 144, 352, 303, 69, 11, 68, 51, 195, 350, 51, 195, 54, 55, 258, 54, 349, 228, 32, 54, 55, 351, 88, 144, 352, 303, 69, 11, 68, 353, 243, 99, 102, 353, 243, 99, 102, 353, 243, 99, 102, 243, 99, 102, 243, 99, 102, 243, 99, 102, 66, 258, 94, 11, 68, 60, 66, 258, 94, 11, 68, 60, 66, 258, 94, 11, 68, 60, 96, 114, 96, 114, 96, 114, 51, 195, 51, 195, 51, 195, 243, 99, 102, 243, 99, 102, 243, 99, 102, 96, 354, 11, 355, 356, 51, 7, 96, 258, 357, 51, 96, 354, 11, 355, 356, 51, 7, 96, 258, 357, 51, 96, 354, 11, 355, 356, 51, 7, 96, 258, 357, 51, 51, 195, 135, 349, 51, 195, 135, 349, 51, 195, 135, 349, 102, 358, 102, 358, 102, 358, 44, 45, 46, 7, 35, 96, 254, 37, 11, 359, 32, 144, 360, 44, 45, 46, 7, 35, 96, 254, 37, 11, 359, 32, 144, 360, 44, 45, 46, 7, 35, 96, 254, 37, 11, 359, 32, 144, 360, 96, 227, 146, 361, 53, 37, 362, 99, 96, 227, 146, 361, 53, 37, 362, 99, 96, 227, 146, 361, 53, 37, 362, 99, 51, 55, 333, 43, 61, 51, 55, 333, 43, 61, 51, 55, 333, 43, 61, 51, 213, 60, 51, 213, 60, 51, 213, 60, 57, 205, 245, 137, 57, 205, 245, 137, 57, 205, 245, 137, 96, 132, 96, 195, 137, 87, 96, 363, 51, 96, 132, 96, 195, 137, 87, 96, 363, 51, 96, 132, 96, 195, 137, 87, 96, 363, 51, 296, 99, 104, 364, 365, 366, 296, 99, 104, 364, 365, 366, 296, 99, 104, 364, 365, 366, 296, 99, 104, 364, 367, 122, 368, 364, 32, 369, 43, 370, 10, 11, 371, 372, 296, 99, 104, 364, 367, 122, 368, 364, 32, 369, 43, 370, 10, 11, 371, 372, 296, 99, 104, 364, 367, 122, 368, 364, 32, 369, 43, 370, 10, 11, 371, 372, 138, 138, 138, 138, 138, 138, 96, 258, 340, 96, 258, 340, 96, 258, 340, 96, 132, 96, 340, 114, 178, 205, 373, 266, 11, 374, 3, 373, 63, 51, 53, 54, 298, 10, 11, 375, 32, 11, 376, 377, 378, 7, 99, 114, 379, 243, 380, 7, 11, 381, 243, 382, 37, 11, 383, 96, 132, 96, 340, 114, 178, 205, 373, 266, 11, 374, 3, 373, 63, 51, 53, 54, 298, 10, 11, 375, 32, 11, 376, 377, 378, 7, 99, 114, 379, 243, 380, 7, 11, 381, 243, 382, 37, 11, 383, 96, 132, 96, 340, 114, 178, 205, 373, 266, 11, 374, 3, 373, 63, 51, 53, 54, 298, 10, 11, 375, 32, 11, 376, 377, 378, 7, 99, 114, 379, 243, 380, 7, 11, 381, 243, 382, 37, 11, 383, 54, 384, 122, 385, 60, 54, 117, 11, 95, 111, 99, 187, 341, 54, 384, 122, 385, 60, 54, 117, 11, 95, 111, 99, 187, 341, 54, 384, 122, 385, 60, 54, 117, 11, 95, 111, 99, 187, 341, 241, 386, 57, 241, 386, 57, 241, 386, 57, 288, 387, 60, 388, 36, 37, 288, 387, 60, 388, 36, 37, 288, 387, 60, 388, 36, 37, 57, 54, 107, 50, 54, 389, 57, 54, 107, 50, 54, 389, 57, 54, 107, 50, 54, 389, 44, 45, 46, 51, 117, 11, 390, 44, 45, 46, 51, 117, 11, 390, 44, 45, 46, 51, 117, 11, 390, 51, 117, 11, 390, 51, 117, 11, 390, 51, 117, 11, 390, 61, 62, 61, 62, 61, 62, 15, 64, 391, 11, 6, 44, 45, 15, 64, 391, 11, 6, 44, 45, 15, 64, 391, 11, 6, 44, 45, 5, 170, 22, 60, 392, 393, 5, 170, 22, 60, 392, 393, 5, 170, 22, 60, 392, 393, 61, 358, 19, 384, 163, 394, 61, 358, 19, 384, 163, 394, 61, 358, 19, 384, 163, 394, 395, 99, 395, 99, 395, 99, 96, 174, 99, 396, 87, 224, 96, 174, 99, 396, 87, 224, 96, 174, 99, 396, 87, 224]\n",
      "3666\n",
      "3666\n",
      "0.5046372067648663\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and vectorize the largest episode\n",
    "tokens_df = create_token_dataframe(largest_episode)\n",
    "vocabulary = build_vocabulary(tokens_df)\n",
    "labels = get_token_indices(tokens_df['token'], vocabulary)\n",
    "label_set = set(labels)\n",
    "labels_as_indices = [list(label_set).index(label) for label in labels]\n",
    "print(labels_as_indices)\n",
    "# one_hot_encoded_data.shape\n",
    "# labels\n",
    "obs = np.array([labels_as_indices])\n",
    "n = 2\n",
    "m = obs.shape[1]\n",
    "\n",
    "# obs_flat = np.nonzero(obs)[1]\n",
    "\n",
    "# hmm\n",
    "h = hmm.CategoricalHMM(n_components=2, n_iter=200, tol=1e-4)\n",
    "h.fit(obs.reshape(-1, 1))\n",
    "# get the hidden states\n",
    "hidden_states = h.predict(obs.reshape(-1, 1))\n",
    "\n",
    "print(len(hidden_states))\n",
    "\n",
    "# create labels from speaker_order column\n",
    "labels = tokens_df['speaker_order'].values\n",
    "print(len(labels))\n",
    "\n",
    "# accuracy\n",
    "accuracy = np.mean(hidden_states == labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_order</th>\n",
       "      <th>turn_order</th>\n",
       "      <th>speaker_order</th>\n",
       "      <th>host_id</th>\n",
       "      <th>is_host</th>\n",
       "      <th>utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14199</th>\n",
       "      <td>713</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>The U.S. ambassador to Ukraine says she was pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>713</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>And the president has decided to allow one U.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14201</th>\n",
       "      <td>713</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>Now the two are exchanging gunfire with U.S. t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14202</th>\n",
       "      <td>713</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>Ron Elving joins us now, our senior Washington...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14203</th>\n",
       "      <td>713</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>Ron, thanks so much for being with us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134639</th>\n",
       "      <td>129584</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>She's the defending champion at the Olympics f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134640</th>\n",
       "      <td>129584</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>Mr. Brown, thanks so much for your time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134641</th>\n",
       "      <td>129584</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>All right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134642</th>\n",
       "      <td>129584</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>Thank you as well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134643</th>\n",
       "      <td>129584</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>Jeremaine Brown, sports broadcaster for Radio ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99790 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         episode  episode_order  turn_order  speaker_order  host_id  is_host   \n",
       "14199        713              1           0              0       12     True  \\\n",
       "14200        713              1           1              0       12     True   \n",
       "14201        713              1           2              0       12     True   \n",
       "14202        713              2           0              0       12     True   \n",
       "14203        713              2           1              0       12     True   \n",
       "...          ...            ...         ...            ...      ...      ...   \n",
       "1134639   129584             22           3              1       -1    False   \n",
       "1134640   129584             23           0              0       12     True   \n",
       "1134641   129584             24           0              1       -1    False   \n",
       "1134642   129584             24           1              1       -1    False   \n",
       "1134643   129584             25           0              0       12     True   \n",
       "\n",
       "                                                 utterance  \n",
       "14199    The U.S. ambassador to Ukraine says she was pu...  \n",
       "14200    And the president has decided to allow one U.S...  \n",
       "14201    Now the two are exchanging gunfire with U.S. t...  \n",
       "14202    Ron Elving joins us now, our senior Washington...  \n",
       "14203               Ron, thanks so much for being with us.  \n",
       "...                                                    ...  \n",
       "1134639  She's the defending champion at the Olympics f...  \n",
       "1134640           Mr. Brown, thanks so much for your time.  \n",
       "1134641                                         All right.  \n",
       "1134642                                 Thank you as well.  \n",
       "1134643  Jeremaine Brown, sports broadcaster for Radio ...  \n",
       "\n",
       "[99790 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_df_by_host(df, host_id):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include all rows from episodes hosted by the specified host_id.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The original DataFrame with columns 'episode', 'episode_order', 'turn_order',\n",
    "                         'speaker_order', 'host_id', 'is_host', 'utterance'.\n",
    "    - host_id (int): The host ID to filter episodes by.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame including all rows from episodes hosted by the given host_id.\n",
    "    \"\"\"\n",
    "    # Identify episodes hosted by the given host_id\n",
    "    hosted_episodes = df[(df['host_id'] == host_id) & (df['is_host'] == True)]['episode'].unique()\n",
    "    \n",
    "    # Filter the DataFrame for rows belonging to these episodes\n",
    "    filtered_df = df[df['episode'].isin(hosted_episodes)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Example usage:\n",
    "# Replace 123 with the actual host_id you're interested in\n",
    "filtered_df = filter_df_by_host(df, host_id=12)\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['episode'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 1\n",
      "part 2\n",
      "part 5\n",
      "part 6\n",
      "part 7\n",
      "1482808\n",
      "1482808\n",
      "0.5415286402555152\n"
     ]
    }
   ],
   "source": [
    "# Preprocess and vectorize the largest episode\n",
    "tokens_df = create_token_dataframe(filtered_df)\n",
    "print('part 1')\n",
    "vocabulary = build_vocabulary(tokens_df)\n",
    "print('part 2')\n",
    "labels = get_token_indices(tokens_df['token'], vocabulary)\n",
    "# print(labels)\n",
    "print('part 3')\n",
    "\n",
    "\n",
    "obs = np.array([labels])\n",
    "n = 2\n",
    "m = obs.shape[1]\n",
    "\n",
    "# obs_flat = np.nonzero(obs)[1]\n",
    "\n",
    "# hmm\n",
    "h = hmm.CategoricalHMM(n_components=n, n_iter=200, tol=1e-4)\n",
    "h.fit(obs.reshape(-1, 1))\n",
    "print('part 5')\n",
    "# get the hidden states\n",
    "hidden_states = h.predict(obs.reshape(-1, 1))\n",
    "\n",
    "print(len(hidden_states))\n",
    "\n",
    "# create labels from speaker_order column\n",
    "labels = tokens_df['speaker_order'].values\n",
    "print(len(labels))\n",
    "\n",
    "# accuracy\n",
    "accuracy = np.mean(hidden_states == labels)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
