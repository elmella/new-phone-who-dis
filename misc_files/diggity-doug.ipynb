{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import warnings\n",
    "from scipy.sparse import vstack\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# Load the spaCy model for English. This needs to be installed via spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 1240112/1240112 [00:36<00:00, 34383.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_threshold = 10\n",
    "\n",
    "# Preprocessing function to be applied to the entire DataFrame\n",
    "def preprocess_text(df):\n",
    "    # Normalize text: replace dashes with spaces, remove non-alphabetic characters except spaces\n",
    "    df['utterance'] = df['utterance'].str.replace('-', ' ').str.lower()\n",
    "    df['utterance'] = df['utterance'].apply(lambda x: re.sub(r'[^a-z\\s]', '', x))\n",
    "    return df\n",
    "\n",
    "# Function to tokenize and lemmatize with word frequency filtering\n",
    "def tokenize_and_lemmatize(text, word_frequencies):\n",
    "    words = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]\n",
    "    filtered_tokens = [word if word_frequencies.get(word, 0) >= word_threshold else 'xxxxx' for word in lemmatized_tokens]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def preprocess_batch(batch, word_frequencies):\n",
    "    # Apply tokenization and lemmatization with word filtering\n",
    "    batch['utterance'] = batch['utterance'].apply(lambda x: tokenize_and_lemmatize(x, word_frequencies))\n",
    "    # batch['is_question'] = batch['utterance'].str.contains(r'\\?').astype(int)\n",
    "    return batch\n",
    "\n",
    "# Main preprocessing function\n",
    "def preprocess_dataframe(dataframe, batch_size=60000):\n",
    "    # Apply initial preprocessing\n",
    "    dataframe = preprocess_text(dataframe)\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    all_words = ' '.join(dataframe['utterance']).split()\n",
    "    word_frequencies = Counter(all_words)\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    progress = tqdm(total=len(dataframe), desc=\"Processing batches\")\n",
    "    \n",
    "    processed_batches = []\n",
    "    for start_row in range(0, len(dataframe), batch_size):\n",
    "        end_row = start_row + batch_size\n",
    "        batch = dataframe.iloc[start_row:end_row]\n",
    "        processed_batch = preprocess_batch(batch, word_frequencies)\n",
    "        processed_batches.append(processed_batch)\n",
    "        \n",
    "        # Update progress\n",
    "        progress.update(len(batch))\n",
    "        \n",
    "    progress.close()  # Ensure the progress bar is closed after processing\n",
    "    \n",
    "    return pd.concat(processed_batches)\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('archive/utterances-2sp.csv')\n",
    "\n",
    "# Process the DataFrame\n",
    "df = preprocess_dataframe(df)\n",
    "\n",
    "# select rows with df['episode_order'] == 1 and df['turn_order'] == 1, turn the first word of the utterance into 'yyyyy'\n",
    "df.loc[(df['episode_order'] == 1) & (df['turn_order'] == 0), 'utterance'] = df.loc[(df['episode_order'] == 1) & (df['turn_order'] == 0), 'utterance'].apply(lambda x: 'yyyyy ' + ' '.join(x.split()[1:]))\n",
    "\n",
    "\n",
    "# Save the processed DataFrame\n",
    "# df_processed.to_csv('archive/processed_utterances-2sp.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_episodes(df, host_id=None):\n",
    "    # Exclude host with ID -1\n",
    "    df_filtered = df[df['host_id'] != -1]\n",
    "\n",
    "    if host_id is None:\n",
    "        # Find the host with the most episodes\n",
    "        top_host = df_filtered.groupby('host_id')['episode'].nunique().idxmax()\n",
    "    else:\n",
    "        top_host = host_id\n",
    "\n",
    "    # Get all episodes hosted by the top host\n",
    "    top_host_episodes = df_filtered[df_filtered['host_id'] == top_host]['episode'].unique()\n",
    "\n",
    "    # Filter the DataFrame to only include utterances from these episodes\n",
    "    df_top_host_all_utterances = df[df['episode'].isin(top_host_episodes)]\n",
    "\n",
    "    # Count the number of utterances in each episode\n",
    "    utterance_counts = df_top_host_all_utterances.groupby('episode')['utterance'].count()\n",
    "\n",
    "    # Get the episodes with more than 30 utterances\n",
    "    episodes_over_30 = utterance_counts[utterance_counts > 30].index\n",
    "\n",
    "    # Filter the DataFrame to only include these episodes\n",
    "    df_top_host_over_30 = df_top_host_all_utterances[df_top_host_all_utterances['episode'].isin(episodes_over_30)]\n",
    "\n",
    "    df_top_host_over_30 = df_top_host_over_30.reset_index(drop=True)\n",
    "\n",
    "    return df_top_host_over_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>episode_order</th>\n",
       "      <th>turn_order</th>\n",
       "      <th>speaker_order</th>\n",
       "      <th>host_id</th>\n",
       "      <th>is_host</th>\n",
       "      <th>utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>713</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>yyyyy u ambassador to ukraine say she wa pushe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>713</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>and the president ha decided to allow one u al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>713</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>now the two are exchanging gunfire with u troo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>713</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>ron elving join u now our senior washington ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>713</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>ron thanks so much for being with u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94509</th>\n",
       "      <td>136261</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>and you know and our little girl is three and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94510</th>\n",
       "      <td>136261</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>True</td>\n",
       "      <td>well scott thank you very much for talking wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94511</th>\n",
       "      <td>136261</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>True</td>\n",
       "      <td>and next time you go out to dinner in london</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94512</th>\n",
       "      <td>136261</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>i think were going to do indian take out actually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94513</th>\n",
       "      <td>136261</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>True</td>\n",
       "      <td>okay scott thank you very much for talking with u</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94514 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       episode  episode_order  turn_order  speaker_order  host_id  is_host  \\\n",
       "0          713              1           0              0       12     True   \n",
       "1          713              1           1              0       12     True   \n",
       "2          713              1           2              0       12     True   \n",
       "3          713              2           0              0       12     True   \n",
       "4          713              2           1              0       12     True   \n",
       "...        ...            ...         ...            ...      ...      ...   \n",
       "94509   136261             18           1              1       12    False   \n",
       "94510   136261             19           0              0      105     True   \n",
       "94511   136261             19           1              0      105     True   \n",
       "94512   136261             20           0              1       12    False   \n",
       "94513   136261             21           0              0      105     True   \n",
       "\n",
       "                                               utterance  \n",
       "0      yyyyy u ambassador to ukraine say she wa pushe...  \n",
       "1      and the president ha decided to allow one u al...  \n",
       "2      now the two are exchanging gunfire with u troo...  \n",
       "3      ron elving join u now our senior washington ed...  \n",
       "4                    ron thanks so much for being with u  \n",
       "...                                                  ...  \n",
       "94509  and you know and our little girl is three and ...  \n",
       "94510  well scott thank you very much for talking wit...  \n",
       "94511       and next time you go out to dinner in london  \n",
       "94512  i think were going to do indian take out actually  \n",
       "94513  okay scott thank you very much for talking with u  \n",
       "\n",
       "[94514 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = filter_episodes(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "words_series = df['utterance'].str.split().explode()\n",
    "\n",
    "# Convert the series to a set to get unique words\n",
    "unique_words = set(words_series)\n",
    "\n",
    "# get word frequencies\n",
    "word_frequencies = words_series.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 19812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 70118),\n",
       " ('and', 41326),\n",
       " ('a', 39255),\n",
       " ('to', 38977),\n",
       " ('of', 36614),\n",
       " ('that', 27877),\n",
       " ('in', 26306),\n",
       " ('it', 20515),\n",
       " ('you', 19575),\n",
       " ('i', 18091)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Number of unique words: {len(unique_words)}')\n",
    "word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "# sort the words by frequency and display the most common words\n",
    "sorted_word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_word_frequencies[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = df['episode'].unique()\n",
    "\n",
    "# Split the filtered episodes into training and testing sets\n",
    "train_episodes = np.random.choice(episodes, int(0.8 * len(episodes)), replace=False)\n",
    "train = df[df['episode'].isin(train_episodes)]\n",
    "test = df[~df['episode'].isin(train_episodes)]\n",
    "\n",
    "train_host = train[train['is_host'] == True]\n",
    "train_guest = train[train['is_host'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random test episode\n",
    "test_episode = np.random.choice(test['episode'].unique())\n",
    "test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "\n",
    "# Split utterances into words\n",
    "test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "\n",
    "# Expand the lists into separate rows, replicating the 'is_host' value for each word\n",
    "test_episode_df = test_episode_df.explode('utterance')\n",
    "\n",
    "# Now, each row in test_episode_df contains a single word and its corresponding 'is_host' value\n",
    "test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "\n",
    "# # get all of the words in the test episode\n",
    "test_episode_words = ' '.join(test_episode_df['utterance']).split()\n",
    "\n",
    "# # get the index of the words from unique_words\n",
    "\n",
    "# # get the indices of the words in the test episode\n",
    "test_episode_word_indices = [word_indices[word] for word in test_episode_words]\n",
    "\n",
    "# save the test episode word indices\n",
    "# np.save('archive/test_episode_word_indices.npy', test_episode_word_indices)\n",
    "\n",
    "# get the in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix_efficient(df, word_to_index, d):\n",
    "    # Initialize the transition counts matrix\n",
    "    transition_counts = np.ones((d, d), dtype=int)\n",
    "    \n",
    "    # Iterate over each utterance\n",
    "    for utterance in df['utterance']:\n",
    "        if type(utterance) != str:\n",
    "            try:\n",
    "                utterance = str(utterance)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        words = utterance.split()\n",
    "        for i in range(1, len(words)):\n",
    "            word1 = words[i - 1]\n",
    "            word2 = words[i]\n",
    "            if word1 in word_to_index and word2 in word_to_index:  # Check if both words are in the index\n",
    "                index1 = word_to_index[word1]\n",
    "                index2 = word_to_index[word2]\n",
    "                transition_counts[index2, index1] += 1  # Increment the count of the transition\n",
    "                \n",
    "    # # Handle columns with zero sum to avoid division by zero\n",
    "    column_sums = transition_counts.sum(axis=0, keepdims=True)\n",
    "    # zero_columns = column_sums == 0\n",
    "    # column_sums[zero_columns] = 1  # Avoid division by zero by setting 0 sums to 1\n",
    "    \n",
    "    # Now make column stochastic\n",
    "    transition_probabilities = transition_counts / column_sums\n",
    "\n",
    "    # make 0 columns have 1 / d probability for each word\n",
    "    # transition_probabilities[:, zero_columns.flatten()] = 1 / d\n",
    "    \n",
    "    return transition_probabilities, word_to_index\n",
    "\n",
    "# Apply the efficient function to get the transition matrices\n",
    "transition_matrix_host, word_to_index_host = get_transition_matrix_efficient(train_host, word_indices, len(unique_words))\n",
    "transition_matrix_guest, word_to_index_guest = get_transition_matrix_efficient(train_guest, word_indices, len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# test if columns sum to 1\n",
    "print(np.allclose(transition_matrix_host.sum(axis=0), 1))\n",
    "\n",
    "# check for any NaN values\n",
    "print(np.any(np.isnan(transition_matrix_host)))\n",
    "print(np.any(np.isnan(transition_matrix_guest)))\n",
    "\n",
    "# # save the transition matrices\n",
    "# np.save('vector-data/transition_matrix_host.npy', transition_matrix_host)\n",
    "# np.save('vector-data/transition_matrix_guest.npy', transition_matrix_guest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_trouble(df, word_to_index, d):\n",
    "    \n",
    "    tensor = np.ones((d, 2, 2), dtype=int)\n",
    "    # Group by episodes and concatenate text with speaker roles\n",
    "    for episode, group in df.groupby('episode'):\n",
    "        # Flatten all text into one string per episode\n",
    "        if type(group['utterance']) != str:\n",
    "            group['utterance'] = group['utterance'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "        text = \" \".join(group['utterance'].astype(str))\n",
    "        words = text.split()\n",
    "\n",
    "        # Replicate 'is_host' values for each word in the utterance\n",
    "        roles = np.concatenate(group.apply(lambda row: [row['is_host']] * len(str(row['utterance']).split()), axis=1).values)\n",
    "        \n",
    "        for i in range(2, len(words)):\n",
    "            # Get the current and next words\n",
    "            word = words[i - 2]\n",
    "            next_role = roles[i - 1]\n",
    "            following_role = roles[i]\n",
    "\n",
    "            # Get the current and next indices\n",
    "            current_index = word_to_index[word]\n",
    "            current_role_index = 0 if next_role else 1\n",
    "            next_role_index = 0 if following_role else 1\n",
    "\n",
    "\n",
    "            \n",
    "            # Skip if any word is not in the index (unlikely given preprocessing, but safe practice)\n",
    "            if current_index == -1:\n",
    "                continue\n",
    "            \n",
    "            # Update the tensor based on speaker transitions\n",
    "            tensor[current_index, next_role_index,current_role_index] += 1\n",
    "\n",
    "    return tensor, word_to_index\n",
    "\n",
    "\n",
    "tensor, word_to_index = tensor_trouble(train, word_indices, len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19812, 2, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the tensor to be column stochastic for each layer\n",
    "tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True))\n",
    "\n",
    "tensor_normalized.shape\n",
    "\n",
    "\n",
    "\n",
    "# check if the tensor is column stochastic, tensor_normalized.sum(axis=1) should be np.array([1., 1.])\n",
    "\n",
    "# save the tensor\n",
    "# np.save('vector-data/tensor.npy', tensor_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make emission matrix\n",
    "def make_emission(host, guest):\n",
    "    emission = np.zeros((len(host), len(host), 2))\n",
    "    emission[:,:,0] = host\n",
    "    emission[:,:,1] = guest\n",
    "\n",
    "    # reorder the axis\n",
    "    return np.swapaxes(emission, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_viterbi(obs, transition, emission, initial, structured = None):\n",
    "    \"\"\"Run the Viterbi algorithm with a conditioned tensor for the emission probabilities.\n",
    "    Inputs:\n",
    "        obs - ndarray (n,): observation sequence of indexes (includes a unique start token)\n",
    "        transition - ndarray (d,2,2): transition tensor of probabilities (index, row, col)\n",
    "        emission - ndarray (d,d,2): emission tensor of probabilities (index, row, col)\n",
    "        initial - ndarray (2,): initial state probabilities\n",
    "        structured - (optional) ndarray (n,): structured sequence of indexes\n",
    "\n",
    "    Outputs:\n",
    "        state_sequence - ndarray (n,): most likely state sequence of indexes\n",
    "    \"\"\"\n",
    "    # Matt Ignore structured variable\n",
    "\n",
    "    b_eps = 1e-25\n",
    "\n",
    "    # Get the lengths and correct indices\n",
    "    start_index = obs[0]\n",
    "    obs = obs[1:]\n",
    "    n = len(obs)\n",
    "    d = transition.shape[0]\n",
    "    \n",
    "    # Initialize the viterbi matrix and the backpointers\n",
    "    eta = np.zeros((n,2))\n",
    "    backpointers = np.zeros((n,2), dtype=int)\n",
    "\n",
    "    # Initialize the first row\n",
    "    eta[0] = np.log(initial) + np.log(emission[start_index,obs[1],:])\n",
    "    obs = obs[1:]\n",
    "\n",
    "    # Loop through the rest of the rows\n",
    "    for i in range(1,n-1):\n",
    "        b = emission[obs[i-1],obs[i],:]\n",
    "        # check if any of the emission probabilities are zero\n",
    "        if np.any(b == 0):\n",
    "            # find the index of the zero probability\n",
    "            zero_index = np.where(b == 0)\n",
    "            # replace the zero probability with a small epsilon value\n",
    "            b[zero_index] = b_eps\n",
    "            \n",
    "        eta_candidate = np.log(transition[obs[i-1],:,:]) + np.log(b)[:,np.newaxis] + eta[i-1][np.newaxis, :]\n",
    "        # eta_candidate = np.log(transition[obs[i-1],:,:]) * eta[i-1][np.newaxis, :] + np.log(emission[obs[i-1],obs[i],:])[:,np.newaxis] #### if statement goes here to replace 0 in the emission tensor\n",
    "        eta[i] = np.max(eta_candidate, axis=1)\n",
    "        backpointers[i] = np.argmax(eta_candidate, axis=1)\n",
    "\n",
    "    # Backtrack\n",
    "    state_sequence = np.zeros(n, dtype=int)\n",
    "    state_sequence[-1] = np.argmax(eta[-1])\n",
    "    for i in range(n-2,-1,-1):\n",
    "        state_sequence[i] = backpointers[i+1,state_sequence[i+1]]\n",
    "\n",
    "    # Return the state sequence\n",
    "    return state_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "initial = np.array([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission = make_emission(transition_matrix_host, transition_matrix_guest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# check if the word 'nan' is in test_episode_words\n",
    "print('nan' in test_episode_words)\n",
    "\n",
    "# how many times does 'nan' appear in test_episode_words\n",
    "print(test_episode_words.count('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.7625301157725464\n",
      "Average Assume 0 Accuracy: 0.6786954007754585\n",
      "Variance of Accuracy: 0.004524246132017364\n",
      "Variance of Assume 0 Accuracy: 0.004798637574034621\n"
     ]
    }
   ],
   "source": [
    "average_accuracy = 0\n",
    "average_assume_0_accuracy = 0\n",
    "accuracy_list = []\n",
    "assume_0_accuracy_list = []\n",
    "iters = len(test['episode'].unique())\n",
    "for i in range(iters):\n",
    "\n",
    "    # select a random test episode\n",
    "    test_episode = np.random.choice(test['episode'].unique())\n",
    "    test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "\n",
    "    # Split utterances into words\n",
    "    test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "\n",
    "    # Expand the lists into separate rows, replicating the 'is_host' value for each word\n",
    "    test_episode_df = test_episode_df.explode('utterance')\n",
    "\n",
    "    # Now, each row in test_episode_df contains a single word and its corresponding 'is_host' value\n",
    "    test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "\n",
    "    # get all of the words in the test episode\n",
    "    test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "\n",
    "    # remove the word 'nan' from the test episode words\n",
    "    test_episode_words = [word for word in test_episode_words if word != 'nan']\n",
    "\n",
    "    # get the indices of the words in the test episode\n",
    "    test_episode_word_indices = [word_indices[word] for word in test_episode_words]\n",
    "    obs = test_episode_word_indices\n",
    "\n",
    "    state_sequence = tensor_viterbi(obs, tensor_normalized, emission, initial)\n",
    "\n",
    "    # get the accuracy of the viterbi algorithm\n",
    "    accuracy = np.mean(state_sequence != test_label[1:])\n",
    "    assume_0_accuracy = np.mean(np.zeros(len(test_label[1:])) != test_label[1:])\n",
    "    if accuracy < 0.5:\n",
    "        accuracy = 1 - accuracy\n",
    "    if assume_0_accuracy < 0.5:\n",
    "        assume_0_accuracy = 1 - assume_0_accuracy\n",
    "\n",
    "    average_accuracy += accuracy\n",
    "    average_assume_0_accuracy += assume_0_accuracy\n",
    "    accuracy_list.append(accuracy)\n",
    "    assume_0_accuracy_list.append(assume_0_accuracy)\n",
    "\n",
    "print(f'Average Accuracy: {average_accuracy / iters}')\n",
    "print(f'Average Assume 0 Accuracy: {average_assume_0_accuracy / iters}')\n",
    "print(f'Variance of Accuracy: {np.var(accuracy_list)}')\n",
    "print(f'Variance of Assume 0 Accuracy: {np.var(assume_0_accuracy_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8238719068413392\n",
      "Episode: 77555\n"
     ]
    }
   ],
   "source": [
    "# select a random test episode\n",
    "test_episode = np.random.choice(test['episode'].unique())\n",
    "test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "\n",
    "# Split utterances into words\n",
    "test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "\n",
    "# Expand the lists into separate rows, replicating the 'is_host' value for each word\n",
    "test_episode_df = test_episode_df.explode('utterance')\n",
    "\n",
    "# Now, each row in test_episode_df contains a single word and its corresponding 'is_host' value\n",
    "test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "\n",
    "# # get all of the words in the test episode\n",
    "test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "\n",
    "# # get the index of the words from unique_words\n",
    "\n",
    "# # get the indices of the words in the test episode\n",
    "test_episode_word_indices = [word_indices[word] for word in test_episode_words]\n",
    "obs = test_episode_word_indices\n",
    "\n",
    "state_sequence = tensor_viterbi(obs, tensor_normalized, emission, initial)\n",
    "\n",
    "\n",
    "# get the accuracy of the viterbi algorithm\n",
    "accuracy = np.mean(state_sequence != test_label[1:])\n",
    "assume_0_accuracy = np.mean(np.zeros(len(test_label[1:])) != test_label[1:])\n",
    "if accuracy < 0.5:\n",
    "    accuracy = 1 - accuracy\n",
    "if assume_0_accuracy < 0.5:\n",
    "    assume_0_accuracy = 1 - assume_0_accuracy\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Episode: {test_episode}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
