{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe split\n",
      "Transition matrices calculated\n",
      "Tensor calculated\n",
      "Emission calculated\n",
      "Starting iterations\n",
      "Average Accuracy: 0.7068571894158199\n",
      "Average Assume 0 Accuracy: 0.6682842485125788\n"
     ]
    }
   ],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_threshold = 5\n",
    "\n",
    "# Precompiled regex patterns for efficiency\n",
    "pattern_dash = re.compile(r'-')\n",
    "pattern_non_alpha = re.compile(r'[^a-z\\s]')\n",
    "\n",
    "# Preprocessing function to be applied to the entire DataFrame\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Efficient text normalization and preprocessing.\"\"\"\n",
    "    text = pattern_dash.sub(' ', text.lower())\n",
    "    text = pattern_non_alpha.sub('', text)\n",
    "    return text\n",
    "\n",
    "def calculate_word_frequencies(df):\n",
    "    \"\"\"Calculate word frequencies in a DataFrame.\"\"\"\n",
    "    all_words = ' '.join(df['utterance']).split()\n",
    "    return Counter(all_words)\n",
    "\n",
    "def filter_low_frequency_words(word_frequencies, threshold):\n",
    "    \"\"\"Remove words with frequencies below a threshold.\"\"\"\n",
    "    return {word: freq for word, freq in word_frequencies.items() if freq >= threshold}\n",
    "\n",
    "def lemmatize_and_filter(tokens, word_frequencies):\n",
    "    \"\"\"Lemmatize tokens and filter based on frequency.\"\"\"\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return [token if token in word_frequencies else 'xxxxx' for token in lemmatized_tokens]\n",
    "\n",
    "def preprocess_dataframe(dataframe):\n",
    "    \"\"\"Preprocess the entire DataFrame with specified criteria.\"\"\"\n",
    "    # Filter episodes by speaker diversity and utterance count\n",
    "    speaker_counts = dataframe.groupby('episode')['is_host'].nunique()\n",
    "    utterance_counts = dataframe.groupby('episode').size()\n",
    "    \n",
    "    # Identify episodes with more than one speaker and more than 50 utterances\n",
    "    valid_episodes = speaker_counts[speaker_counts > 1].index.intersection(\n",
    "        utterance_counts[utterance_counts > 50].index\n",
    "    )\n",
    "    \n",
    "    # Filter the DataFrame to include only valid episodes\n",
    "    dataframe = dataframe[dataframe['episode'].isin(valid_episodes)]\n",
    "    \n",
    "    # Apply existing preprocessing steps\n",
    "    dataframe['utterance'] = dataframe['utterance'].apply(preprocess_text)\n",
    "    word_frequencies = calculate_word_frequencies(dataframe)\n",
    "    filtered_frequencies = filter_low_frequency_words(word_frequencies, word_threshold)\n",
    "    dataframe['utterance'] = dataframe['utterance'].apply(lambda x: ' '.join(lemmatize_and_filter(x.split(), filtered_frequencies)))\n",
    "    dataframe['is_question'] = dataframe['utterance'].str.contains(r'\\?').astype(int)\n",
    "    return dataframe\n",
    "\n",
    "def create_b_matrices(df, word_to_index, d):\n",
    "    \"\"\"Calculate transition matrix for word sequences in DataFrame.\"\"\"\n",
    "    # Initialize the transition matrix with zeros\n",
    "    transition_counts = np.zeros((d, d), dtype=np.int32)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        words = row['utterance'].split()\n",
    "        # Convert words to indices, ignoring those not found in `word_to_index`\n",
    "        indices = [word_to_index[word] for word in words if word in word_to_index]\n",
    "        \n",
    "        for i in range(1, len(indices)):\n",
    "            prev_index = indices[i - 1]\n",
    "            curr_index = indices[i]\n",
    "            transition_counts[curr_index, prev_index] += 1\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    column_sums = transition_counts.sum(axis=0)\n",
    "    # Avoid division by zero\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        transition_probabilities = np.divide(transition_counts, column_sums, where=column_sums!=0)\n",
    "        transition_probabilities[:, column_sums==0] = 1.0 / d  # Assign equal probability if no transitions\n",
    "    \n",
    "    return transition_probabilities\n",
    "\n",
    "def create_a_tensor(df, word_to_index, d):\n",
    "    \n",
    "    tensor = np.zeros((d, 2, 2), dtype=int)\n",
    "    # Group by episodes and concatenate text with speaker roles\n",
    "    for episode, group in df.groupby('episode'):\n",
    "        # Flatten all text into one string per episode\n",
    "        if type(group['utterance']) != str:\n",
    "            group['utterance'] = group['utterance'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "        text = \" \".join(group['utterance'].astype(str))\n",
    "        words = text.split()\n",
    "\n",
    "        # Replicate 'is_host' values for each word in the utterance\n",
    "        roles = np.concatenate(group.apply(lambda row: [row['is_host']] * len(str(row['utterance']).split()), axis=1).values)\n",
    "        \n",
    "        for i in range(2, len(words)):\n",
    "            # Get the current and next words\n",
    "            word = words[i - 2]\n",
    "            next_role = roles[i - 1]\n",
    "            following_role = roles[i]\n",
    "\n",
    "            # Get the current and next indices\n",
    "            current_index = word_to_index[word]\n",
    "            current_role_index = 0 if next_role else 1\n",
    "            next_role_index = 0 if following_role else 1\n",
    "\n",
    "\n",
    "            \n",
    "            # Skip if any word is not in the index (unlikely given preprocessing, but safe practice)\n",
    "            if current_index == -1:\n",
    "                continue\n",
    "            \n",
    "            # Update the tensor based on speaker transitions\n",
    "            tensor[current_index, next_role_index,current_role_index] += 1\n",
    "\n",
    "    return tensor, word_to_index\n",
    "\n",
    "\n",
    "# make emission matrix\n",
    "def make_emission(host, guest):\n",
    "    emission = np.zeros((len(host), len(host), 2))\n",
    "    emission[:,:,0] = host\n",
    "    emission[:,:,1] = guest\n",
    "\n",
    "    # reorder the axis\n",
    "    return np.swapaxes(emission, 0, 1)\n",
    "\n",
    "\n",
    "def tensor_viterbi(obs, transition, emission, initial, structured = None):\n",
    "    \"\"\"Run the Viterbi algorithm with a conditioned tensor for the emission probabilities.\n",
    "    Inputs:\n",
    "        obs - ndarray (n,): observation sequence of indexes (includes a unique start token)\n",
    "        transition - ndarray (d,2,2): transition tensor of probabilities (index, row, col)\n",
    "        emission - ndarray (d,d,2): emission tensor of probabilities (index, row, col)\n",
    "        initial - ndarray (2,): initial state probabilities\n",
    "        structured - (optional) ndarray (n,): structured sequence of indexes\n",
    "\n",
    "    Outputs:\n",
    "        state_sequence - ndarray (n,): most likely state sequence of indexes\n",
    "    \"\"\"\n",
    "    # Matt Ignore structured variable\n",
    "\n",
    "    b_eps = 1e-15\n",
    "\n",
    "    # Get the lengths and correct indices\n",
    "    start_index = obs[0]\n",
    "    obs = obs[1:]\n",
    "    n = len(obs)\n",
    "    d = transition.shape[0]\n",
    "    \n",
    "    # Initialize the viterbi matrix and the backpointers\n",
    "    eta = np.zeros((n,2))\n",
    "    backpointers = np.zeros((n,2), dtype=int)\n",
    "\n",
    "    # Initialize the first row\n",
    "    eta[0] = np.log(initial) + np.log(emission[start_index,obs[1],:])\n",
    "    obs = obs[1:]\n",
    "\n",
    "    # Loop through the rest of the rows\n",
    "    for i in range(1,n-1):\n",
    "        b = emission[obs[i-1],obs[i],:]\n",
    "        # check if any of the emission probabilities are zero\n",
    "        if np.any(b == 0):\n",
    "            # find the index of the zero probability\n",
    "            zero_index = np.where(b == 0)\n",
    "            # replace the zero probability with a small epsilon value\n",
    "            b[zero_index] = b_eps\n",
    "            \n",
    "        eta_candidate = np.log(transition[obs[i-1],:,:]) + np.log(b)[:,np.newaxis] + eta[i-1][np.newaxis, :]\n",
    "        # eta_candidate = np.log(transition[obs[i-1],:,:]) * eta[i-1][np.newaxis, :] + np.log(emission[obs[i-1],obs[i],:])[:,np.newaxis] #### if statement goes here to replace 0 in the emission tensor\n",
    "        eta[i] = np.max(eta_candidate, axis=1)\n",
    "        backpointers[i] = np.argmax(eta_candidate, axis=1)\n",
    "\n",
    "    # Backtrack\n",
    "    state_sequence = np.zeros(n, dtype=int)\n",
    "    state_sequence[-1] = np.argmax(eta[-1])\n",
    "    for i in range(n-2,-1,-1):\n",
    "        state_sequence[i] = backpointers[i+1,state_sequence[i+1]]\n",
    "\n",
    "    # Return the state sequence\n",
    "    return state_sequence\n",
    "\n",
    "\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv('archive/utterances-2sp.csv')\n",
    "\n",
    "# Process the DataFrame\n",
    "df_processed = preprocess_dataframe(df)\n",
    "\n",
    "# select rows with df['episode_order'] == 1 and df['turn_order'] == 1, turn the first word of the utterance into 'yyyyy'\n",
    "df_processed.loc[(df_processed['episode_order'] == 1) & (df_processed['turn_order'] == 0), 'utterance'] = df_processed.loc[(df_processed['episode_order'] == 1) & (df_processed['turn_order'] == 0), 'utterance'].apply(lambda x: 'yyyyy ' + ' '.join(x.split()[1:]))\n",
    "\n",
    "print('Dataframe processed')\n",
    "\n",
    "# Split each utterance into a list of words and explode the DataFrame to get a row per word\n",
    "words_series = df_processed['utterance'].str.split().explode()\n",
    "\n",
    "# Convert the series to a set to get unique words\n",
    "unique_words = set(words_series)\n",
    "\n",
    "word_indices = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "episodes = df_processed['episode'].unique()\n",
    "\n",
    "# Split the filtered episodes into training and testing sets\n",
    "train_episodes = np.random.choice(episodes, int(0.8 * len(episodes)), replace=False)\n",
    "train = df_processed[df_processed['episode'].isin(train_episodes)]\n",
    "test = df_processed[~df_processed['episode'].isin(train_episodes)]\n",
    "\n",
    "train_host = train[train['is_host'] == True]\n",
    "train_guest = train[train['is_host'] == False]\n",
    "\n",
    "print('Dataframe split')\n",
    "\n",
    "# Apply the efficient function to get the transition matrices\n",
    "b_matrix_host = create_b_matrices(train_host, word_indices, len(unique_words))\n",
    "b_matrix_gueset = create_b_matrices(train_guest, word_indices, len(unique_words))\n",
    "\n",
    "print('Transition matrices calculated')\n",
    "\n",
    "tensor, word_to_index = create_a_tensor(train, word_indices, len(unique_words))\n",
    "\n",
    "tensor_normalized = tensor / (tensor.sum(axis=1, keepdims=True) +  1e-9)\n",
    "\n",
    "print('Tensor calculated')\n",
    "\n",
    "initial = np.array([0.9, 0.1])\n",
    "\n",
    "emission = make_emission(b_matrix_host, b_matrix_gueset)\n",
    "\n",
    "print('Emission calculated')\n",
    "\n",
    "average_accuracy = 0\n",
    "average_assume_0_accuracy = 0\n",
    "iters = len(test['episode'].unique())\n",
    "print('Starting iterations')\n",
    "for i in range(iters):\n",
    "\n",
    "    # select a random test episode\n",
    "    test_episode = np.random.choice(test['episode'].unique())\n",
    "    test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "\n",
    "    # Split utterances into words\n",
    "    test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "\n",
    "    # Expand the lists into separate rows, replicating the 'is_host' value for each word\n",
    "    test_episode_df = test_episode_df.explode('utterance')\n",
    "\n",
    "    # Now, each row in test_episode_df contains a single word and its corresponding 'is_host' value\n",
    "    test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "\n",
    "    # # get all of the words in the test episode\n",
    "    test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "\n",
    "\n",
    "    #  get the indices of the words in the test episode\n",
    "    test_episode_word_indices = [word_indices[word] for word in test_episode_words]\n",
    "    obs = test_episode_word_indices\n",
    "\n",
    "    state_sequence = tensor_viterbi(obs, tensor_normalized, emission, initial)\n",
    "\n",
    "\n",
    "    # get the accuracy of the viterbi algorithm\n",
    "    accuracy = np.mean(state_sequence != test_label[1:])\n",
    "    assume_0_accuracy = np.mean(np.zeros(len(test_label[1:])) != test_label[1:])\n",
    "    if accuracy < 0.5:\n",
    "        accuracy = 1 - accuracy\n",
    "    if assume_0_accuracy < 0.5:\n",
    "        assume_0_accuracy = 1 - assume_0_accuracy\n",
    "    # print(f'Accuracy: {accuracy}')\n",
    "    # print(f'Episode: {test_episode}')\n",
    "    # print(f'Length of state sequence: {len(state_sequence)}')\n",
    "    # print(f'Percentage of host words: {np.mean(state_sequence)}')\n",
    "    average_accuracy += accuracy\n",
    "    average_assume_0_accuracy += assume_0_accuracy\n",
    "\n",
    "print(f'Average Accuracy: {average_accuracy / iters}')\n",
    "print(f'Average Assume 0 Accuracy: {average_assume_0_accuracy / iters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 'welcome' by the host: 62.43%\n",
      "Percentage of 'welcome' by the guest: 37.57%\n"
     ]
    }
   ],
   "source": [
    "# Filter utterances containing the word 'welcome' (case-insensitive)\n",
    "df_welcome = df[df['utterance'].str.contains('welcome', case=False, na=False)]\n",
    "\n",
    "# Calculate the total number of 'welcome' utterances\n",
    "total_welcome = len(df_welcome)\n",
    "\n",
    "# Calculate the number and percentage of 'welcome' utterances by the host\n",
    "host_welcome_count = df_welcome[df_welcome['is_host'] == True].shape[0]\n",
    "host_welcome_percentage = (host_welcome_count / total_welcome) * 100 if total_welcome > 0 else 0\n",
    "\n",
    "# Calculate the number and percentage of 'welcome' utterances by the guest\n",
    "guest_welcome_count = df_welcome[df_welcome['is_host'] == False].shape[0]\n",
    "guest_welcome_percentage = (guest_welcome_count / total_welcome) * 100 if total_welcome > 0 else 0\n",
    "\n",
    "print(f\"Percentage of 'welcome' by the host: {host_welcome_percentage:.2f}%\")\n",
    "print(f\"Percentage of 'welcome' by the guest: {guest_welcome_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95136778, 0.14692483],\n",
       "       [0.04863222, 0.85307517]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index for the word welcome\n",
    "welcome_index = word_indices['hi']\n",
    "\n",
    "# get the probability of the word welcome being spoken by the host\n",
    "tensor_normalized[welcome_index,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yyyyy', 'trump', 'ha', 'announced', 'another', 'new', 'immigration', 'rule', 'set', 'to', 'take', 'effect', 'next', 'month', 'though', 'it', 'likely', 'headed', 'to', 'federal', 'court', 'the', 'rule', 'would', 'require', 'foreign', 'national', 'to', 'prove', 'they', 'have', 'health', 'insurance', 'or', 'the', 'money', 'to', 'pay', 'for', 'their', 'own', 'health', 'care', 'cost', 'before', 'they', 'can', 'legally', 'enter', 'the', 'united', 'state', 'simply', 'put', 'no', 'proof', 'no', 'visa', 'the', 'president', 'said', 'this', 'action', 'is', 'necessary', 'to', 'quote', 'protect', 'the', 'availability', 'of', 'health', 'care', 'benefit', 'for', 'american', 'we', 'wanted', 'to', 'get', 'a', 'better', 'understanding', 'of', 'how', 'immigrant', 'use', 'and', 'pay', 'for', 'health', 'care', 'in', 'the', 'united', 'state', 'anne', 'xxxxx', 'is', 'an', 'associate', 'director', 'of', 'the', 'center', 'for', 'public', 'policy', 'priority', 'in', 'austin', 'texas', 'shes', 'spent', 'her', 'career', 'working', 'on', 'health', 'care', 'access', 'issue', 'and', 'she', 'join', 'u', 'now', 'welcome', 'good', 'morning', 'how', 'would', 'this', 'work', 'i', 'mean', 'if', 'youre', 'a', 'visitor', 'and', 'youre', 'traveling', 'you', 'would', 'obviously', 'get', 'travel', 'insurance', 'which', 'isnt', 'terribly', 'expensive', 'if', 'you', 'are', 'coming', 'to', 'immigrate', 'then', 'what', 'would', 'you', 'have', 'to', 'show', 'it', 'a', 'great', 'question', 'that', 'we', 'probably', 'cant', 'answer', 'it', 'sound', 'to', 'me', 'like', 'you', 'know', 'it', 'could', 'conceivably', 'require', 'the', 'creation', 'of', 'a', 'whole', 'new', 'segment', 'of', 'the', 'health', 'insurance', 'industry', 'thats', 'just', 'designed', 'to', 'answer', 'this', 'new', 'requirement', 'for', 'even', 'visiting', 'the', 'united', 'state', 'the', 'other', 'thing', 'that', 'xxxxx', 'me', 'a', 'little', 'bit', 'about', 'this', 'is', 'this', 'seems', 'like', 'an', 'additional', 'step', 'that', 'already', 'when', 'you', 'apply', 'for', 'a', 'visa', 'outside', 'of', 'the', 'united', 'state', 'you', 'do', 'have', 'to', 'prove', 'some', 'sort', 'of', 'financial', 'stability', 'youre', 'absolutely', 'right', 'even', 'for', 'these', 'short', 'term', 'visitor', 'type', 'visa', 'there', 'already', 'screening', 'to', 'try', 'to', 'detect', 'people', 'who', 'might', 'be', 'coming', 'here', 'without', 'mean', 'and', 'likely', 'to', 'for', 'example', 'xxxxx', 'their', 'visa', 'getting', 'insurance', 'in', 'this', 'country', 'is', 'complicated', 'for', 'american', 'it', 'is', 'even', 'more', 'complicated', 'if', 'youre', 'coming', 'into', 'this', 'country', 'and', 'presumably', 'if', 'you', 'have', 'a', 'work', 'visa', 'and', 'youre', 'being', 'sponsored', 'by', 'someone', 'you', 'would', 'have', 'insurance', 'already', 'so', 'it', 'unclear', 'a', 'little', 'bit', 'who', 'this', 'is', 'targeting', 'exactly', 'it', 'sound', 'frankly', 'more', 'like', 'the', 'intention', 'is', 'for', 'the', 'optic', 'of', 'the', 'policy', 'a', 'much', 'a', 'it', 'is', 'you', 'know', 'having', 'a', 'practical', 'system', 'in', 'place', 'to', 'do', 'it', 'if', 'you', 'read', 'the', 'document', 'that', 'wa', 'announced', 'you', 'find', 'that', 'it', 'actually', 'outline', 'many', 'of', 'the', 'other', 'thing', 'that', 'the', 'trump', 'administration', 'ha', 'either', 'proposed', 'or', 'is', 'in', 'the', 'process', 'of', 'trying', 'to', 'pursue', 'so', 'theyve', 'presented', 'it', 'in', 'their', 'own', 'document', 'a', 'part', 'of', 'what', 'advocate', 'for', 'immigrant', 'refer', 'to', 'a', 'the', 'invisible', 'wall', 'your', 'state', 'texas', 'ha', 'one', 'of', 'the', 'highest', 'number', 'of', 'uninsured', 'people', 'in', 'the', 'u', 'so', 'are', 'the', 'people', 'who', 'are', 'uninsured', 'american', 'or', 'is', 'the', 'bigger', 'portion', 'people', 'who', 'are', 'immigrant', 'seventy', 'five', 'percent', 'at', 'least', 'of', 'our', 'uninsured', 'are', 'u', 'citizen', 'and', 'then', 'maybe', 'another', 'half', 'a', 'million', 'are', 'lawfully', 'present', 'immigrant', 'some', 'of', 'the', 'barrier', 'to', 'coverage', 'that', 'both', 'u', 'citizen', 'and', 'lawfully', 'present', 'immigrant', 'face', 'you', 'know', 'are', 'thing', 'that', 'are', 'well', 'within', 'the', 'authority', 'of', 'our', 'legislature', 'and', 'our', 'governor', 'to', 'address', 'and', 'obviously', 'some', 'of', 'the', 'xxxxx', 'of', 'the', 'affordable', 'care', 'act', 'that', 'have', 'left', 'some', 'american', 'still', 'unable', 'to', 'afford', 'coverage', 'are', 'squarely', 'in', 'the', 'lap', 'of', 'the', 'u', 'congress', 'so', 'when', 'the', 'president', 'say', 'that', 'uninsured', 'individual', 'who', 'are', 'immigrant', 'are', 'a', 'burden', 'on', 'the', 'health', 'care', 'system', 'that', 'is', 'not', 'the', 'main', 'cause', 'of', 'the', 'burden', 'of', 'the', 'uninsured', 'most', 'of', 'that', 'is', 'citizen', 'so', 'he', 'is', 'actually', 'incorrect', 'absolutely', 'the', 'vast', 'majority', 'of', 'uninsured', 'american', 'are', 'u', 'citizen', 'that', 'is', 'the', 'big', 'challenge', 'we', 'have', 'we', 'are', 'obviously', 'way', 'more', 'complicated', 'here', 'and', 'were', 'looking', 'to', 'make', 'it', 'even', 'more', 'complicated', 'for', 'people', 'to', 'visit', 'the', 'united', 'state', 'potentially', 'more', 'expensive', 'and', 'thereby', 'discourage', 'particularly', 'people', 'who', 'are', 'not', 'of', 'mean', 'from', 'visiting', 'the', 'united', 'state', 'that', 'wa', 'anne', 'xxxxx', 'of', 'the', 'center', 'for', 'public', 'policy', 'priority', 'in', 'austin', 'texas', 'thank', 'you', 'so', 'much', 'thank', 'you', 'lulu']\n",
      "['yyyyy', 'trump', 'ha', 'announced', 'another', 'new', 'immigration', 'rule', 'set', 'to', 'take', 'effect', 'next', 'month', 'though', 'it', 'likely', 'headed', 'to', 'federal', 'court', 'the', 'rule', 'would', 'require', 'foreign', 'national', 'to', 'prove', 'they', 'have', 'health', 'insurance', 'or', 'the', 'money', 'to', 'pay', 'for', 'their', 'own', 'health', 'care', 'cost', 'before', 'they', 'can', 'legally', 'enter', 'the', 'united', 'state', 'simply', 'put', 'no', 'proof', 'no', 'to', 'quote', 'protect', 'the', 'availability', 'of', 'health', 'care', 'benefit', 'for', 'american', 'we', 'wanted', 'to', 'get', 'a', 'better', 'understanding', 'of', 'how', 'immigrant', 'use', 'and', 'pay', 'for', 'health', 'care', 'in', 'the', 'united', 'state', 'anne', 'xxxxx', 'is', 'an', 'associate', 'director', 'of', 'the', 'center', 'for', 'public', 'policy', 'priority', 'in', 'austin', 'texas', 'shes', 'spent', 'her', 'career', 'working', 'on', 'health', 'care', 'access', 'issue', 'and', 'she', 'join', 'u', 'now', 'welcome', 'good', 'morning', 'how', 'would', 'this', 'work', 'i', 'mean', 'if', 'youre', 'a', 'visitor', 'and', 'youre', 'traveling', 'you', 'would', 'obviously', 'get', 'travel', 'insurance', 'which', 'isnt', 'terribly', 'expensive', 'if', 'you', 'are', 'coming', 'to', 'immigrate', 'then', 'what', 'would', 'me', 'a', 'little', 'bit', 'about', 'this', 'is', 'this', 'seems', 'like', 'an', 'additional', 'step', 'that', 'already', 'when', 'you', 'apply', 'complicated', 'if', 'youre', 'coming', 'into', 'this', 'country', 'and', 'presumably', 'if', 'you', 'have', 'a', 'work', 'visa', 'and', 'youre', 'being', 'sponsored', 'by', 'someone', 'you', 'would', 'have', 'insurance', 'already', 'so', 'it', 'unclear', 'a', 'little', 'bit', 'who', 'this', 'is', 'targeting', 'exactly', 'it', 'your', 'state', 'texas', 'ha', 'one', 'of', 'the', 'highest', 'number', 'of', 'uninsured', 'people', 'in', 'the', 'u', 'so', 'are', 'the', 'people', 'who', 'are', 'uninsured', 'american', 'or', 'is', 'the', 'bigger', 'portion', 'people', 'who', 'are', 'immigrant', 'the', 'u', 'congress', 'so', 'when', 'the', 'president', 'say', 'that', 'uninsured', 'individual', 'who', 'are', 'immigrant', 'are', 'a', 'burden', 'on', 'the', 'health', 'care', 'system', 'that', 'is', 'not', 'the', 'main', 'cause', 'of', 'the', 'burden', 'of', 'the', 'uninsured', 'most', 'of', 'that', 'is', 'citizen', 'united', 'state', 'that', 'wa', 'anne', 'xxxxx', 'of', 'the', 'center', 'for', 'public', 'policy', 'priority', 'in', 'austin', 'texas', 'thank', 'you', 'so', 'thank', 'you', 'lulu']\n",
      "['visa', 'the', 'president', 'said', 'this', 'action', 'is', 'necessary', 'you', 'have', 'to', 'show', 'it', 'a', 'great', 'question', 'that', 'we', 'probably', 'cant', 'answer', 'it', 'sound', 'to', 'me', 'like', 'you', 'know', 'it', 'could', 'conceivably', 'require', 'the', 'creation', 'of', 'a', 'whole', 'new', 'segment', 'of', 'the', 'health', 'insurance', 'industry', 'thats', 'just', 'designed', 'to', 'answer', 'this', 'new', 'requirement', 'for', 'even', 'visiting', 'the', 'united', 'state', 'the', 'other', 'thing', 'that', 'xxxxx', 'for', 'a', 'visa', 'outside', 'of', 'the', 'united', 'state', 'you', 'do', 'have', 'to', 'prove', 'some', 'sort', 'of', 'financial', 'stability', 'youre', 'absolutely', 'right', 'even', 'for', 'these', 'short', 'term', 'visitor', 'type', 'visa', 'there', 'already', 'screening', 'to', 'try', 'to', 'detect', 'people', 'who', 'might', 'be', 'coming', 'here', 'without', 'mean', 'and', 'likely', 'to', 'for', 'example', 'xxxxx', 'their', 'visa', 'getting', 'insurance', 'in', 'this', 'country', 'is', 'complicated', 'for', 'american', 'it', 'is', 'even', 'more', 'sound', 'frankly', 'more', 'like', 'the', 'intention', 'is', 'for', 'the', 'optic', 'of', 'the', 'policy', 'a', 'much', 'a', 'it', 'is', 'you', 'know', 'having', 'a', 'practical', 'system', 'in', 'place', 'to', 'do', 'it', 'if', 'you', 'read', 'the', 'document', 'that', 'wa', 'announced', 'you', 'find', 'that', 'it', 'actually', 'outline', 'many', 'of', 'the', 'other', 'thing', 'that', 'the', 'trump', 'administration', 'ha', 'either', 'proposed', 'or', 'is', 'in', 'the', 'process', 'of', 'trying', 'to', 'pursue', 'so', 'theyve', 'presented', 'it', 'in', 'their', 'own', 'document', 'a', 'part', 'of', 'what', 'advocate', 'for', 'immigrant', 'refer', 'to', 'a', 'the', 'invisible', 'wall', 'seventy', 'five', 'percent', 'at', 'least', 'of', 'our', 'uninsured', 'are', 'u', 'citizen', 'and', 'then', 'maybe', 'another', 'half', 'a', 'million', 'are', 'lawfully', 'present', 'immigrant', 'some', 'of', 'the', 'barrier', 'to', 'coverage', 'that', 'both', 'u', 'citizen', 'and', 'lawfully', 'present', 'immigrant', 'face', 'you', 'know', 'are', 'thing', 'that', 'are', 'well', 'within', 'the', 'authority', 'of', 'our', 'legislature', 'and', 'our', 'governor', 'to', 'address', 'and', 'obviously', 'some', 'of', 'the', 'xxxxx', 'of', 'the', 'affordable', 'care', 'act', 'that', 'have', 'left', 'some', 'american', 'still', 'unable', 'to', 'afford', 'coverage', 'are', 'squarely', 'in', 'the', 'lap', 'of', 'so', 'he', 'is', 'actually', 'incorrect', 'absolutely', 'the', 'vast', 'majority', 'of', 'uninsured', 'american', 'are', 'u', 'citizen', 'that', 'is', 'the', 'big', 'challenge', 'we', 'have', 'we', 'are', 'obviously', 'way', 'more', 'complicated', 'here', 'and', 'were', 'looking', 'to', 'make', 'it', 'even', 'more', 'complicated', 'for', 'people', 'to', 'visit', 'the', 'united', 'state', 'potentially', 'more', 'expensive', 'and', 'thereby', 'discourage', 'particularly', 'people', 'who', 'are', 'not', 'of', 'mean', 'from', 'visiting', 'the', 'much']\n"
     ]
    }
   ],
   "source": [
    "# test_text = ['hello', 'my', 'name', 'is', 'matt', 'and', 'i', 'am', 'your', 'host', 'today', 'im', 'joined', 'by', 'my', 'friend', 'john', 'who', 'is', 'our', 'guest',\n",
    "#              'good', 'morning', 'john', 'how', 'are', 'you', 'doing', 'today', 'i', 'am', 'doing', 'well', 'thank', 'you', 'for', 'asking', 'i', 'am', 'excited', 'to', 'be', 'here']\n",
    "# test_text = ['yyyyy', 'who', 'are', 'you', 'i', 'am', 'john']\n",
    "\n",
    "# test_text_indices = [word_indices[word] for word in test_text]\n",
    "\n",
    "# get all the text from episode 1\n",
    "test_text = ' '.join(df_processed[df_processed['episode'] == 2]['utterance'].astype(str)).split()\n",
    "test_text_indices = [word_indices[word] for word in test_text]\n",
    "print(test_text)\n",
    "\n",
    "state_sequence = tensor_viterbi(test_text_indices, tensor_normalized, emission, initial)\n",
    "state_sequence\n",
    "\n",
    "# get the words that are host words\n",
    "host_words = [word for i, word in enumerate(test_text) if state_sequence[i - 1] == 0]\n",
    "print(host_words)\n",
    "\n",
    "# get the words that are guest words\n",
    "guest_words = [word for i, word in enumerate(test_text) if state_sequence[i - 1] == 1]\n",
    "print(guest_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 32005\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "test_episode = np.random.choice(test['episode'].unique())\n",
    "test_episode_df = test[test['episode'] == test_episode][['is_host', 'utterance']]\n",
    "\n",
    "# Split utterances into words\n",
    "test_episode_df['utterance'] = test_episode_df['utterance'].str.split()\n",
    "\n",
    "# Expand the lists into separate rows, replicating the 'is_host' value for each word\n",
    "test_episode_df = test_episode_df.explode('utterance')\n",
    "\n",
    "# Now, each row in test_episode_df contains a single word and its corresponding 'is_host' value\n",
    "test_label = test_episode_df['is_host'].to_numpy().astype(int)\n",
    "\n",
    "# # get all of the words in the test episode\n",
    "test_episode_words = ' '.join(test_episode_df['utterance'].astype(str)).split()\n",
    "\n",
    "\n",
    "#  get the indices of the words in the test episode\n",
    "test_episode_word_indices = [word_indices[word] for word in test_episode_words]\n",
    "obs = test_episode_word_indices\n",
    "\n",
    "state_sequence = tensor_viterbi(obs, tensor_normalized, emission, initial)\n",
    "\n",
    "# view the test_label and the state_sequence side by side\n",
    "label_sequence = np.zeros((len(test_label[1:]), 2))\n",
    "label_sequence[:,0] = test_label[1:]\n",
    "label_sequence[:,1] = state_sequence\n",
    "\n",
    "#print the episode number\n",
    "print(f'Episode: {test_episode}')\n",
    "\n",
    "# print the full output ( not truncated)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(label_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
